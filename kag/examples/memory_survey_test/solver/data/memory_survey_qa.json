{
    "question": [
        "What role do memory-driven communication frameworks play in fostering a common understanding among agents?",
        "What does the adaptation time refer to in the context of memory writing and management?",
        "What are the challenges associated with storing information about agent-environment interactions?",
        "What do typical agent applications demonstrate about the importance of the memory module in different scenarios?",
        "How does integrating domain-specific knowledge through fine-tuning methods benefit agents?",
        "What are some approaches used in recommendation systems to simulate user behaviors and improve performance?",
        "What advancements have been made in transformer architecture for long-context large language models?",
        "How do practical downstream scenarios contribute to the evaluation of memory in agents?",
        "What role do memory-driven communication frameworks play in fostering a common understanding among agents?",
        "How are LLM-based agents adapted for use in the medical domain?",
        "What is the role of the Faiss vector store in obtaining top-K successful trajectories for a task?",
        "What are the two types of approaches for representing memory in parametric form?",
        "How are the knowledge, alignment, and safety control capabilities of LLMs evaluated?",
        "How do macroeconomic trends impact agents' decision-making in social network simulation systems?",
        "What is the purpose of the agent memory pool in social network simulation systems?",
        "What are the key problems in subjective evaluation?",
        "What is the focus of the survey on multimodal large language models presented at the 2023 IEEE International Conference on Big Data?",
        "How are SQL statements used in the memory reading operation in ChatDB?",
        "What is the purpose of using a flash memory cache mechanism in storing recent textual memories?",
        "What problems do LLM inaccuracies and bias cause, and how can they be addressed?",
        "What self-evolving features help LLM agents adapt autonomously?",
        "How does Reflexion use past chats to boost LLM task performance, and how does this compare to memory in other models?",
        "How does measuring a memory module independently improve its reliability over task-based evaluations?",
        "What societal issues stem from LLM bias, and how can trust be improved?",
        "How do agents use temporal caching for film suggestions?",
        "How might domain-specific tuning affect LLMs' general knowledge?",
        "How is memory read latency measured, and why does it matter for computational efficiency?",
        "How does contextual memory improve code consistency and refinement?",
        "How do memory sync and comms boost MAS adaptability and intelligence?",
        "How is memory effectiveness in LLM agents evaluated?",
        "How do agents use public info from digital sources in decisions?",
        "How do true positives and false negatives assess memory retrieval?",
        "How does memory consistency with roles boost realism and task adherence in social sims?",
        "How is historical data combined for memory in agent loops?",
        "How do memory trade-offs impact LLM inference cost and time?",
        "How does long-term memory affect an agent's exploration?",
        "How do LLM agents use past experiences and resources to enhance exploration and code generation?",
        "How does using past interactions and external knowledge help an agent adapt over trials?",
        "How do synced memory modules boost message interpretation and decision consistency?",
        "How does using external expertise boost an agent's performance but risk overfitting and memory loss?",
        "How does fine-tuning with domain expertise improve LLM agents' decision-making?",
        "How do selective fact adjustments in LLMs compare to pattern extraction in terms of memory use and efficiency?",
        "How does high-level info from past trials boost an agent's adaptability in new settings?",
        "How does RAG improve LLMs' accuracy and reduce errors?",
        "What time and memory challenges do LLM agents face in learning over time?",
        "How do LLM misconceptions impact reliability, and which models tackle this?",
        "Which method swaps adjacent elements to sort a list?",
        "How does RAG boost LLMs' accuracy and domain adaptation?",
        "What key questions and design factors are crucial for memory in LLM agents?",
        "How do TP, FP, & FN affect memory retrieval assessment?",
        "How do memory models aid agent sync and strategy in MAS via better communication?",
        "How do LLM agents use past interactions to improve decisions?",
        "How does the memory module boost agent accuracy and decisions in multi-agent simulations?",
        "How do LLM agents use concatenated sequences for better memory in personalized dialogues?",
        "What challenges do LLM agents face with time and memory in learning?",
        "How do LLM agents boost code coherence and error fixing via chat interfaces and memory?",
        "How does Reflexion use past trials to boost LLM agents' performance?",
        "Which model rates dialogue consistency?",
        "What task does CodeAgent aim for in code gen with complex dependencies?",
        "How do memory & simulation improve user modeling in recommender systems?",
        "How does ChatDB boost data retrieval with symbolic memory & SQL?",
        "What's the deployment issue for LLMs with more computing needs and memory limits?",
        "Which agents excel in dialogue and info gathering?",
        "How do agents boost efficiency with past successes?",
        "How do LLMs get molecule data?",
        "In which areas do LLMs aid query and extraction?",
        "How does quadratic growth in attention impact latency?",
        "Which method boosts agents' tool use in math?",
        "Why don't LLMs handle all text equally in long contexts?",
        "How are memory retrieval scores derived using TP, FP, and FN?",
        "Which framework did Montazeralghaem et al. propose for feedback at SIGIR 2020?",
        "How do targeted LLM tweaks avoid losing other info?",
        "How do memory frameworks help agents cooperate?",
        "How does MemGPT use virtual context for recent histories?",
        "In what scenarios do LLM agents use memory for suggestions?",
        "How do LLM tweaks stop forgetting during updates?",
        "How does SCM boost info recency with flash memory?",
        "How does MemoChat use topics to index memory?"
    ],
    "contexts": [
        [
            "For example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a\ncommon understanding among agents. In addition to cooperative scenarios, some studies also focus\non competitive scenarios, and the information asymmetry becomes a crucial issue [172]."
        ],
        [
            "Time & Hardware Cost. The total time cost includes the time leveraged for memory adaption and\ninference. The adaptation time refers to the time of memory writing and memory management, while\nthe inference time indicates the time latency of memory reading. In specific, the difference from the\n\n21\n\n\fend time to the start time of memory operations can be considered as the time consumption. Formally,\nthe average time consumption of each type of operation can be represented as\n\n∆time =\n\n1\nM\n\nM\n(cid:88)\n\ni=1"
        ],
        [
            "Effectiveness. The textual memory stores raw information about the agent-environment interactions,\nwhich is more comprehensive and detailed. However, it is constrained by the token limitation\nof LLM prompts, which makes the agent hard to store extensive information. In contrast, the\nparametric memory is not limited by the prompt length, but it may suffer from information loss when\ntransforming texts into parameters, and the complex memory training can bring additional challenges.\n\n17"
        ],
        [
            "The main contributions of this paper can be summarized as follows: (1) We formally define the mem-\nory module and comprehensively analyze its necessity for LLM-based agents. (2) We systematically\nsummarize existing studies on designing and evaluating the memory module in LLM-based agents,\nproviding clear taxonomies and intuitive insights. (3) We present typical agent applications to show\nthe importance of the memory module in different scenarios. (4) We analyze the key limitations of"
        ],
        [
            "Fine-tuning Methods. Integrating external knowledge into the memory of agents is beneficial\nfor enriching domain-specific knowledge on top of its general knowledge. To infuse the domain\nknowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with\nthe memory of domain experts. It significantly improves the agent’s ability to accomplish domain-\nspecific tasks. In task (A) of the example in Section 3.1, the external knowledge of attractions from"
        ],
        [
            "In the field of recommendation, some previous works focus on simulating users in recommender\nsystems [95, 108], where the memory can represent the user profiles and histories in the real\nworld. Others try to improve the performance of recommendation, or provide other formats of\nrecommendation interfaces [149, 102]. Wang et al. [95] simulate user behaviors in recommendation\nscenarios to generate data for recommender systems, and the agents store past observations and"
        ],
        [
            "[19] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan\nYang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large\nlanguage models: A comprehensive survey. arXiv preprint arXiv:2311.12351, 2023.\n\n29\n\n\f[20] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Ar-\nmaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in\nlarge language models. arXiv preprint arXiv:2402.02244, 2024."
        ],
        [
            "The evaluation of long-context applications provides broader approaches to assess the function of\nmemory in agents, focusing on practical downstream scenarios. The comprehensive benchmarks [138,\n140] also provide an objective assessment for the ability of long-context understanding.\n\n6.2.4 Other Tasks\n\nIn addition to the above three types of major tasks for indirect evaluation, there are also some other\nmetrics in general tasks that can reveal the effectiveness of the memory module."
        ],
        [
            "For example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a\ncommon understanding among agents. In addition to cooperative scenarios, some studies also focus\non competitive scenarios, and the information asymmetry becomes a crucial issue [172]."
        ],
        [
            "Minecraft Wiki and craft recipes to provide an infinite source of knowledge for their navigation.\nCodeAgent [114] focuses on the repo-level code generation task, which commonly requires complex\ndependencies and extensive documentation. It designs a web search strategy for acquiring related\nexternal knowledge. ChatDoctor [115] adapts LLM-based agents to the medical domain. It fine-tunes\nan acquisition process to retrieve external knowledge from Wikipedia and medical databases."
        ],
        [
            "the Faiss [124] vector store as the pool of memory, and obtains the top-K successful trajectories that\nshare the highest similarity scores with the current task."
        ],
        [
            "5.2.2 Memory in Parametric Form\n\nAn alternative type of approaches is to represent memory in parametric form. They do not take up\nthe extra length of context in prompts, so they are not constrained by the length limitations of LLM\ncontext. However, the parametric memory form is still under-researched, and we categorize previous\nworks into two types: fine-tuning methods and memory editing methods."
        ],
        [
            "Evaluation. The surveys in this category focus on how to evaluate the capability of LLMs. Specifi-\ncally, Chang et al. [34] comprehensively summarize the evaluation methods from an overall perspec-\ntive. It encompasses different evaluation tasks, methods, and benchmarks, which serve as critical\nparts in assessing LLM performances. Guo et al. [35] care more about the evaluation targets and\ndescribe how to evaluate the knowledge, alignment, and safety control capabilities of LLMs, which"
        ],
        [
            "[2] focus on social network simulation systems. Each agent in the system has a memory pool, which\nconsists of diverse user messages from online platforms to identify the user. Li et al. [163] maintain\nconversation contexts, encompassing the economic environment and agent decisions from previous\nmonths, in order to simulate the impact of broad macroeconomic trends on agents’ decision-making\nand to make the agents grasp market dynamics. Li et al. [109] simulate the job-seeking scenario"
        ],
        [
            "[2] focus on social network simulation systems. Each agent in the system has a memory pool, which\nconsists of diverse user messages from online platforms to identify the user. Li et al. [163] maintain\nconversation contexts, encompassing the economic environment and agent decisions from previous\nmonths, in order to simulate the impact of broad macroeconomic trends on agents’ decision-making\nand to make the agents grasp market dynamics. Li et al. [109] simulate the job-seeking scenario"
        ],
        [
            "6.1.1 Subjective Evaluation\n\nIn subjective evaluation, there are two key problems, that is, (1) what aspects should be evaluated\nand (2) how to conduct the evaluation process. To begin with, the following two aspects are the most\ncommon perspectives leveraged to evaluate the memory module."
        ],
        [
            "[21] Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The\nwhat, why, and how of context length extension techniques in large language models–a detailed\nsurvey. arXiv preprint arXiv:2401.07872, 2024.\n\n[22] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\npages 2247–2256. IEEE, 2023."
        ],
        [
            "Representative Studies. In ChatDB [96], the memory reading operation is executed by the SQL\nstatements. These statements will be generated by agents as a series of Chain-of-Memory in advance.\nIn MPC [101], the agents can retrieve relevant memory from the memory pool. This method also\nproposes to provide Chain-of-Thought examples for ignoring certain memory. ExpeL [82] utilizes\nthe Faiss [124] vector store as the pool of memory, and obtains the top-K successful trajectories that"
        ],
        [
            "In previous studies, there are various strategies to store recent textual memories. For example,\nSCM [98] proposes a flash memory based on the cache mechanism, which preserves observations from\nthe recent t − 1 time steps, aimed at enhancing the recency of information. MemGPT [100] considers\nthe agent as an operating system, which can dynamically interact with users through a natural interface.\nIt designs the working context to hold recent histories, as a part of virtual context management. In"
        ],
        [
            "Challenges. The surveys in this category focus on trustworthiness in LLMs, such as hallucination,\nbias, unfairness, explainability, security, and privacy. Hallucination in LLMs refers to the problem\nthat LLMs may generate misconceptions or fabrications, impacting their reliability for downstream\napplications. Zhang et al. [53], Huang et al. [54], Rawte et al. [55], Ye et al. [56], Ji et al. [57],\nTonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the",
            "Tonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the\nhallucination problem in LLMs. The bias and unfairness problems refer to the phenomenon that\nLLMs may unequally treat different humans or objectives, which can lead to the propagation of\nsocietal stereotypes and discrimination. Gallegos et al. [60], Kotek et al. [61] and Li et al. [62]\ncomprehensively discuss these challenges and summarize existing methods for alleviating them. The"
        ],
        [
            "user’s satisfaction. To further push the boundary of LLMs towards AGI, recent years have witnessed a\nlarge number of studies on LLM-based agents [3, 4], where the key is to equip LLMs with additional\nmodules to enhance their self-evolving capability in real-world environments.",
            "be able to improve themselves by autonomously exploring and learning from the real world. For\nexample, if a trip-planning agent intends to book a ticket, it should send an order request to the ticket\nwebsite, and observe the response before taking the next action. A personal assistant agent should\nadjust its behaviors according to the user’s feedback, providing personalized responses to improve\nuser’s satisfaction. To further push the boundary of LLMs towards AGI, recent years have witnessed a"
        ],
        [
            "Representative Studies. One of the most prominent studies is Reflexion [5], which proposes verbal\nreinforcement learning for LLM-based agents. It derives the experiences from past trials in verbal\nform, and applies them in subsequent trials to improve the performance of the same task. Furthermore,\nRetroformer [103] fine-tunes the reflection model, enabling the agent to extract cross-trial information\nfrom past trials more effectively. In Synapse [91], the agents focus on solving the computer control",
            "Representative Studies. Generative Agents [83] aims to simulate human’s daily behaviors by\nusing LLM-based agents. The memory of an agent is derived from the historical behaviors to\nachieve a target, for example, the collection of relevant papers when researching on a specific topic.\nMemoChat [94] aims to chat with humans, where the memory of the agent is derived based on\nthe conversation history of a dialogue session. TiM [97] aims to enhance the agent’s reasoning"
        ],
        [
            "Compared with direct evaluation, indirect evaluation via specific tasks can be easier to conduct, since\nthere are already many public benchmarks. However, the performance on tasks can be attributed\nto various factors, and memory is only one of them, which may make the evaluation results biased.\nBy direct evaluation, the effectiveness of the memory module can be independently evaluated,\nwhich improves the reliability of the evaluation results. However, to our knowledge, there are no",
            "is, (1) direct evaluation, which independently measures the capability of the memory module. (2)\nindirect evaluation, which evaluates the memory module via end-to-end agent tasks. If the tasks can\nbe effectively accomplished, the memory module is demonstrated to be useful."
        ],
        [
            "Tonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the\nhallucination problem in LLMs. The bias and unfairness problems refer to the phenomenon that\nLLMs may unequally treat different humans or objectives, which can lead to the propagation of\nsocietal stereotypes and discrimination. Gallegos et al. [60], Kotek et al. [61] and Li et al. [62]\ncomprehensively discuss these challenges and summarize existing methods for alleviating them. The",
            "Challenges. The surveys in this category focus on trustworthiness in LLMs, such as hallucination,\nbias, unfairness, explainability, security, and privacy. Hallucination in LLMs refers to the problem\nthat LLMs may generate misconceptions or fabrications, impacting their reliability for downstream\napplications. Zhang et al. [53], Huang et al. [54], Rawte et al. [55], Ye et al. [56], Ji et al. [57],\nTonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the"
        ],
        [
            "It designs the working context to hold recent histories, as a part of virtual context management. In\nRecAgent [95], the agents are designed to simulate user behaviors in movie recommendations.\nIt stores some temporal information in short-term memory as an intermediate cache, which can\nsimulate the memory mechanism of the human brain [122, 123]. These representative methods can\ndynamically update memories based on recent interactions, and pay more attention to the recent"
        ],
        [
            "The fine-tuning methods can effectively bridge the gap between general agents and specialized\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies",
            "Fine-tuning Methods. Integrating external knowledge into the memory of agents is beneficial\nfor enriching domain-specific knowledge on top of its general knowledge. To infuse the domain\nknowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with\nthe memory of domain experts. It significantly improves the agent’s ability to accomplish domain-\nspecific tasks. In task (A) of the example in Section 3.1, the external knowledge of attractions from"
        ],
        [
            "Time & Hardware Cost. The total time cost includes the time leveraged for memory adaption and\ninference. The adaptation time refers to the time of memory writing and memory management, while\nthe inference time indicates the time latency of memory reading. In specific, the difference from the\n\n21\n\n\fend time to the start time of memory operations can be considered as the time consumption. Formally,\nthe average time consumption of each type of operation can be represented as\n\n∆time =\n\n1\nM\n\nM\n(cid:88)\n\ni=1",
            "∆time =\n\n1\nM\n\nM\n(cid:88)\n\ni=1\n\ni − tstart\ntend\n\ni\n\n,\n\nwhere M represents the number of these operations, tend\ni means the end time of the i-th operation, and\ntstart\nindicates the start time of that operation. As for the computation overhead, it can be evaluated\ni\nby the peak GPU memory allocation. In previous works, Tack et al. [106] utilize the peak memory\nallocation and adaptation time to assess the efficiency of memory operations."
        ],
        [
            "coherence of the generated code. Furthermore, the memory is also crucial for the iterative optimization\nof code, as it can identify the developer’s targets based on the histories.",
            "By leveraging external resources, the agents can learn from code-related knowledge and store it\ninto their memory, thereby enhancing the capabilities of code generation. In addition, the memory\ncan improve the continuity and consistency in code generation. By integrating contextual memory,\nthe agent can better understand the requirements for software development, thereby enhancing the\ncoherence of the generated code. Furthermore, the memory is also crucial for the iterative optimization"
        ],
        [
            "The exploration of memory mechanisms within LLMs has burgeoned into the dynamic domain of\nmulti-agent systems (MAS), marking significant advancements in the realms of synchronization,\ncommunication, and the management of information asymmetry. One pivotal aspect that emerges in\nthe cooperative scenarios is memory synchronization among agents. This process is fundamental\nfor establishing a unified knowledge base, ensuring consistency in decision-making across different",
            "modules that can further enhance agent synchronization, enable more effective communication, and\nprovide strategic advantages in information-rich environments. The development of such memory\nmodels would not only necessitate addressing the current challenges of memory integration and man-\nagement, but also explore the untapped potentials of memory in facilitating more robust, intelligent,\nand adaptable MAS. As evidenced by pioneering research, the evolving landscape of LLM-based"
        ],
        [
            "6 How to Evaluate the Memory in LLM-based Agent\n\n6.1 Direct Evaluation .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n6.1.1\n\nSubjective Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n6.1.2 Objective Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n6.2\n\nIndirect Evaluation .\n\n.\n\n6.2.1 Conversation .\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n. . .\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "6.2\n\nIndirect Evaluation\n\nBesides the above method that directly evaluates the memory module, evaluating via task completion\nis also a popular evaluation strategy. The intuition behind this type of approaches is that if the\nagent can successfully complete a task that highly depends on memory, it suggests that the designed\nmemory module is effective. In the following parts, we present several representative tasks that are\nleveraged to evaluate the memory module in indirect ways.\n\n6.2.1 Conversation"
        ],
        [
            "Wikipedia and OpenWeatherMap4, are available online (either free of charge or on a paying basis),\nand can be conveniently accessed through API calls. For instance, in [Step 2] of task (A) of the\nexample in Section 3.1, external knowledge from the digital magazine is obtained with tool methods.",
            "External Knowledge. To obtain more information, some agents acquire external knowledge by\ninvoking tools, with the aim of transforming additional relevant knowledge into their own memories\nfor decision-making. For instance, accessing external knowledge through Application Programming\nInterface (API) is a common practice [104, 5]. Nowadays, abundant public information, such as\nWikipedia and OpenWeatherMap4, are available online (either free of charge or on a paying basis),"
        ],
        [
            "F1 = 2 ·\n\nPrecision · Recall\nPrecision + Recall\n\n,\n\nwhere the precision and recall scores are calculated as Precision = TP\nTP+FN . The\nTP represents the number of true positive memory contents, FP means the number of false positive\nmemory contents, and FN indicates the number of false negative memory contents. In previous works,\nLu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al. [6]\nfocus on assessing whether related memory can be successfully retrieved."
        ],
        [
            "There are several insights in designing an agent’s memory for role-play and social simulation. First,\nthe memory should be consistent with the roles’ characteristics, which can be used to identify each\nrole and distinguish it from the others. This is crucial for improving the realism of role-play and the\ndiversity of social simulation. Second, the memory should appropriately influence the subsequent\n\n24",
            "the agent consistently follow the role profiles. Without memory, the agent may easily step out of the\nrole during the simulation process [95]. Both of the above examples show that the memory is not an\noptional component, but is necessary for the agents to accomplish given tasks."
        ],
        [
            "Complete Interactions. This method stores all the information of the agent-environment interaction\nhistory based on long-context strategies [116]. For the example in Section 3.1, the memory of the\n\n14\n\n\fagent in task (A) after step 2 can be implemented by concatenating all the information before step 2,\nand the final textual form memory is: \"Your memory is [Step 1] (Agent) ... (Online Ticket Office) ...\n[Step 2] ... Please infer based on your memory\".",
            "memory to store four types of information including (1) complete agent-environment interactions, (2)\nrecent agent-environment interactions, (3) retrieved agent-environment interactions, and (4) external\nknowledge. In the former three methods, the memory leverages natural languages to describe the\ninformation within the agent-environment interaction loop. In the former three types, they record\nthe information inside the agent-environment interaction loop, while the last type leverages natural"
        ],
        [
            "Efficiency. For textual memory, each LLM inference requires to integrate memory into the context\nprompt, which leads to higher costs and longer processing times. In contrast, for parametric memory,\nthe information can be integrated into the parameters of the LLM, eliminating the extra costs of these\ncontexts. However, parametric memory takes additional costs in the writing process, but textual\nmemory is easier to write, especially for small amounts of data. In a nutshell, textual memory is more",
            "At present, the memory of LLM-based agents is predominantly in textual form, especially for\ncontextual knowledge such as observation records, trial experiences, and textual knowledge databases.\nAlthough textual memory possesses the advantages of being interpretable and easy to expand and\nedit, it also implies a sacrifice in efficiency compared to parametric memory. Essentially, parametric\nmemory boasts a higher information density, expressing semantics through continuous real-number"
        ],
        [
            "In summary, no matter inside-trial or cross-trial information, the key aspect of memory is to reflect on\npast interactions and draw experiences that can be applied to the subsequent exploration. In addition\nto accumulating experience through self-involving trials, absorbing external knowledge as part of the\nagent’s memory is also an important way to enhance the exploratory capabilities of the agent.\n\n7.4 Code Generation",
            "Discussion. According to the accumulated memory of cross-trial information, the agents are able to\naccumulate experiences, which is important for their evolution. Based on the past experiences, the\nagents can adjust their actions based on the overall feedback of the whole process. In contrast to the\ninside-trial observations, which serve as short-term memory, the trial experiences can be considered\nas long-term memory. It utilizes feedback from different trials to support a wider range of agent"
        ],
        [
            "For games and open-world exploration, LLM-based agents always maintain post observations as\ntask contexts, and store experiences in previous successful trials. By leveraging past experiences,\nagents can avoid making the same mistakes repeatedly and achieve a high-level understanding of\nenvironments, thus exploring more effectively. Some of them can acquire external databases or\nAPIs to obtain general knowledge [99, 93, 159, 161]. Wang et al. [99] save obtained skills into",
            "In the scenario of code generation, LLM-based agents can search relevant information from the\nmemory, thereby obtaining more knowledge for development. They can save previous experiences for\nfuture problems, and also maintain context in conversational development interfaces [142, 144, 1, 109].\nTsai et al. [142] construct an external non-parametric memory database, which stores the compiler\nerrors and human expert instructions for automatic syntax error fixing. In [144], personal information"
        ],
        [
            "In summary, no matter inside-trial or cross-trial information, the key aspect of memory is to reflect on\npast interactions and draw experiences that can be applied to the subsequent exploration. In addition\nto accumulating experience through self-involving trials, absorbing external knowledge as part of the\nagent’s memory is also an important way to enhance the exploratory capabilities of the agent.\n\n7.4 Code Generation",
            "Discussion. According to the accumulated memory of cross-trial information, the agents are able to\naccumulate experiences, which is important for their evolution. Based on the past experiences, the\nagents can adjust their actions based on the overall feedback of the whole process. In contrast to the\ninside-trial observations, which serve as short-term memory, the trial experiences can be considered\nas long-term memory. It utilizes feedback from different trials to support a wider range of agent"
        ],
        [
            "For example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a\ncommon understanding among agents. In addition to cooperative scenarios, some studies also focus\non competitive scenarios, and the information asymmetry becomes a crucial issue [172].",
            "for establishing a unified knowledge base, ensuring consistency in decision-making across different\nagents. For example, Chen et al. [170] emphasize the significance of integrating synchronized\nmemory modules for multi-robot collaboration. Another important aspect is the communication\namong agents, which heavily relies on memory for maintaining context and interpreting messages.\nFor example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a"
        ],
        [
            "The fine-tuning methods can effectively bridge the gap between general agents and specialized\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies",
            "Fine-tuning Methods. Integrating external knowledge into the memory of agents is beneficial\nfor enriching domain-specific knowledge on top of its general knowledge. To infuse the domain\nknowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with\nthe memory of domain experts. It significantly improves the agent’s ability to accomplish domain-\nspecific tasks. In task (A) of the example in Section 3.1, the external knowledge of attractions from"
        ],
        [
            "according to the task needs, thus mitigating the problem of outdated knowledge. Integrating external\nknowledge into the memory of LLM-based agents significantly expands their knowledge boundaries,\nproviding them with unlimited, up-to-date, and well-founded knowledge for decision-making.",
            "Fine-tuning Methods. Integrating external knowledge into the memory of agents is beneficial\nfor enriching domain-specific knowledge on top of its general knowledge. To infuse the domain\nknowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with\nthe memory of domain experts. It significantly improves the agent’s ability to accomplish domain-\nspecific tasks. In task (A) of the example in Section 3.1, the external knowledge of attractions from"
        ],
        [
            "Memory Editing Methods. Apart from the fine-tuning approaches, another type of methods for\ninfusing memory into model parameters is knowledge editing [133, 134]. Unlike fine-tuning methods\nthat extract patterns from certain datasets, knowledge editing methods specifically target and adjust\nonly the facts that need to be changed. It ensures that unrelated knowledge remains unaffected.\nKnowledge editing methods are more suitable for small-scale memory adjustments. Generally, they",
            "Knowledge editing methods provide an innovative way to update the information stored within the\nparameters of LLMs. By specifically targeting and adjusting the facts, these methods can ensure\nthe non-targeted knowledge unaffected during updates, thus mitigating the issue of catastrophic\nforgetting. Moreover, the targeted adjustment mechanism allows for more efficient and less resource-\nintensive updates, making knowledge editing an appealing choice for high-precision and real-time"
        ],
        [
            "explorations, for example, focusing more on previously failed trials or actions with lower exploring\nfrequencies [93]. (3) Knowledge abstraction. Another important function of the memory is to\nsummarize and abstract high-level information from raw observations, which is the basis for the agent\nto be more adaptive and generalizable to unseen environments [82]. In summary, self-evolution is the\nbasic characteristic of LLM-based agents, and memory is of key importance to self-evolution."
        ],
        [
            "LLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\nis key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\net al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\nis fundamental for LLMs to expand their capability in domains that require specialized knowledge."
        ],
        [
            "and apply the past observations. Lifelong learning in LLM-based agents holds significant practical\nvalue, such as in long-term social simulations and personal assistance. However, it also faces several\nchallenges. Firstly, lifelong learning is temporal, necessitating that an agent’s memory captures\ntemporality. This temporality could cause interactions between memories, such as memory overlap.\nFurthermore, due to the extended period of lifelong learning, it needs to store a vast amount of"
        ],
        [
            "Challenges. The surveys in this category focus on trustworthiness in LLMs, such as hallucination,\nbias, unfairness, explainability, security, and privacy. Hallucination in LLMs refers to the problem\nthat LLMs may generate misconceptions or fabrications, impacting their reliability for downstream\napplications. Zhang et al. [53], Huang et al. [54], Rawte et al. [55], Ye et al. [56], Ji et al. [57],\nTonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the",
            "Tonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the\nhallucination problem in LLMs. The bias and unfairness problems refer to the phenomenon that\nLLMs may unequally treat different humans or objectives, which can lead to the propagation of\nsocietal stereotypes and discrimination. Gallegos et al. [60], Kotek et al. [61] and Li et al. [62]\ncomprehensively discuss these challenges and summarize existing methods for alleviating them. The"
        ],
        [
            "GameSkills & KnowledgeHPMPSPCode Generationdef bubble_sort(arr):    n = len(arr)    # Traverse through all array elements    for i in range(n):        # Last i elements are already in place        for j in range(0, n-i-1):            # Traverse the array from 0 to n-i-1            # Swap if the element found is greater than the next element            if arr[j] > arr[j+1]:                arr[j], arr[j+1] = arr[j+1], arr[j]    return arrSort the numbers in ascending order.Bubble sort repeatedly steps",
            "arr[j], arr[j+1] = arr[j+1], arr[j]    return arrSort the numbers in ascending order.Bubble sort repeatedly steps through the list, compares adjacent elements and swaps them if they are in wrong order. The pass through the list is repeated until the list is sorted.Bubble sort can reorder a list of numbers.Development GroupRecommendationI want to buy a dress for the graduation party.(Context) She just bought a new blue dress. So she may need a white accessories to match it.(Personal"
        ],
        [
            "LLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which\nis key to providing LLMs with factual and up-to-date knowledge and removing hallucinations. Qin\net al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which\nis fundamental for LLMs to expand their capability in domains that require specialized knowledge."
        ],
        [
            "In this survey, we provide a systematical review on the memory mechanism of LLM-based agents,\nwhere we focus on three key problems including \"What is\", \"Why do we need\" and \"How to design\nand evaluate\" the memory module in LLM-based agents. To show the importance of the agent’s\nmemory, we also present many typical applications, where the memory module plays an important\nrole. We believe this survey can offer valuable references for newcomers to this domain, and also",
            "mechanism of LLM-based agents. In specific, we first discuss “what is” and “why\ndo we need” the memory in LLM-based agents. Then, we systematically review\nprevious studies on how to design and evaluate the memory module. In addition, we\nalso present many agent applications, where the memory module plays an important\nrole. At last, we analyze the limitations of existing work and show important future\ndirections. To keep up with the latest advances in this field, we create a repository"
        ],
        [
            "F1 = 2 ·\n\nPrecision · Recall\nPrecision + Recall\n\n,\n\nwhere the precision and recall scores are calculated as Precision = TP\nTP+FN . The\nTP represents the number of true positive memory contents, FP means the number of false positive\nmemory contents, and FN indicates the number of false negative memory contents. In previous works,\nLu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al. [6]\nfocus on assessing whether related memory can be successfully retrieved."
        ],
        [
            "modules that can further enhance agent synchronization, enable more effective communication, and\nprovide strategic advantages in information-rich environments. The development of such memory\nmodels would not only necessitate addressing the current challenges of memory integration and man-\nagement, but also explore the untapped potentials of memory in facilitating more robust, intelligent,\nand adaptable MAS. As evidenced by pioneering research, the evolving landscape of LLM-based",
            "The exploration of memory mechanisms within LLMs has burgeoned into the dynamic domain of\nmulti-agent systems (MAS), marking significant advancements in the realms of synchronization,\ncommunication, and the management of information asymmetry. One pivotal aspect that emerges in\nthe cooperative scenarios is memory synchronization among agents. This process is fundamental\nfor establishing a unified knowledge base, ensuring consistency in decision-making across different"
        ],
        [
            "5.1.2 Cross-trial Information\n\nFor LLM-based agents, the information accumulated across multiple trials in the environment is also\na crucial part of the memory, typically including successful and failed actions and their insights, such\nas failure reasons, common action patterns to succeed, and so on."
        ],
        [
            "Social simulation is basically an extension of role-playing, which focuses more on multi-agent\nmodeling. The memory module is an important component for such applications, which helps to\naccurately simulate human dynamic behaviors. In previous studies, Kaiya et al. [148] propose a\nSummarize-and-Forget memory mechanism for better self-monitoring in social scenarios. Gao et al.\n[2] focus on social network simulation systems. Each agent in the system has a memory pool, which",
            "[2] focus on social network simulation systems. Each agent in the system has a memory pool, which\nconsists of diverse user messages from online platforms to identify the user. Li et al. [163] maintain\nconversation contexts, encompassing the economic environment and agent decisions from previous\nmonths, in order to simulate the impact of broad macroeconomic trends on agents’ decision-making\nand to make the agents grasp market dynamics. Li et al. [109] simulate the job-seeking scenario"
        ],
        [
            "chatbot’s responses are concatenated to form a sequence as memory. Wang et al. [145] infuse\nrole-specific knowledge and episode memories into LLM-based agents, where context QA pairs\nare concatenated to form episode memory. Zhao et al. [146] aim to generate human-like responses,\nguided by personality traits extracted from narratives, which can be stored and retrieved by relevance\nand importance. Zhou et al. [147] generate character-based dialogues for different roles and empower",
            "LLM-based agents are well-suited for creating personal assistants, such as agents capable of engaging\nin long-term conversations with users [94, 101, 153], as well as those tasked with automatically\nseeking information [164]. These agents often need to memorize previous dialogues to maintain\nthe consistency, and remember critical styles and events to generate more personalized and relevant\nresponses. Lu et al. [94] maintain the context consistency for dialogues by saving contents and"
        ],
        [
            "and apply the past observations. Lifelong learning in LLM-based agents holds significant practical\nvalue, such as in long-term social simulations and personal assistance. However, it also faces several\nchallenges. Firstly, lifelong learning is temporal, necessitating that an agent’s memory captures\ntemporality. This temporality could cause interactions between memories, such as memory overlap.\nFurthermore, due to the extended period of lifelong learning, it needs to store a vast amount of"
        ],
        [
            "In the scenario of code generation, LLM-based agents can search relevant information from the\nmemory, thereby obtaining more knowledge for development. They can save previous experiences for\nfuture problems, and also maintain context in conversational development interfaces [142, 144, 1, 109].\nTsai et al. [142] construct an external non-parametric memory database, which stores the compiler\nerrors and human expert instructions for automatic syntax error fixing. In [144], personal information",
            "By leveraging external resources, the agents can learn from code-related knowledge and store it\ninto their memory, thereby enhancing the capabilities of code generation. In addition, the memory\ncan improve the continuity and consistency in code generation. By integrating contextual memory,\nthe agent can better understand the requirements for software development, thereby enhancing the\ncoherence of the generated code. Furthermore, the memory is also crucial for the iterative optimization"
        ],
        [
            "Representative Studies. One of the most prominent studies is Reflexion [5], which proposes verbal\nreinforcement learning for LLM-based agents. It derives the experiences from past trials in verbal\nform, and applies them in subsequent trials to improve the performance of the same task. Furthermore,\nRetroformer [103] fine-tunes the reflection model, enabling the agent to extract cross-trial information\nfrom past trials more effectively. In Synapse [91], the agents focus on solving the computer control"
        ],
        [
            "In the context of conversation, consistency and engagement are two commonly used methods to\nevaluate the effectiveness of the agents’ memory. Consistency refers to how the response from agents\nis consistent with the context because dramatic changes should be avoided during the conversation.\nFor example, Lu et al. [94] evaluate the consistency of agents on interactive dialogues, using GPT-4\nto score on the responses from agents. Engagement refers to how the user is engaged to continue the"
        ],
        [
            "Minecraft Wiki and craft recipes to provide an infinite source of knowledge for their navigation.\nCodeAgent [114] focuses on the repo-level code generation task, which commonly requires complex\ndependencies and extensive documentation. It designs a web search strategy for acquiring related\nexternal knowledge. ChatDoctor [115] adapts LLM-based agents to the medical domain. It fine-tunes\nan acquisition process to retrieve external knowledge from Wikipedia and medical databases."
        ],
        [
            "In the field of recommendation, some previous works focus on simulating users in recommender\nsystems [95, 108], where the memory can represent the user profiles and histories in the real\nworld. Others try to improve the performance of recommendation, or provide other formats of\nrecommendation interfaces [149, 102]. Wang et al. [95] simulate user behaviors in recommendation\nscenarios to generate data for recommender systems, and the agents store past observations and"
        ],
        [
            "(LSH) to retrieve tuples with relative entries in the database to provide more information. In addition,\nChatDB [96] designs to utilize symbolic memory, and proposes to generate SQL statements to retrieve\nfrom database to obtain stored information."
        ],
        [
            "length. It thus requires much more computing resources and significantly increases inference latency,\nwhich hinders its practical deployment. What’s more, with its fast growth, the memory length can\neasily exceed the upper bound of the sequence length during LLM’s pretraining, which makes a\ntruncation of memory necessary. Thus, it can lead to information loss due to the incompleteness\nof agent memory. Last but not least, it can lead to biases and unrobustness in LLM’s inference."
        ],
        [
            "LLM-based agents are well-suited for creating personal assistants, such as agents capable of engaging\nin long-term conversations with users [94, 101, 153], as well as those tasked with automatically\nseeking information [164]. These agents often need to memorize previous dialogues to maintain\nthe consistency, and remember critical styles and events to generate more personalized and relevant\nresponses. Lu et al. [94] maintain the context consistency for dialogues by saving contents and"
        ],
        [
            "from past trials more effectively. In Synapse [91], the agents focus on solving the computer control\ntasks. Their memory can record cross-trial information through successful exemplars, which would\nbe used as references on similar trials. In ExpeL [82], the agents are required to solve a collection of\ncomplex interactive tasks within the environment. They store and organize completed trajectories,\nand recall similar ones for the new task. In the recalled trajectories, successful cases will be compared"
        ],
        [
            "Science. In the domain of science, some existing works design LLM-based agents with a large\namount of knowledge in memory to solve problems [158, 160, 162]. Chen et al. [158] include\nmolecule database and online literature as external knowledge for memory in LLM-based agents, and\nretrieve them when they need related information. Zhao et al. [160] and Chen et al. [162] empower\ndomain knowledge by fine-tuning in Chemistry and structured materials respectively."
        ],
        [
            "Applications. The surveys in this category aim to summarize models that leverage LLMs to improve\ndifferent applications. More concretely, Zhu et al. [37] focus on the field of information retrieval\n(IR) and summarize studies on LLM-based query processes. Xu et al. [38] pay more attention to\ninformation extraction (IE) and provide comprehensive taxonomies for LLM-based models in this\nfield. Li et al. [50], Lin et al. [51] and Wang et al. [52] discuss the applications of LLMs in the field"
        ],
        [
            "While storing all the agent-environment interactions can maintain comprehensive information, obvious\nlimitations exist in terms of computational cost, inference time, and inference robustness. Firstly,\nthe fast-growing long-context memory in practice results in high computational cost during LLM\ninference, due to the quadratic growth of the time complexity of attention computation with sequence\nlength. It thus requires much more computing resources and significantly increases inference latency,"
        ],
        [
            "and tool usage, in order to tackle intricate problems. The follow-up work [111] further improves\nits ability extensively like retrieval. In ToRA [128], the agents are required to solve mathematical\nproblems. They utilize imitation learning to improve their ability to use program-based tools."
        ],
        [
            "of agent memory. Last but not least, it can lead to biases and unrobustness in LLM’s inference.\nSpecifically, a previous research [120] has shown that, the positions of text segments in a long\ncontext can greatly affect their utilization, so the memory in the long-context prompt can not be\ntreated equally and stably. All these drawbacks show the need to design extra memory modules for\nLLM-based agents, rather than straightforwardly concatenating all the information into a prompt."
        ],
        [
            "F1 = 2 ·\n\nPrecision · Recall\nPrecision + Recall\n\n,\n\nwhere the precision and recall scores are calculated as Precision = TP\nTP+FN . The\nTP represents the number of true positive memory contents, FP means the number of false positive\nmemory contents, and FN indicates the number of false negative memory contents. In previous works,\nLu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al. [6]\nfocus on assessing whether related memory can be successfully retrieved."
        ],
        [
            "prompting with memory for computer control.\nDecision Making Workshop, 2023.\n\n[92] Ali Montazeralghaem, Hamed Zamani, and James Allan. A reinforcement learning framework\nfor relevance feedback. In Proceedings of the 43rd international acm sigir conference on\nresearch and development in information retrieval, pages 59–68, 2020."
        ],
        [
            "Knowledge editing methods provide an innovative way to update the information stored within the\nparameters of LLMs. By specifically targeting and adjusting the facts, these methods can ensure\nthe non-targeted knowledge unaffected during updates, thus mitigating the issue of catastrophic\nforgetting. Moreover, the targeted adjustment mechanism allows for more efficient and less resource-\nintensive updates, making knowledge editing an appealing choice for high-precision and real-time"
        ],
        [
            "For example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a\ncommon understanding among agents. In addition to cooperative scenarios, some studies also focus\non competitive scenarios, and the information asymmetry becomes a crucial issue [172]."
        ],
        [
            "In previous studies, there are various strategies to store recent textual memories. For example,\nSCM [98] proposes a flash memory based on the cache mechanism, which preserves observations from\nthe recent t − 1 time steps, aimed at enhancing the recency of information. MemGPT [100] considers\nthe agent as an operating system, which can dynamically interact with users through a natural interface.\nIt designs the working context to hold recent histories, as a part of virtual context management. In"
        ],
        [
            "provide insights for the design of memory functionalities in LLM-based agents, in this section, we\nreview and summarize how memory mechanisms are manifested in LLM-based agents across various\napplication scenarios. In specific, we categorize them into several classes: role-playing and social\nsimulation, personal assistant, open-world games, code generation, recommendation, expert systems\nin specific domains, and other applications. The summarization is shown in Table 4."
        ],
        [
            "Knowledge editing methods provide an innovative way to update the information stored within the\nparameters of LLMs. By specifically targeting and adjusting the facts, these methods can ensure\nthe non-targeted knowledge unaffected during updates, thus mitigating the issue of catastrophic\nforgetting. Moreover, the targeted adjustment mechanism allows for more efficient and less resource-\nintensive updates, making knowledge editing an appealing choice for high-precision and real-time"
        ],
        [
            "In previous studies, there are various strategies to store recent textual memories. For example,\nSCM [98] proposes a flash memory based on the cache mechanism, which preserves observations from\nthe recent t − 1 time steps, aimed at enhancing the recency of information. MemGPT [100] considers\nthe agent as an operating system, which can dynamically interact with users through a natural interface.\nIt designs the working context to hold recent histories, as a part of virtual context management. In"
        ],
        [
            "the memory writing is entirely self-directed. The agents can autonomously update the memory based\non the contexts. In MemoChat [94], the agents summarize each conversation segment by abstracting\nthe mainly discussed topics and storing them as keys for indexing memory pieces."
        ]
    ],
    "ground_truth": [
        "Memory-driven communication frameworks foster a common understanding among agents.",
        "The adaptation time refers to the time of memory writing and memory management.",
        "The challenges associated with storing information about agent-environment interactions include the token limitation of LLM prompts, which makes it hard for the agent to store extensive information, and the potential for information loss when transforming texts into parameters in parametric memory, along with the additional challenges brought by complex memory training.",
        "Typical agent applications demonstrate the importance of the memory module in different scenarios.",
        "Integrating domain-specific knowledge through fine-tuning methods benefits agents by enriching their domain-specific knowledge on top of their general knowledge. This approach significantly improves the agent's ability to accomplish domain-specific tasks.",
        "Some approaches used in recommendation systems include simulating users to represent user profiles and histories, improving the performance of recommendations, and providing different formats of recommendation interfaces. Specifically, Wang et al. simulate user behaviors to generate data for recommender systems.",
        "The context mentions a comprehensive survey on advancing transformer architecture in long-context large language models, but specific advancements are not detailed.",
        "Practical downstream scenarios contribute to the evaluation of memory in agents by providing broader approaches to assess the function of memory.",
        "Memory-driven communication frameworks foster a common understanding among agents.",
        "LLM-based agents are adapted for use in the medical domain by fine-tuning an acquisition process to retrieve external knowledge from Wikipedia and medical databases.",
        "The Faiss vector store acts as the pool of memory and is used to obtain the top-K successful trajectories that share the highest similarity scores with the current task.",
        "The two types of approaches for representing memory in parametric form are fine-tuning methods and memory editing methods.",
        "The knowledge, alignment, and safety control capabilities of LLMs are evaluated as described by Guo et al., who focus on these evaluation targets.",
        "Macroeconomic trends impact agents' decision-making in social network simulation systems by maintaining conversation contexts that encompass the economic environment and agent decisions from previous months. This allows the simulation to reflect the influence of broad macroeconomic trends on agents' decision-making and helps agents grasp market dynamics.",
        "The purpose of the agent memory pool in social network simulation systems is to consist of diverse user messages from online platforms to identify the user.",
        "The key problems in subjective evaluation are (1) what aspects should be evaluated and (2) how to conduct the evaluation process.",
        "The focus of the survey on multimodal large language models is not explicitly detailed in the context provided.",
        "In ChatDB, the memory reading operation is executed by the SQL statements, which are generated by agents as a series of Chain-of-Memory in advance.",
        "The purpose of using a flash memory cache mechanism in storing recent textual memories is to preserve observations from the recent t - 1 time steps, aimed at enhancing the recency of information.",
        "LLM inaccuracies, such as hallucination, can lead to misconceptions or fabrications, impacting their reliability for downstream applications. Bias and unfairness in LLMs can result in unequal treatment of different humans or objectives, propagating societal stereotypes and discrimination. These issues can be addressed by using mainstream models and methods summarized by various researchers to alleviate hallucination, bias, and unfairness.",
        "LLM agents can improve themselves by autonomously exploring and learning from the real world, such as sending requests and observing responses or adjusting behaviors based on user feedback.",
        "Reflexion uses verbal reinforcement learning to derive experiences from past trials in verbal form and applies them in subsequent trials to improve task performance. This approach is compared to other models like Generative Agents, which derive memory from historical behaviors, and MemoChat, which bases memory on conversation history.",
        "Measuring a memory module independently improves its reliability over task-based evaluations because it allows for the effectiveness of the memory module to be evaluated without the influence of other factors that can affect task performance, thus providing a more reliable evaluation result.",
        "The societal issues stemming from LLM bias include the unequal treatment of different humans or objectives, which can lead to the propagation of societal stereotypes and discrimination. Trust can be improved by addressing challenges such as hallucination, bias, unfairness, explainability, security, and privacy in LLMs.",
        "Agents use temporal caching for film suggestions by storing some temporal information in short-term memory as an intermediate cache, which can simulate the memory mechanism of the human brain.",
        "Domain-specific tuning could potentially lead to overfitting and raises concerns about catastrophic forgetting, where LLMs may forget the original knowledge because of updating their parameters.",
        "Memory read latency is measured as the inference time, which indicates the time latency of memory reading. It matters for computational efficiency because it contributes to the total time cost, which includes both memory adaptation and inference times.",
        "Contextual memory improves code consistency and refinement by enhancing the coherence of the generated code and improving the continuity and consistency in code generation. It helps the agent better understand the requirements for software development.",
        "Memory synchronization among agents is fundamental for establishing a unified knowledge base, ensuring consistency in decision-making across different modules. This enhances agent synchronization, enables more effective communication, and provides strategic advantages in information-rich environments, thereby boosting the adaptability and intelligence of multi-agent systems.",
        "Memory effectiveness in LLM agents is evaluated through direct evaluation methods, which include subjective and objective evaluations, and indirect evaluation methods, such as task completion. The intuition behind indirect evaluation is that if the agent can successfully complete a task that highly depends on memory, it suggests that the designed memory module is effective.",
        "Agents use public information from digital sources by acquiring external knowledge through tools, such as accessing information via Application Programming Interfaces (APIs), to transform additional relevant knowledge into their own memories for decision-making.",
        "True positives (TP) and false negatives (FN) are used to calculate precision and recall, which are components of the F1-score. The F1-score is utilized to evaluate the retrieval process of memory, as mentioned in the context.",
        "Memory consistency with roles boosts realism and task adherence in social simulations by ensuring that the memory aligns with the roles' characteristics, which helps in identifying and distinguishing each role. This consistency is crucial for improving the realism of role-play and ensuring that the agent consistently follows the role profiles, preventing the agent from stepping out of the role during the simulation process.",
        "Historical data is combined for memory in agent loops by storing four types of information: (1) complete agent-environment interactions, (2) recent agent-environment interactions, (3) retrieved agent-environment interactions, and (4) external knowledge. The memory leverages natural languages to describe the information within the agent-environment interaction loop.",
        "Memory trade-offs impact LLM inference cost and time by requiring textual memory to be integrated into the context prompt, leading to higher costs and longer processing times. In contrast, parametric memory integrates information into the parameters of the LLM, eliminating these extra costs, although it incurs additional costs in the writing process.",
        "Long-term memory affects an agent's exploration by allowing the agent to accumulate experiences from past interactions, which can be applied to subsequent exploration. It utilizes feedback from different trials to support a wider range of agent exploration.",
        "LLM-based agents use past experiences and resources to enhance exploration and code generation by maintaining post observations as task contexts, storing experiences from previous successful trials, and leveraging these past experiences to avoid repeating mistakes. This helps them achieve a high-level understanding of environments for more effective exploration. In code generation, they search relevant information from memory to obtain more knowledge for development, save previous experiences for future problems, and maintain context in conversational development interfaces.",
        "Using past interactions and external knowledge helps an agent adapt over trials by allowing the agent to reflect on past interactions and draw experiences that can be applied to subsequent exploration. This accumulation of experience enhances the exploratory capabilities of the agent, enabling it to adjust actions based on overall feedback from the whole process.",
        "The context does not provide a direct explanation of how synced memory modules boost message interpretation and decision consistency.",
        "Using external expertise, such as integrating domain-specific knowledge through supervised fine-tuning, boosts an agent's performance by enriching its domain-specific knowledge on top of its general knowledge. However, this process risks overfitting and memory loss, as fine-tuning LLMs for specific domains could lead to overfitting and raise concerns about catastrophic forgetting, where LLMs may forget the original knowledge due to parameter updates.",
        "Fine-tuning with domain expertise improves LLM agents' decision-making by enriching domain-specific knowledge on top of their general knowledge. This process, known as supervised fine-tuning, empowers agents with the memory of domain experts, significantly enhancing their ability to accomplish domain-specific tasks.",
        "Selective fact adjustments in LLMs, through knowledge editing methods, are more suitable for small-scale memory adjustments and provide a more efficient and less resource-intensive way to update information compared to pattern extraction methods like fine-tuning. Knowledge editing specifically targets and adjusts only the facts that need to be changed, ensuring unrelated knowledge remains unaffected and mitigating catastrophic forgetting.",
        "High-level information from past trials boosts an agent's adaptability in new settings by allowing the agent to summarize and abstract high-level information from raw observations, which serves as the basis for being more adaptive and generalizable to unseen environments.",
        "RAG improves LLMs' accuracy and reduces errors by providing them with factual and up-to-date knowledge, which helps in removing hallucinations.",
        "Lifelong learning in LLM-based agents faces challenges such as the need for the agent's memory to capture temporality, which could cause interactions between memories, like memory overlap.",
        "LLM misconceptions, referred to as hallucinations, impact reliability by generating misconceptions or fabrications, which affect their reliability for downstream applications. Zhang et al., Huang et al., Rawte et al., Ye et al., Ji et al., Tonmoy et al., and Jiang et al. summarize the mainstream models for alleviating the hallucination problem in LLMs.",
        "Bubble sort repeatedly steps through the list, compares adjacent elements and swaps them if they are in wrong order.",
        "RAG boosts LLMs' accuracy and domain adaptation by providing them with factual and up-to-date knowledge and removing hallucinations.",
        "The key questions and design factors crucial for memory in LLM agents include 'What is', 'Why do we need', and 'How to design and evaluate' the memory module in LLM-based agents.",
        "TP (True Positives) represent the number of true positive memory contents, FP (False Positives) represent the number of false positive memory contents, and FN (False Negatives) indicate the number of false negative memory contents. These metrics are used to calculate precision and recall, which are then used to compute the F1-score for evaluating the retrieval process of memory.",
        "Memory models aid agent synchronization and strategy in multi-agent systems (MAS) by enhancing communication and providing strategic advantages in information-rich environments. They facilitate more robust, intelligent, and adaptable MAS by addressing challenges of memory integration and management, and by exploring the potentials of memory in these systems.",
        "LLM-based agents use past interactions to improve decisions by accumulating information across multiple trials, which includes successful and failed actions, insights such as failure reasons, and common action patterns to succeed.",
        "The memory module helps to accurately simulate human dynamic behaviors by maintaining conversation contexts, encompassing the economic environment and agent decisions from previous months, in order to simulate the impact of broad macroeconomic trends on agents’ decision-making and to make the agents grasp market dynamics.",
        "LLM agents use concatenated sequences for better memory in personalized dialogues by concatenating context QA pairs to form episode memory, which helps in maintaining context consistency and generating more personalized and relevant responses.",
        "Lifelong learning in LLM-based agents faces challenges such as capturing temporality in an agent's memory, which could cause interactions between memories, like memory overlap.",
        "LLM-based agents boost code coherence and error fixing by searching relevant information from memory, maintaining context in conversational development interfaces, and leveraging external resources. They store compiler errors and human expert instructions in an external non-parametric memory database for automatic syntax error fixing, enhancing the capabilities of code generation and improving continuity and consistency.",
        "Reflexion uses past trials by deriving experiences in verbal form and applying them in subsequent trials to improve the performance of the same task.",
        "GPT-4 is used to score the consistency of agents on interactive dialogues.",
        "CodeAgent focuses on the repo-level code generation task, which commonly requires complex dependencies and extensive documentation.",
        "Memory can represent user profiles and histories, and simulation of user behaviors can generate data for recommender systems.",
        "ChatDB is designed to utilize symbolic memory and proposes to generate SQL statements to retrieve information from the database, thereby boosting data retrieval.",
        "The deployment issue for LLMs with more computing needs and memory limits includes requiring much more computing resources, significantly increasing inference latency, and the necessity of truncating memory due to exceeding the upper bound of sequence length during pretraining, which can lead to information loss and biases in inference.",
        "LLM-based agents excel in dialogue and information gathering, as they are well-suited for creating personal assistants capable of engaging in long-term conversations and automatically seeking information.",
        "Agents boost efficiency with past successes by recording cross-trial information through successful exemplars, which are used as references on similar trials. They store and organize completed trajectories and recall similar ones for new tasks, comparing successful cases.",
        "LLMs get molecule data by including a molecule database as external knowledge for memory in LLM-based agents, which can be retrieved when they need related information.",
        "LLMs aid in the areas of information retrieval (IR) and information extraction (IE).",
        "Quadratic growth in attention impacts latency by significantly increasing inference latency due to the high computational cost associated with the fast-growing long-context memory, which requires much more computing resources.",
        "Imitation learning boosts agents' tool use in math.",
        "The positions of text segments in a long context can greatly affect their utilization, so the memory in the long-context prompt cannot be treated equally and stably.",
        "Memory retrieval scores are derived using TP (true positives), FP (false positives), and FN (false negatives) by calculating precision and recall. Precision is calculated as TP / (TP + FP), and recall is calculated as TP / (TP + FN). The F1-score is then calculated as 2 * (Precision * Recall) / (Precision + Recall).",
        "A reinforcement learning framework for relevance feedback.",
        "Targeted LLM tweaks avoid losing other information by specifically targeting and adjusting the facts, ensuring that non-targeted knowledge remains unaffected during updates, thus mitigating the issue of catastrophic forgetting.",
        "Memory-driven communication frameworks foster a common understanding among agents, which helps them cooperate.",
        "MemGPT considers the agent as an operating system, which can dynamically interact with users through a natural interface. It designs the working context to hold recent histories, as a part of virtual context management.",
        "LLM agents use memory for suggestions in scenarios such as role-playing and social simulation, personal assistant, open-world games, code generation, recommendation, expert systems in specific domains, and other applications.",
        "LLM tweaks, through knowledge editing methods, stop forgetting during updates by specifically targeting and adjusting the facts, ensuring that non-targeted knowledge remains unaffected, thus mitigating the issue of catastrophic forgetting.",
        "SCM proposes a flash memory based on the cache mechanism, which preserves observations from the recent t - 1 time steps, aimed at enhancing the recency of information.",
        "In MemoChat, the agents summarize each conversation segment by abstracting the mainly discussed topics and storing them as keys for indexing memory pieces."
    ],
    "evolution_type": [
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "simple",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "multi_context",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning",
        "reasoning"
    ],
    "metadata": [
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            },
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ],
        [
            {
                "source": "data/mixed_documents/memory_survey.pdf"
            }
        ]
    ],
    "episode_done": [
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true,
        true
    ]
}