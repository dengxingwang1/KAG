{"question": ["What advancements in retrieval accuracy are achieved by Dense Hierarchical Retrieval (DHR)?", "What is the purpose of fine-tuning the REALM model for open-domain question answering?", "How does the use of multiple clusters benefit the clustering approach in text segmentation?", "What challenges do aspiring writers face in finding a foothold in the industry?", "Who is one of the authors of the paper titled \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\"?", "How does RAPTOR utilize subtopics and hierarchical structures to enhance reading comprehension?", "What was the purpose of conducting an ablation study on the RAPTOR approach?", "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?", "What is a limitation of existing retrieval-augmented approaches in handling thematic questions?", "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?", "Why is selecting the most relevant information important for knowledge-intensive tasks?", "What do the empirical observations suggest about the effectiveness of the Gaussian assumption in GMMs for modeling text data?", "What role does gpt-3.5-turbo play in model-based summarization?", "How is cosine similarity used in tree traversal and collapsed tree retrieval mechanisms?", "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?", "Who helped publish research on long-range context in language models at EMNLP 2021?", "How does RAPTOR use iterative text grouping for efficient text retrieval?", "How does a magical figure aid a character's journey from hardship to fulfillment in a story of a lost item and family forgiveness?", "How do retrieval advances and passage processing boost RALM reader function?", "How does a hierarchical tree boost LLMs' knowledge synthesis and retrieval?", "Does RAPTOR boost retrieval on QuALITY and work similarly on datasets like NarrativeQA?", "How does DPR/RAPTOR affect UnifiedQA-3B's retrieval in QuALITY/QASPER?", "How does a dimensionality reduction method balancing local/global structures overcome traditional GMM limits in high-D spaces?", "How does text grouping in RAPTOR trees boost efficiency?", "Which datasets assess RAPTOR's QA?", "How does RAPTOR+GPT-4 impact QuALITY & QASPER benchmarks?", "How does linear token scaling improve RAPTOR's handling of complex data?", "How does the token threshold affect node selection in the Collapsed Tree Algorithm?", "How does gpt-3.5-turbo aid in clustering and summarization for doc structuring?", "What limits retrieval methods in fully grasping themes in datasets like NarrativeQA?", "How do Gaussian assumptions aid GMM in skewed text clustering?", "How does RAPTOR's synthesis boost query efficiency?", "What challenges exist in tracing LLMs' knowledge origins vs. retrieval systems?", "Which 2020 EMNLP paper by Colin Raffel examines language model capacity, and how does his later work tackle long-tail knowledge issues?", "How does Cinderella get to the ball, and when does the magic end?", "How does Cinderella get to the ball, and what's the time limit?", "How does RAPTOR's recursive tree aid in finding distant semantic links in long texts?", "How does Wu et al.'s 2021 model use task decomposition for summarization, possibly missing details?", "How does the scalar product affect node choice in a single-layer tree?", "How does Cinderella get to the ball, and what's the time limit?", "How does BIC weigh complexity vs. fit for optimal clusters in GMM?", "How do extended contexts challenge models, and can recursive summarization help?", "How do networking platforms help new writers find connections and mentors?", "How do input dimensions and cluster count affect GMM parameters, with BIC in model selection?", "How does text segmentation improve METEOR's accuracy over basic splits in literary reviews?", "How does RAPTOR affect retrieval with UnifiedQA-3B?", "Where was the ACL's 57th meeting with Transformer-XL in July 2019?", "Which API accesses GPT for QA & summarization?", "Which metrics, like B-1 & B-4, evaluate literary dataset performance?", "How does recursive summarization boost context in retrieval models?", "How does collective node evaluation differ from layer-by-layer in tree queries?", "How do LLMs' size and params boost performance?", "How does text summarization improve retrieval for UnifiedQA, GPT-3, and GPT-4?", "How does RAPTOR's linear scaling boost efficiency with large corpora?", "How to keep a cluster's context within token limits?", "How does 4-gram focus impact BLEU-4 balance?", "What's the chunking method for LLM context in QA?", "How does RAPTOR's scaling boost large corpus processing?", "How's the Prince's search linked to finding the glass slipper?", "How's top-k node retrieval linked to cosine similarity in trees?", "Which task tests a model's understanding of full narratives?", "Which 1972 work by K. Spärck Jones affects term specificity in retrieval?", "How does retrieval augmentation use text chunks in QA?", "Which method helps language models adapt by using hierarchical text summaries?", "How does RAPTOR's tree help with text efficiency?", "Who helped publish UNIFIEDQA at EMNLP 2020?", "How does Atlas refine an encoder-decoder with a retriever in end-to-end training?", "Why does soft clustering's flexibility matter for text relevance?", "How do retrieval systems clarify and trace sources vs. LLMs?", "How does Cinderella magically attend the ball and impress the Prince?", "How does RAPTOR's tree structure improve retrieval?"], "contexts": [["retriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-\ndecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-\nerarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023)."], ["End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\ndecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-"], ["One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\ntial because individual text segments often contain information relevant to various topics, thereby\nwarranting their inclusion in multiple summaries."], ["world of storytelling. However, like many aspiring writers, he struggled to find a\nfoothold in the industry. He took a job as a content writer for an online marketing\nfirm, but it was growing increasingly evident to him that this was not the path he\nwanted to pursue. It was during this time that he stumbled upon the Pathways\napp. The app offered a platform for people in similar professions to connect and\nshare knowledge, and he saw it as an opportunity to finally connect with others"], ["Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation\nand Projection for Dimension Reduction, 2018. URL https://arxiv.org/abs/1802.\n03426. arXiv preprint arXiv:1802.03426."], ["3 METHODS\n\nOverview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi-\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\nilarity not just order in the text."], ["B ABLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\n\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\nan ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\nclustering method.\n\nB.1 METHODOLOGY"], ["E.3\n\nIMPACT ON QA TASKS\n\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\n\nF PSEUDOCODE FOR RETRIEVAL METHODS\n\nAlgorithm 1 Tree Traversal Algorithm\n\nfunction TRAVERSETREE(tree, query, k)\n\nScurrent ← tree.layer[0]\nfor layer in range(tree.num layers) do\n\ntopk ← []\nfor node in Scurrent do\n\nscore ← dot product(query, node)\ntop k.append((node, score))"], ["Nevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\nan entire book, as in the NarrativeQA dataset (Koˇcisk`y et al., 2018). Consider the fairy tale of"], ["E.3\n\nIMPACT ON QA TASKS\n\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\n\nF PSEUDOCODE FOR RETRIEVAL METHODS\n\nAlgorithm 1 Tree Traversal Algorithm\n\nfunction TRAVERSETREE(tree, query, k)\n\nScurrent ← tree.layer[0]\nfor layer in range(tree.num layers) do\n\ntopk ← []\nfor node in Scurrent do\n\nscore ← dot product(query, node)\ntop k.append((node, score))"], ["text length increases, especially when pertinent information is embedded within a lengthy context.\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\nmost relevant information for knowledge-intensive tasks is still crucial."], ["While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\ncontiguous chunks and provide details in Appendix B."], ["Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the\nnodes in each cluster are sent to a language model for summarization. This step allows the model\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\ndenses the potentially large volume of retrieved information into a manageable size. We provide"], ["Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The"], ["E.3\n\nIMPACT ON QA TASKS\n\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug-\ngests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\n\nF PSEUDOCODE FOR RETRIEVAL METHODS\n\nAlgorithm 1 Tree Traversal Algorithm\n\nfunction TRAVERSETREE(tree, query, k)\n\nScurrent ← tree.layer[0]\nfor layer in range(tree.num layers) do\n\ntopk ← []\nfor node in Scurrent do\n\nscore ← dot product(query, node)\ntop k.append((node, score))"], ["Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\nmodels actually use long-range context?\nIn Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pp. 807–822, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n62. URL https://aclanthology.org/2021.emnlp-main.62."], ["To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build", "To address this, we design an indexing and retrieval system that uses a tree structure to capture both\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\nthe bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n\n1"], ["and others at the ball.” This answer only takes into account the first portion of the story, up until\nCinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\nyoung woman who ultimately finds happiness and love with the Prince.” This is a more complete", "Question: What is the central theme of the story?\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper."], ["Retrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and\nBM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using", "et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\n2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\nindependently in the encoder and RETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes"], ["In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\nparametric knowledge of large language models with contextual information at various levels of\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\nhierarchical tree structure that is capable of synthesizing information across various sections of the\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective"], ["7\n\n\fPublished as a conference paper at ICLR 2024\n\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\nforms baselines of each respective retrieval method for both datasets.\n\nModel\n\nAccuracy (QuALITY) Answer F1 (QASPER)", "Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\nspective retrieval method.\n\nModel\n\nROUGE BLEU-1\n\nBLEU-4 METEOR\n\nSBERT with RAPTOR\nSBERT without RAPTOR\nBM25 with RAPTOR\nBM25 without RAPTOR\nDPR with RAPTOR\nDPR without RAPTOR"], ["Controlled Baseline Comparisons We first present controlled comparisons using the UnifiedQA\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY. As shown in Tables 1 and 2,\n\n6\n\n\fPublished as a conference paper at ICLR 2024", "7\n\n\fPublished as a conference paper at ICLR 2024\n\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\nforms baselines of each respective retrieval method for both datasets.\n\nModel\n\nAccuracy (QuALITY) Answer F1 (QASPER)"], ["The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis-\ntance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Ag-\ngarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\nnumber of nearest neighbors parameter, n neighbors, in UMAP determines the balance between", "number of nearest neighbors parameter, n neighbors, in UMAP determines the balance between\nthe preservation of local and global structures. Our algorithm varies n neighbors to create a hierar-\nchical clustering structure: it first identifies global clusters and then performs local clustering within\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\namong the text data, from broad themes to specific details."], ["Clustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text\nsegments into cohesive groups. This step groups related content together, which helps the subse-\nquent retrieval process.", "To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build"], ["4 EXPERIMENTS\n\nDatasets We measure RAPTOR’s performance across three question-answering datasets: Narra-\ntiveQA, QASPER, and QuALITY.", "Evaluation Datasets The three evaluation datasets used in our experiments—QuALITY,\nQASPER, and NarrativeQA—are all publicly accessible. These datasets ensure that the retrieval\nand QA tests conducted in this study can be replicated.\n\nSource Code The source code for RAPTOR will be publicly available here.\n\nREFERENCES"], ["In the QuALITY dataset, as shown in Table 7,\nRAPTOR paired with GPT-4 sets a new state-\nof-the-art with an accuracy of 82.6%, surpass-\ning the previous best result of 62.3%. In par-\nticular, it outperforms CoLISA by 21.5% on\nQuALITY-HARD, which represents questions\nthat humans took unusually long to correctly\nanswer, requiring rereading parts of the text,\ndifficult reasoning, or both.", "Retriever\n\nGPT-3 F-1 Match GPT-4 F-1 Match UnifiedQA F-1 Match\n\nTitle + Abstract\nBM25\nDPR\nRAPTOR\n\n25.2\n46.6\n51.3\n53.1\n\n22.2\n50.2\n53.0\n55.7\n\n17.5\n26.4\n32.1\n36.6\n\nComparison to State-of-the-art Systems\nBuilding upon our controlled comparisons,\nwe examine RAPTOR’s performance relative\nto other state-of-the-art models. As shown\nin Table 5, RAPTOR with GPT-4 sets a new\nbenchmark on QASPER, with a 55.7% F-1\nscore, surpassing the CoLT5 XL’s score of\n53.9%."], ["aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A.", "Conclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-\ntionally efficient and well-suited for processing large and diverse corpora.\n\nB ABLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR"], ["18\n\n\fPublished as a conference paper at ICLR 2024\n\nAlgorithm 2 Collapsed Tree Algorithm\n\nfunction COLLAPSEDTREE(tree, query, k, max tokens)\n\ntree ← flatten(tree)\ntop nodes ← []\nfor node in tree do\n\ntop nodes.append((node, dot product(query, node))\n\nend for\ntop nodes ← sorted(top nodes)\nresult ← []\ntotal tokens ← 0\nfor node in top nodes do\n\nif total tokens + node.token size < max tokens then\n\nresult.append(node)\n\nend if\ntotal tokens ← total tokens + node.token size\n\nend for\nreturn result\n\nend function", "Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The"], ["Model-Based Summarization After clustering the nodes using Gaussian Mixture Models, the\nnodes in each cluster are sent to a language model for summarization. This step allows the model\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step con-\ndenses the potentially large volume of retrieved information into a manageable size. We provide", "To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build"], ["an entire book, as in the NarrativeQA dataset (Koˇcisk`y et al., 2018). Consider the fairy tale of\nCinderella, and the question “How did Cinderella reach her happy ending?”. The top-k retrieved\nshort contiguous texts will not contain enough context to answer the question.", "Nevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\nan entire book, as in the NarrativeQA dataset (Koˇcisk`y et al., 2018). Consider the fairy tale of"], ["Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\nmixture of several Gaussian distributions.\n\n3\n\n\fPublished as a conference paper at ICLR 2024", "While the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\noften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\ncontiguous chunks and provide details in Appendix B."], ["retrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.", "In this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\nparametric knowledge of large language models with contextual information at various levels of\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\nhierarchical tree structure that is capable of synthesizing information across various sections of the\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective"], ["question as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).", "particularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,"], ["Adam Roberts, Colin Raffel, and Noam Shazeer. How Much Knowledge Can You Pack Into\nIn Proceedings of the 2020 Conference on Empir-\nthe Parameters of a Language Model?\nical Methods in Natural Language Processing (EMNLP), pp. 5418–5426, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL\nhttps://aclanthology.org/2020.emnlp-main.437.", "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\nModels struggle to learn Long-Tail Knowledge. In International Conference on Machine Learn-\ning, pp. 15696–15707. PMLR, 2023. URL https://proceedings.mlr.press/v202/\nkandpal23a/kandpal23a.pdf."], ["Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it", "Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it"], ["Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it", "Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it"], ["intermediate nodes thus storing varying levels of detail, keeping granular details. However, both\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\noverlook distant interdependencies within the text, which we can find and group with RAPTOR.", "3 METHODS\n\nOverview of RAPTOR Building on the idea that long texts often present subtopics and hierarchi-\ncal structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\ncomprehension with granular details and which allows nodes to be grouped based on semantic sim-\nilarity not just order in the text."], ["Recursive summarization as Context Summarization techniques provide a condensed view of\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition", "The recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both"], ["18\n\n\fPublished as a conference paper at ICLR 2024\n\nAlgorithm 2 Collapsed Tree Algorithm\n\nfunction COLLAPSEDTREE(tree, query, k, max tokens)\n\ntree ← flatten(tree)\ntop nodes ← []\nfor node in tree do\n\ntop nodes.append((node, dot product(query, node))\n\nend for\ntop nodes ← sorted(top nodes)\nresult ← []\ntotal tokens ← 0\nfor node in top nodes do\n\nif total tokens + node.token size < max tokens then\n\nresult.append(node)\n\nend if\ntotal tokens ← total tokens + node.token size\n\nend for\nreturn result\n\nend function", "Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The"], ["Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it", "Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it"], ["value of the likelihood function of the model. In the context of GMM, the number of parameters k\nis a function of the dimensionality of the input vectors and the number of clusters.", "To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N )k − 2 ln( ˆL), where N is the number\nof text segments (or data points), k is the number of model parameters, and ˆL is the maximized\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k"], ["Why Retrieval?\nRecent advances in hardware and algorithms have indeed expanded the con-\ntext lengths that models can handle, leading to questions about the need for retrieval systems (Dai\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\nhave noted, models tend to underutilize long-range context and see diminishing performance as con-\ntext length increases, especially when pertinent information is embedded within a lengthy context.", "Retrieval-augmented language models can better adapt to changes in world state\nand incorporate long-tail knowledge. However, most existing methods retrieve\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\nstanding of the overall document context. We introduce the novel approach of\nrecursively embedding, clustering, and summarizing chunks of text, constructing\na tree with differing levels of summarization from the bottom up. At inference"], ["world of storytelling. However, like many aspiring writers, he struggled to find a\nfoothold in the industry. He took a job as a content writer for an online marketing\nfirm, but it was growing increasingly evident to him that this was not the path he\nwanted to pursue. It was during this time that he stumbled upon the Pathways\napp. The app offered a platform for people in similar professions to connect and\nshare knowledge, and he saw it as an opportunity to finally connect with others", "share knowledge, and he saw it as an opportunity to finally connect with others\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\nshared his passion and could offer guidance and mentorship. He quickly signed\nup and was surprised by the number of writers he found on the platform, from\nwell establish professionals to beginners just starting out in the business.”"], ["value of the likelihood function of the model. In the context of GMM, the number of parameters k\nis a function of the dimensionality of the input vectors and the number of clusters.", "To determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\n(Schwarz, 1978). The BIC for a given GMM is BIC = ln(N )k − 2 ln( ˆL), where N is the number\nof text segments (or data points), k is the number of model parameters, and ˆL is the maximized\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters k"], ["• Tokenization before Mapping in METEOR Calculation: The original script utilized a\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\ncalculation by taking into account the correct linguistic boundaries of words."], ["7\n\n\fPublished as a conference paper at ICLR 2024\n\nTable 2: QuALITY and QASPER Performance With + Without RAPTOR: Performance com-\nparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outper-\nforms baselines of each respective retrieval method for both datasets.\n\nModel\n\nAccuracy (QuALITY) Answer F1 (QASPER)", "Table 1: NarrativeQA Performance With + Without RAPTOR: Performance comparison of\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each re-\nspective retrieval method.\n\nModel\n\nROUGE BLEU-1\n\nBLEU-4 METEOR\n\nSBERT with RAPTOR\nSBERT without RAPTOR\nBM25 with RAPTOR\nBM25 without RAPTOR\nDPR with RAPTOR\nDPR without RAPTOR"], ["Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\nhttps://aclanthology.org/P19-1285."], ["9\n\n\fPublished as a conference paper at ICLR 2024\n\n6 REPRODUCIBILITY STATEMENT\n\nLanguage Models for QA and Summarization Four language models are used in our RAPTOR\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\ngpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,\nwhich is used for QA tasks, is publicly available at Hugging Face."], ["the literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the Narra-\ntiveQA evaluation script used in our experiments."], ["Retrieval-augmented language models can better adapt to changes in world state\nand incorporate long-tail knowledge. However, most existing methods retrieve\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\nstanding of the overall document context. We introduce the novel approach of\nrecursively embedding, clustering, and summarizing chunks of text, constructing\na tree with differing levels of summarization from the bottom up. At inference"], ["For querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\nthe most relevant ones."], ["Large Language Models (LLMs) have emerged as transformative tools showing impressive perfor-\nmance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream"], ["Our main contribution is the idea of using text summarization to allow retrieval augmentation of\ncontext at different scales, and to show its effectiveness in experiments on collections of long doc-\numents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-"], ["aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A."], ["Should a local cluster’s combined context ever exceed the summarization model’s token threshold,\nour algorithm recursively applies clustering within the cluster, ensuring that the context remains\nwithin the token threshold."], ["• Modified BLEU-4 Weighting: The original script applied a weight of 1 to the highest\norder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\n0.25)."], ["particularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,"], ["aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\ncomprehensive discussion on RAPTOR’s scalability, please refer to the Appendix A."], ["Question: What is the central theme of the story?\nRAPTOR Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella\nimpresses the Prince at the ball. . . she loses track of time and has to run home alone in\nthe darkness. The Prince is unable to find Cinderella and goes in search of her . . . She\nforgave her sisters, and treated them always very kindly, and the Prince had great cause\nto be glad that he had found the glass slipper."], ["Figure 2: Illustration of the tree traversal and collapsed tree retrieval mechanisms. Tree traver-\nsal starts at the root level of the tree and retrieves the top-k (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top-k node(s) from the child nodes of\nthe previous layer’s top-k. Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The"], ["NarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\nand movie transcripts, totaling 1,572 documents (Koˇcisk`y et al., 2018; Wu et al., 2021). The\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\nto accurately answer its questions, thus testing the model’s ability to comprehend longer texts in\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),"], ["Gideon Schwarz. Estimating the Dimension of a Model. The annals of statistics, pp. 461–464,\n1978. URL https://projecteuclid.org/journals/annals-of-statistics/\nvolume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/\naos/1176344136.full.\n\nKaren Sp¨arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re-\ntrieval. Journal of documentation, 28(1):11–21, 1972. URL https://doi.org/10.1108/\neb026526."], ["particularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-\nnative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\nquestion as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,"], ["Retrieval-augmented language models can better adapt to changes in world state\nand incorporate long-tail knowledge. However, most existing methods retrieve\nonly short contiguous chunks from a retrieval corpus, limiting holistic under-\nstanding of the overall document context. We introduce the novel approach of\nrecursively embedding, clustering, and summarizing chunks of text, constructing\na tree with differing levels of summarization from the bottom up. At inference"], ["To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\nof embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build", "To address this, we design an indexing and retrieval system that uses a tree structure to capture both\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\nthe bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n\n1"], ["Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\nIn Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.171. URL https://aclanthology.org/2020.findings-emnlp.\n171."], ["End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-\ndecoder model in conjunction with the retriever; REALM (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-\ntion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-"], ["One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen-\ntial because individual text segments often contain information relevant to various topics, thereby\nwarranting their inclusion in multiple summaries."], ["question as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022)."], ["Question: How does Cinderella find a happy ending?\nRAPTOR The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin\ninto a grand coach with her wand and allows Cinderella to attend the ball. However,\nCinderella must return home before the clock strikes eleven or her dress will turn back\ninto rags. . . Cinderella impresses the Prince at the ball but leaves before he can find\nout who she is. . . The Prince searched for the owner of a lost glass slipper and found it"], ["retrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\nretrieval methods but also sets new performance benchmarks on several question-answering tasks."]], "ground_truth": ["Dense Hierarchical Retrieval (DHR) represents advancements in retrieval accuracy by combining document and passage level retrievals and integrating sparse and dense retrieval methods.", "The purpose of fine-tuning the REALM model is for open-domain question answering.", "The use of multiple clusters benefits the clustering approach in text segmentation by allowing nodes to belong to multiple clusters, which is essential because individual text segments often contain information relevant to various topics. This flexibility warrants their inclusion in multiple summaries.", "Aspiring writers often struggle to find a foothold in the industry, as exemplified by the individual in the context who had to take a job as a content writer for an online marketing firm despite it not being the path he wanted to pursue.", "Leland McInnes is one of the authors of the paper titled \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\".", "RAPTOR enhances reading comprehension by building a recursive tree structure that balances broader thematic comprehension with granular details, allowing nodes to be grouped based on semantic similarity rather than just order in the text.", "The purpose of conducting an ablation study on the RAPTOR approach was to assess the effectiveness of the clustering mechanism.", "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.", "A limitation of existing retrieval-augmented approaches in handling thematic questions is that they retrieve only a few short, contiguous text chunks, which limits their ability to represent and leverage large-scale discourse structure.", "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.", "Selecting the most relevant information is important for knowledge-intensive tasks because it helps manage text length, making it less expensive and faster to process, especially when pertinent information is embedded within a lengthy context.", "The empirical observations suggest that the Gaussian assumption in GMMs offers an effective model for the purpose, despite not perfectly aligning with the nature of text data.", "gpt-3.5-turbo is used to generate summaries in the model-based summarization process.", "Cosine similarity is used in tree traversal to retrieve the top-k nodes at each level based on their similarity to the query vector. In collapsed tree retrieval, nodes are retrieved until a threshold number of tokens is reached, also based on cosine similarity to the query vector.", "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.", "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer helped publish research on long-range context in language models at EMNLP 2021.", "RAPTOR uses iterative text grouping by employing a clustering algorithm to group similar text chunks. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents.", "The Fairy Godmother helps Cinderella attend a ball by transforming her rags, which aids her journey from hardship to fulfillment. This transformation allows Cinderella to impress the Prince at the ball, leading to her eventual happiness and love with him. Additionally, Cinderella forgives her sisters and treats them kindly, which is part of her journey towards fulfillment.", "Retrieval advances and passage processing boost RALM reader function by employing methods like Fusion-in-Decoder (FiD), which uses both DPR and BM25 for retrieval and processes passages independently in the encoder.", "RAPTOR creates a hierarchical tree structure by employing recursive clustering and summarization techniques, which synthesizes information across various sections of the retrieval corpora. This tree structure is leveraged during the query phase for more effective retrieval.", "RAPTOR outperforms baselines of each respective retrieval method for both QuALITY and QASPER datasets, and similarly outperforms baselines on the NarrativeQA dataset.", "RAPTOR outperforms baselines of each respective retrieval method for both datasets, including DPR, when used with UnifiedQA-3B.", "The dimensionality reduction method, specifically UMAP, overcomes traditional GMM limits in high-dimensional spaces by varying the number of nearest neighbors parameter, n neighbors, to balance the preservation of local and global structures. This approach allows for a hierarchical clustering structure that first identifies global clusters and then performs local clustering within these global clusters, capturing a broad spectrum of relationships among the text data.", "Text grouping in RAPTOR trees boosts efficiency by organizing text segments into cohesive groups, which aids the subsequent retrieval process. This structured, multi-layered tree representation allows for efficient summarization and re-embedding, enhancing computational efficiency.", "RAPTOR's QA is assessed using three datasets: NarrativeQA, QASPER, and QuALITY.", "RAPTOR paired with GPT-4 sets a new state-of-the-art on the QuALITY dataset with an accuracy of 82.6%, surpassing the previous best result of 62.3%. It outperforms CoLISA by 21.5% on QuALITY-HARD. On the QASPER benchmark, RAPTOR with GPT-4 sets a new benchmark with a 55.7% F-1 score, surpassing the CoLT5 XL's score of 53.9%.", "The context indicates that RAPTOR scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. This linear scaling ensures that even as the complexity and volume of the input text grow, the cost of constructing the tree scales predictably and linearly, demonstrating computational efficiency.", "The token threshold affects node selection in the Collapsed Tree Algorithm by limiting the total number of tokens that can be included in the result. Nodes are added to the result until the total token size reaches the maximum token threshold.", "GPT-3.5-turbo aids in clustering and summarization by generating summaries of the nodes in each cluster after they have been grouped using Gaussian Mixture Models. This helps transform large chunks of text into concise, coherent summaries, which are then re-embedded and further processed until a structured, multi-layered tree representation of the original documents is achieved.", "Most existing methods retrieve only a few short, contiguous text chunks, which limits their ability to represent and leverage large-scale discourse structure.", "The Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, but empirical observations suggest that it offers an effective model for the purpose of clustering.", "RAPTOR boosts query efficiency by leveraging a hierarchical tree structure created through recursive clustering and summarization techniques. This structure synthesizes information across various sections of the retrieval corpora, allowing for more effective retrieval during the query phase.", "The challenges in tracing LLMs' knowledge origins include the opacity and difficulty in tracing back to its source, whereas retrieval systems provide easy interpretability and provenance tracking.", "The 2020 EMNLP paper by Colin Raffel is titled 'How Much Knowledge Can You Pack Into the Parameters of a Language Model?' and his later work, 'Large Language Models struggle to learn Long-Tail Knowledge,' addresses issues related to long-tail knowledge.", "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand. The magic ends when the clock strikes eleven, as Cinderella must return home before then or her dress will turn back into rags.", "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach. She must return home before the clock strikes eleven, or her dress will turn back into rags.", "RAPTOR builds a recursive tree structure that balances broader thematic comprehension with granular details, allowing nodes to be grouped based on semantic similarity, not just order in the text.", "Wu et al.'s 2021 model uses task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sections. While this method is effective for capturing broader themes, it can miss granular details.", "The scalar product (dot product) is used to determine the similarity between the query and each node in the tree. Nodes are then sorted based on this similarity, and the top nodes are selected based on their scalar product values.", "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand. The time limit is that Cinderella must return home before the clock strikes eleven, or her dress will turn back into rags.", "BIC penalizes model complexity while rewarding goodness of fit, helping to determine the optimal number of clusters in GMM.", "Models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context. Recursive summarization can help by embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up.", "Networking platforms like the Pathways app help new writers find connections and mentors by offering a platform for people in similar professions to connect and share knowledge. This allows aspiring writers to meet others who share their passion and can offer guidance and mentorship.", "In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of clusters. The Bayesian Information Criterion (BIC) is used for model selection, which penalizes model complexity and rewards goodness of fit.", "Text segmentation improves METEOR's accuracy by tokenizing the text before mapping the tokens, which takes into account the correct linguistic boundaries of words, unlike a simple split method.", "RAPTOR outperforms baselines of each respective retrieval method for both QuALITY and QASPER datasets, as well as the NarrativeQA dataset, when using UnifiedQA-3B as the language model.", "The ACL's 57th meeting with Transformer-XL in July 2019 was held in Florence, Italy.", "The GPT-3, GPT-4, and GPT-3.5-turbo models can be accessed via API calls (OpenAI API).", "The metrics that evaluate literary dataset performance are BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M).", "Recursive summarization boosts context in retrieval models by embedding, clustering, and summarizing chunks of text to construct a tree with differing levels of summarization from the bottom up.", "Collective node evaluation, as used in the collapsed tree method, differs from layer-by-layer evaluation in that it evaluates nodes collectively across all layers to find the most relevant ones, rather than traversing the tree layer-by-layer and selecting nodes at each level.", "With the growing size of LLMs, they can serve standalone as very effective knowledge stores, with facts encoded within their parameters.", "Text summarization improves retrieval for UnifiedQA, GPT-3, and GPT-4 by allowing retrieval augmentation of context at different scales, which has been shown to be effective in experiments on collections of long documents.", "RAPTOR's linear scaling boosts efficiency with large corpora by ensuring that both build time and token expenditure increase linearly, making it suitable for processing large and complex corpora.", "To keep a cluster's context within token limits, the algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold.", "Focusing solely on 4-gram matches in BLEU-4 calculation by applying a weight of 1 to the highest order n-gram and 0 to the rest may overly emphasize 4-gram matches while neglecting lower-order matches, leading to an imbalanced evaluation.", "The chunking method for LLM context in QA involves splitting large quantities of text into chunks (paragraphs) and indexing them in a separate information retrieval system.", "RAPTOR's scaling boosts large corpus processing by being computationally efficient, as it scales linearly in terms of both build time and token expenditure.", "The Prince's search is linked to finding the glass slipper because he uses it to identify and find Cinderella, who impressed him at the ball but had to leave in a hurry, leaving the slipper behind.", "Top-k node retrieval in trees is linked to cosine similarity by retrieving the top-k node(s) based on their cosine similarity to the query vector at each level of the tree.", "The NarrativeQA-Story task tests a model's understanding of full narratives.", "A Statistical Interpretation of Term Specificity and its Application in Retrieval.", "Retrieval augmentation involves indexing large quantities of text by splitting it into chunks (paragraphs) in a separate information retrieval system. Retrieved information is then presented to the LLM along with the question as context.", "The novel approach of recursively embedding, clustering, and summarizing chunks of text helps language models adapt by using hierarchical text summaries.", "RAPTOR's tree structure helps with text efficiency by capturing both high-level and low-level details about a text, enabling it to load into an LLM's context chunks representing the text at different levels. This allows RAPTOR to effectively and efficiently answer questions at different levels.", "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi helped publish UNIFIEDQA at EMNLP 2020.", "Atlas fine-tunes an encoder-decoder model in conjunction with the retriever.", "Soft clustering's flexibility matters for text relevance because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.", "Retrieval systems clarify and trace sources by providing a system with current knowledge particular to some domain, enabling easy interpretability and provenance tracking, whereas the parametric knowledge of LLMs is opaque and difficult to trace back to its source.", "Cinderella magically attends the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand, allowing Cinderella to attend the ball. She impresses the Prince at the ball.", "RAPTOR leverages its tree structure during the query phase for more effective retrieval, which contributes to its improved performance."], "evolution_type": ["simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "simple", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "multi_context", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning", "reasoning"], "metadata": [[{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}, {"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}], [{"source": "raptor_paper.pdf"}]], "episode_done": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true]}