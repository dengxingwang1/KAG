{
  "question": [
    "What advancements in retrieval accuracy are achieved by Dense Hierarchical Retrieval (DHR)?",
    "What is the purpose of fine-tuning the REALM model for open-domain question answering?",
    "How does the use of multiple clusters benefit the clustering approach in text segmentation?",
    "What challenges do aspiring writers face in finding a foothold in the industry?",
    "Who is one of the authors of the paper titled \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\"?",
    "How does RAPTOR utilize subtopics and hierarchical structures to enhance reading comprehension?",
    "What was the purpose of conducting an ablation study on the RAPTOR approach?",
    "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?",
    "What is a limitation of existing retrieval-augmented approaches in handling thematic questions?",
    "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?",
    "Why is selecting the most relevant information important for knowledge-intensive tasks?",
    "What do the empirical observations suggest about the effectiveness of the Gaussian assumption in GMMs for modeling text data?",
    "What role does gpt-3.5-turbo play in model-based summarization?",
    "How is cosine similarity used in tree traversal and collapsed tree retrieval mechanisms?",
    "What is the impact of hallucinations on the summarization component in the RAPTOR architecture?",
    "Who helped publish research on long-range context in language models at EMNLP 2021?",
    "How does RAPTOR use iterative text grouping for efficient text retrieval?",
    "How does a magical figure aid a character's journey from hardship to fulfillment in a story of a lost item and family forgiveness?",
    "How do retrieval advances and passage processing boost RALM reader function?",
    "How does a hierarchical tree boost LLMs' knowledge synthesis and retrieval?",
    "Does RAPTOR boost retrieval on QuALITY and work similarly on datasets like NarrativeQA?",
    "How does DPR/RAPTOR affect UnifiedQA-3B's retrieval in QuALITY/QASPER?",
    "How does a dimensionality reduction method balancing local/global structures overcome traditional GMM limits in high-D spaces?",
    "How does text grouping in RAPTOR trees boost efficiency?",
    "Which datasets assess RAPTOR's QA?",
    "How does RAPTOR+GPT-4 impact QuALITY & QASPER benchmarks?",
    "How does linear token scaling improve RAPTOR's handling of complex data?",
    "How does the token threshold affect node selection in the Collapsed Tree Algorithm?",
    "How does gpt-3.5-turbo aid in clustering and summarization for doc structuring?",
    "What limits retrieval methods in fully grasping themes in datasets like NarrativeQA?",
    "How do Gaussian assumptions aid GMM in skewed text clustering?",
    "How does RAPTOR's synthesis boost query efficiency?",
    "What challenges exist in tracing LLMs' knowledge origins vs. retrieval systems?",
    "Which 2020 EMNLP paper by Colin Raffel examines language model capacity, and how does his later work tackle long-tail knowledge issues?",
    "How does Cinderella get to the ball, and when does the magic end?",
    "How does Cinderella get to the ball, and what's the time limit?",
    "How does RAPTOR's recursive tree aid in finding distant semantic links in long texts?",
    "How does Wu et al.'s 2021 model use task decomposition for summarization, possibly missing details?",
    "How does the scalar product affect node choice in a single-layer tree?",
    "How does Cinderella get to the ball, and what's the time limit?",
    "How does BIC weigh complexity vs. fit for optimal clusters in GMM?",
    "How do extended contexts challenge models, and can recursive summarization help?",
    "How do networking platforms help new writers find connections and mentors?",
    "How do input dimensions and cluster count affect GMM parameters, with BIC in model selection?",
    "How does text segmentation improve METEOR's accuracy over basic splits in literary reviews?",
    "How does RAPTOR affect retrieval with UnifiedQA-3B?",
    "Where was the ACL's 57th meeting with Transformer-XL in July 2019?",
    "Which API accesses GPT for QA & summarization?",
    "Which metrics, like B-1 & B-4, evaluate literary dataset performance?",
    "How does recursive summarization boost context in retrieval models?",
    "How does collective node evaluation differ from layer-by-layer in tree queries?",
    "How do LLMs' size and params boost performance?",
    "How does text summarization improve retrieval for UnifiedQA, GPT-3, and GPT-4?",
    "How does RAPTOR's linear scaling boost efficiency with large corpora?",
    "How to keep a cluster's context within token limits?",
    "How does 4-gram focus impact BLEU-4 balance?",
    "What's the chunking method for LLM context in QA?",
    "How does RAPTOR's scaling boost large corpus processing?",
    "How's the Prince's search linked to finding the glass slipper?",
    "How's top-k node retrieval linked to cosine similarity in trees?",
    "Which task tests a model's understanding of full narratives?",
    "Which 1972 work by K. Spärck Jones affects term specificity in retrieval?",
    "How does retrieval augmentation use text chunks in QA?",
    "Which method helps language models adapt by using hierarchical text summaries?",
    "How does RAPTOR's tree help with text efficiency?",
    "Who helped publish UNIFIEDQA at EMNLP 2020?",
    "How does Atlas refine an encoder-decoder with a retriever in end-to-end training?",
    "Why does soft clustering's flexibility matter for text relevance?",
    "How do retrieval systems clarify and trace sources vs. LLMs?",
    "How does Cinderella magically attend the ball and impress the Prince?",
    "How does RAPTOR's tree structure improve retrieval?"
  ],
  "contexts": [
    [],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical"
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.\nThisfindingsubstantiatesourhypothesisthattheclusteringstrategyinRAPTORismoreeffectiveincapturinghomogeneouscontentforsummarization,therebyenhancingtheoverallretrievalperformance.\n16PublishedasaconferencepaperatICLR2024Table9:AblationstudyresultscomparingRAPTORwitharecency-basedtreeapproachConfigurationAccuracyRAPTOR+SBERTembeddings+UnifiedQARecency-basedtree+SBERTembeddings+UnifiedQA56.\n6%55.\n8%CDATASETSTATISTICSANDCOMPRESSIONRATIOSTheaverageratioofthesummarylengthtothesumofchildnodelengthsacrossalldatasetsis0.\n28,indicatinga72%compressionrate.\nOnaverage,thesummarylengthis131tokens,andtheaveragechildnodelengthis86tokens.\nBelowarethedetailedstatisticsforallthreedatasets:Table10:StatisticsofAverageSummaryLengthandChildNodeLengthAcrossDatasetsDatasetAllDatasetsQuALITYNarrativeQAQASPERAvg.\nSummaryLength(tokens)131124.\n4129.\n7145.\n9Avg.\nChildNodeTextLength(tokens)85.\n687.\n985.\n586.\n2Avg.\n#ofChildNodesPerParentAvg.\nCompressionRatio(%)6.\n75.\n76.\n85.\n7.\n28.\n28.\n27.\n35DSUMMARIZATIONPROMPTTable11showsthepromptusedforsummarization.\nTable11:PromptforSummarizationContentRolesystemYouareaSummarizingTextPortaluserWriteasummaryofthefollowing,includingasmanykeydetailsaspossible:{context}:EHALLUCINATIONANALYSISToassessthequalityandaccuracyofthesummarizationswithinourRAPTORmodel,weconductedananalysisfocusingonhallucinationsinthegeneratedsummaries.\nThesummariesweregeneratedbygpt-3.\n5-turboandsubsequentlyannotatedtoquantifytheratesofhallucinations,toexaminewhethersuchinaccuraciespropagatetoparentnodes,andtoevaluatetheirimpactonquestion-answering(QA)tasks.\nE.\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.",
      "#raptor_paper#Findings#raptor_paper#Findings\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.\n2FINDINGSOutofthe150nodessampled,4%(6nodes)containedsomeformofhallucination.\nMostcommonly,thesehallucinationsoriginatedfromthemodeladdingminorinformationpossiblyfromitstrainingdatathatwasnotpresentinthetextbeingsummarized,orfromincorrectlyextrapolatingsomeinformationwhencreatingthesummary.\n17PublishedasaconferencepaperatICLR2024Example:Textofthechildnodes:”Andyouwillcomewithmetomypeople?\nWemaylivehereamongthem,andyouwillbeagreatwarrior–oh,whenJordiesyoumayevenbechief,forthereisnonesomightyasmywarrior.\n.\n.\n”Butyourfatherwillnotpermitit–Jor,myfather,HighChiefoftheGalus,willnotpermitit,forlikemeyouarecos-ata-lo.\nOh,Co-Tan,ifwebutcould!\n.\n.\n.\nBradleynoticedthatshespokeinEnglish–brokenEnglishlikeCo-Tan’sbutequallyappealing.\nSummaryfoundintheparentofthatnode:Theprotagonist,Bradley,isbeingaskedbyCo-Tantostaywithherpeopleandbecomeagreatwarrior,butherefusesandmustreturntohisowncountry.\nTomBillingsofSantaMonicaarrivesandtellsthemhecametosearchforamannamedBowenJ.\nTyler,Jr.\nAjor,Co-Tan’ssister,isexcitedaboutthepossibilityofgoingtoTom’scountrytoseestrangeandwonderfulthings.\n.\n.\nThehallucinationhereisthatthesummarystatesthatJr.\nAjorandCo-Tanaresisters,butdoesnotexplicitlymentionorimplythis.\nUponreviewingallparentnodes,wefoundthathallucinationsdidnotpropagatetohigherlayers.\nGenerally,thehallucinationswereminoranddidnotalterthethematicinterpretationofthetext.\nE.\n3IMPACTONQATASKSInourfindings,hallucinationshadnodiscernibleimpactontheperformanceofQAtasks.\nThissug-geststhathallucinationisnotamajorconcernsforthesummarizationcomponentinourRAPTORarchitecture.\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.\nThisfindingsubstantiatesourhypothesisthattheclusteringstrategyinRAPTORismoreeffectiveincapturinghomogeneouscontentforsummarization,therebyenhancingtheoverallretrievalperformance.\n16PublishedasaconferencepaperatICLR2024Table9:AblationstudyresultscomparingRAPTORwitharecency-basedtreeapproachConfigurationAccuracyRAPTOR+SBERTembeddings+UnifiedQARecency-basedtree+SBERTembeddings+UnifiedQA56.\n6%55.\n8%CDATASETSTATISTICSANDCOMPRESSIONRATIOSTheaverageratioofthesummarylengthtothesumofchildnodelengthsacrossalldatasetsis0.\n28,indicatinga72%compressionrate.\nOnaverage,thesummarylengthis131tokens,andtheaveragechildnodelengthis86tokens.\nBelowarethedetailedstatisticsforallthreedatasets:Table10:StatisticsofAverageSummaryLengthandChildNodeLengthAcrossDatasetsDatasetAllDatasetsQuALITYNarrativeQAQASPERAvg.\nSummaryLength(tokens)131124.\n4129.\n7145.\n9Avg.\nChildNodeTextLength(tokens)85.\n687.\n985.\n586.\n2Avg.\n#ofChildNodesPerParentAvg.\nCompressionRatio(%)6.\n75.\n76.\n85.\n7.\n28.\n28.\n27.\n35DSUMMARIZATIONPROMPTTable11showsthepromptusedforsummarization.\nTable11:PromptforSummarizationContentRolesystemYouareaSummarizingTextPortaluserWriteasummaryofthefollowing,includingasmanykeydetailsaspossible:{context}:EHALLUCINATIONANALYSISToassessthequalityandaccuracyofthesummarizationswithinourRAPTORmodel,weconductedananalysisfocusingonhallucinationsinthegeneratedsummaries.\nThesummariesweregeneratedbygpt-3.\n5-turboandsubsequentlyannotatedtoquantifytheratesofhallucinations,toexaminewhethersuchinaccuraciespropagatetoparentnodes,andtoevaluatetheirimpactonquestion-answering(QA)tasks.\nE.\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.",
      "#raptor_paper#Findings#raptor_paper#Findings\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.\n2FINDINGSOutofthe150nodessampled,4%(6nodes)containedsomeformofhallucination.\nMostcommonly,thesehallucinationsoriginatedfromthemodeladdingminorinformationpossiblyfromitstrainingdatathatwasnotpresentinthetextbeingsummarized,orfromincorrectlyextrapolatingsomeinformationwhencreatingthesummary.\n17PublishedasaconferencepaperatICLR2024Example:Textofthechildnodes:”Andyouwillcomewithmetomypeople?\nWemaylivehereamongthem,andyouwillbeagreatwarrior–oh,whenJordiesyoumayevenbechief,forthereisnonesomightyasmywarrior.\n.\n.\n”Butyourfatherwillnotpermitit–Jor,myfather,HighChiefoftheGalus,willnotpermitit,forlikemeyouarecos-ata-lo.\nOh,Co-Tan,ifwebutcould!\n.\n.\n.\nBradleynoticedthatshespokeinEnglish–brokenEnglishlikeCo-Tan’sbutequallyappealing.\nSummaryfoundintheparentofthatnode:Theprotagonist,Bradley,isbeingaskedbyCo-Tantostaywithherpeopleandbecomeagreatwarrior,butherefusesandmustreturntohisowncountry.\nTomBillingsofSantaMonicaarrivesandtellsthemhecametosearchforamannamedBowenJ.\nTyler,Jr.\nAjor,Co-Tan’ssister,isexcitedaboutthepossibilityofgoingtoTom’scountrytoseestrangeandwonderfulthings.\n.\n.\nThehallucinationhereisthatthesummarystatesthatJr.\nAjorandCo-Tanaresisters,butdoesnotexplicitlymentionorimplythis.\nUponreviewingallparentnodes,wefoundthathallucinationsdidnotpropagatetohigherlayers.\nGenerally,thehallucinationswereminoranddidnotalterthethematicinterpretationofthetext.\nE.\n3IMPACTONQATASKSInourfindings,hallucinationshadnodiscernibleimpactontheperformanceofQAtasks.\nThissug-geststhathallucinationisnotamajorconcernsforthesummarizationcomponentinourRAPTORarchitecture.\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.\nThisfindingsubstantiatesourhypothesisthattheclusteringstrategyinRAPTORismoreeffectiveincapturinghomogeneouscontentforsummarization,therebyenhancingtheoverallretrievalperformance.\n16PublishedasaconferencepaperatICLR2024Table9:AblationstudyresultscomparingRAPTORwitharecency-basedtreeapproachConfigurationAccuracyRAPTOR+SBERTembeddings+UnifiedQARecency-basedtree+SBERTembeddings+UnifiedQA56.\n6%55.\n8%CDATASETSTATISTICSANDCOMPRESSIONRATIOSTheaverageratioofthesummarylengthtothesumofchildnodelengthsacrossalldatasetsis0.\n28,indicatinga72%compressionrate.\nOnaverage,thesummarylengthis131tokens,andtheaveragechildnodelengthis86tokens.\nBelowarethedetailedstatisticsforallthreedatasets:Table10:StatisticsofAverageSummaryLengthandChildNodeLengthAcrossDatasetsDatasetAllDatasetsQuALITYNarrativeQAQASPERAvg.\nSummaryLength(tokens)131124.\n4129.\n7145.\n9Avg.\nChildNodeTextLength(tokens)85.\n687.\n985.\n586.\n2Avg.\n#ofChildNodesPerParentAvg.\nCompressionRatio(%)6.\n75.\n76.\n85.\n7.\n28.\n28.\n27.\n35DSUMMARIZATIONPROMPTTable11showsthepromptusedforsummarization.\nTable11:PromptforSummarizationContentRolesystemYouareaSummarizingTextPortaluserWriteasummaryofthefollowing,includingasmanykeydetailsaspossible:{context}:EHALLUCINATIONANALYSISToassessthequalityandaccuracyofthesummarizationswithinourRAPTORmodel,weconductedananalysisfocusingonhallucinationsinthegeneratedsummaries.\nThesummariesweregeneratedbygpt-3.\n5-turboandsubsequentlyannotatedtoquantifytheratesofhallucinations,toexaminewhethersuchinaccuraciespropagatetoparentnodes,andtoevaluatetheirimpactonquestion-answering(QA)tasks.\nE.\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.",
      "#raptor_paper#Findings#raptor_paper#Findings\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.\n2FINDINGSOutofthe150nodessampled,4%(6nodes)containedsomeformofhallucination.\nMostcommonly,thesehallucinationsoriginatedfromthemodeladdingminorinformationpossiblyfromitstrainingdatathatwasnotpresentinthetextbeingsummarized,orfromincorrectlyextrapolatingsomeinformationwhencreatingthesummary.\n17PublishedasaconferencepaperatICLR2024Example:Textofthechildnodes:”Andyouwillcomewithmetomypeople?\nWemaylivehereamongthem,andyouwillbeagreatwarrior–oh,whenJordiesyoumayevenbechief,forthereisnonesomightyasmywarrior.\n.\n.\n”Butyourfatherwillnotpermitit–Jor,myfather,HighChiefoftheGalus,willnotpermitit,forlikemeyouarecos-ata-lo.\nOh,Co-Tan,ifwebutcould!\n.\n.\n.\nBradleynoticedthatshespokeinEnglish–brokenEnglishlikeCo-Tan’sbutequallyappealing.\nSummaryfoundintheparentofthatnode:Theprotagonist,Bradley,isbeingaskedbyCo-Tantostaywithherpeopleandbecomeagreatwarrior,butherefusesandmustreturntohisowncountry.\nTomBillingsofSantaMonicaarrivesandtellsthemhecametosearchforamannamedBowenJ.\nTyler,Jr.\nAjor,Co-Tan’ssister,isexcitedaboutthepossibilityofgoingtoTom’scountrytoseestrangeandwonderfulthings.\n.\n.\nThehallucinationhereisthatthesummarystatesthatJr.\nAjorandCo-Tanaresisters,butdoesnotexplicitlymentionorimplythis.\nUponreviewingallparentnodes,wefoundthathallucinationsdidnotpropagatetohigherlayers.\nGenerally,thehallucinationswereminoranddidnotalterthethematicinterpretationofthetext.\nE.\n3IMPACTONQATASKSInourfindings,hallucinationshadnodiscernibleimpactontheperformanceofQAtasks.\nThissug-geststhathallucinationisnotamajorconcernsforthesummarizationcomponentinourRAPTORarchitecture.\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.\n2FINDINGSOutofthe150nodessampled,4%(6nodes)containedsomeformofhallucination.\nMostcommonly,thesehallucinationsoriginatedfromthemodeladdingminorinformationpossiblyfromitstrainingdatathatwasnotpresentinthetextbeingsummarized,orfromincorrectlyextrapolatingsomeinformationwhencreatingthesummary.\n17PublishedasaconferencepaperatICLR2024Example:Textofthechildnodes:”Andyouwillcomewithmetomypeople?\nWemaylivehereamongthem,andyouwillbeagreatwarrior–oh,whenJordiesyoumayevenbechief,forthereisnonesomightyasmywarrior.\n.\n.\n”Butyourfatherwillnotpermitit–Jor,myfather,HighChiefoftheGalus,willnotpermitit,forlikemeyouarecos-ata-lo.\nOh,Co-Tan,ifwebutcould!\n.\n.\n.\nBradleynoticedthatshespokeinEnglish–brokenEnglishlikeCo-Tan’sbutequallyappealing.\nSummaryfoundintheparentofthatnode:Theprotagonist,Bradley,isbeingaskedbyCo-Tantostaywithherpeopleandbecomeagreatwarrior,butherefusesandmustreturntohisowncountry.\nTomBillingsofSantaMonicaarrivesandtellsthemhecametosearchforamannamedBowenJ.\nTyler,Jr.\nAjor,Co-Tan’ssister,isexcitedaboutthepossibilityofgoingtoTom’scountrytoseestrangeandwonderfulthings.\n.\n.\nThehallucinationhereisthatthesummarystatesthatJr.\nAjorandCo-Tanaresisters,butdoesnotexplicitlymentionorimplythis.\nUponreviewingallparentnodes,wefoundthathallucinationsdidnotpropagatetohigherlayers.\nGenerally,thehallucinationswereminoranddidnotalterthethematicinterpretationofthetext.\nE.\n3IMPACTONQATASKSInourfindings,hallucinationshadnodiscernibleimpactontheperformanceofQAtasks.\nThissug-geststhathallucinationisnotamajorconcernsforthesummarizationcomponentinourRAPTORarchitecture.\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.",
      "#raptor_paper#Findings#raptor_paper#Findings\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.\nThisfindingsubstantiatesourhypothesisthattheclusteringstrategyinRAPTORismoreeffectiveincapturinghomogeneouscontentforsummarization,therebyenhancingtheoverallretrievalperformance.\n16PublishedasaconferencepaperatICLR2024Table9:AblationstudyresultscomparingRAPTORwitharecency-basedtreeapproachConfigurationAccuracyRAPTOR+SBERTembeddings+UnifiedQARecency-basedtree+SBERTembeddings+UnifiedQA56.\n6%55.\n8%CDATASETSTATISTICSANDCOMPRESSIONRATIOSTheaverageratioofthesummarylengthtothesumofchildnodelengthsacrossalldatasetsis0.\n28,indicatinga72%compressionrate.\nOnaverage,thesummarylengthis131tokens,andtheaveragechildnodelengthis86tokens.\nBelowarethedetailedstatisticsforallthreedatasets:Table10:StatisticsofAverageSummaryLengthandChildNodeLengthAcrossDatasetsDatasetAllDatasetsQuALITYNarrativeQAQASPERAvg.\nSummaryLength(tokens)131124.\n4129.\n7145.\n9Avg.\nChildNodeTextLength(tokens)85.\n687.\n985.\n586.\n2Avg.\n#ofChildNodesPerParentAvg.\nCompressionRatio(%)6.\n75.\n76.\n85.\n7.\n28.\n28.\n27.\n35DSUMMARIZATIONPROMPTTable11showsthepromptusedforsummarization.\nTable11:PromptforSummarizationContentRolesystemYouareaSummarizingTextPortaluserWriteasummaryofthefollowing,includingasmanykeydetailsaspossible:{context}:EHALLUCINATIONANALYSISToassessthequalityandaccuracyofthesummarizationswithinourRAPTORmodel,weconductedananalysisfocusingonhallucinationsinthegeneratedsummaries.\nThesummariesweregeneratedbygpt-3.\n5-turboandsubsequentlyannotatedtoquantifytheratesofhallucinations,toexaminewhethersuchinaccuraciespropagatetoparentnodes,andtoevaluatetheirimpactonquestion-answering(QA)tasks.\nE.\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable12:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nquestionathand.\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?"
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021)."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR."
    ],
    [],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021)."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical"
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable12:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nquestionathand.\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?",
      "#raptor_paper#Findings#raptor_paper#Findings\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.\nTocontrolforthelanguagemodel’spotentialfamiliaritywiththeCinderellastory,weinstructedittorelysolelyontheretrievedinformationforitsanswers.\nTable13showsthetextretrievedbybothRAPTORandDPRforthisquestion.\nRAPTOR’scontextsuccinctlydescribesCinderella’sjourneytohappiness,whileDPR’sleafnodesprimarilyfocusonherinitialtransformation.\nThedifferenceinretrievedinformation19PublishedasaconferencepaperatICLR2024significantlyimpactsdownstreamtasks.\nWhenGPT-4isprovidedwithRAPTOR’scontext,itgen-eratesadetailedanswer:“CinderellafindsahappyendingwhenthePrincesearchesfortheownerofthelostglassslipperanddiscoversitbelongstoCinderella.\nTheyeventuallymarry,transform-ingCinderella’slifeforthebetter.\n”Incontrast,usingDPR’scontext,GPT-4states:“Basedonthegivencontext,itisnotpossibletodeterminehowCinderellafindsahappyending,asthetextlacksinformationaboutthestory’sconclusion.\n”Thesecondquestionweexamineis“Whatisthecentralthemeofthestory?\n”,athematicquestionthatrequiresholisticunderstandingoftheentiretext.\nThetextretrievedbyRAPTORandDPRforthisquestionisshowninTable13.\nThetextretrievedbyRAPTORcontainsshortdescriptionsofallthemajorpartsofthestory,whereasthetextretrievedbyDPRcontainsdetaileddescriptionsofanarrowsubsetofthestory.\nAgain,thedifferenceinretrievalmechanismsaffectstheperformanceofGPT-4whenansweringthequestion.\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable12:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nquestionathand.\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.",
      "#raptor_paper#Findings#raptor_paper#Findings\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.\nTocontrolforthelanguagemodel’spotentialfamiliaritywiththeCinderellastory,weinstructedittorelysolelyontheretrievedinformationforitsanswers.\nTable13showsthetextretrievedbybothRAPTORandDPRforthisquestion.\nRAPTOR’scontextsuccinctlydescribesCinderella’sjourneytohappiness,whileDPR’sleafnodesprimarilyfocusonherinitialtransformation.\nThedifferenceinretrievedinformation19PublishedasaconferencepaperatICLR2024significantlyimpactsdownstreamtasks.\nWhenGPT-4isprovidedwithRAPTOR’scontext,itgen-eratesadetailedanswer:“CinderellafindsahappyendingwhenthePrincesearchesfortheownerofthelostglassslipperanddiscoversitbelongstoCinderella.\nTheyeventuallymarry,transform-ingCinderella’slifeforthebetter.\n”Incontrast,usingDPR’scontext,GPT-4states:“Basedonthegivencontext,itisnotpossibletodeterminehowCinderellafindsahappyending,asthetextlacksinformationaboutthestory’sconclusion.\n”Thesecondquestionweexamineis“Whatisthecentralthemeofthestory?\n”,athematicquestionthatrequiresholisticunderstandingoftheentiretext.\nThetextretrievedbyRAPTORandDPRforthisquestionisshowninTable13.\nThetextretrievedbyRAPTORcontainsshortdescriptionsofallthemajorpartsofthestory,whereasthetextretrievedbyDPRcontainsdetaileddescriptionsofanarrowsubsetofthestory.\nAgain,thedifferenceinretrievalmechanismsaffectstheperformanceofGPT-4whenansweringthequestion.\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable12:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nquestionathand.\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?",
      "#raptor_paper#Findings#raptor_paper#Findings\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.\nTocontrolforthelanguagemodel’spotentialfamiliaritywiththeCinderellastory,weinstructedittorelysolelyontheretrievedinformationforitsanswers.\nTable13showsthetextretrievedbybothRAPTORandDPRforthisquestion.\nRAPTOR’scontextsuccinctlydescribesCinderella’sjourneytohappiness,whileDPR’sleafnodesprimarilyfocusonherinitialtransformation.\nThedifferenceinretrievedinformation19PublishedasaconferencepaperatICLR2024significantlyimpactsdownstreamtasks.\nWhenGPT-4isprovidedwithRAPTOR’scontext,itgen-eratesadetailedanswer:“CinderellafindsahappyendingwhenthePrincesearchesfortheownerofthelostglassslipperanddiscoversitbelongstoCinderella.\nTheyeventuallymarry,transform-ingCinderella’slifeforthebetter.\n”Incontrast,usingDPR’scontext,GPT-4states:“Basedonthegivencontext,itisnotpossibletodeterminehowCinderellafindsahappyending,asthetextlacksinformationaboutthestory’sconclusion.\n”Thesecondquestionweexamineis“Whatisthecentralthemeofthestory?\n”,athematicquestionthatrequiresholisticunderstandingoftheentiretext.\nThetextretrievedbyRAPTORandDPRforthisquestionisshowninTable13.\nThetextretrievedbyRAPTORcontainsshortdescriptionsofallthemajorpartsofthestory,whereasthetextretrievedbyDPRcontainsdetaileddescriptionsofanarrowsubsetofthestory.\nAgain,thedifferenceinretrievalmechanismsaffectstheperformanceofGPT-4whenansweringthequestion.\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.\nIncontrast,givenRAPTOR’scontext,GPT-4outputs“Thecentralthemeofthestoryistransformationandovercomingadversity,asCinderella,withthehelpofherFairyGodmother,transformsfromamistreatedanddowntroddengirlintoabeautifulandconfidentyoungwomanwhoultimatelyfindshappinessandlovewiththePrince.\n”Thisisamorecompleteanswer,demonstratingacomprehensiveunderstandingofthestory.\nThisqualitativeanalysisindicatesthatRAPTORoutperformspriorretrievalmechanismsbecausetheinformationthatitretrievesismorerelevantandexhaustive,allowingforbetterperformanceondownstreamtasks.\nWealsocreateda2600-wordstoryalongwithquestionsaboutitsnarrativeandtheme.\nAnexcerptfromthestoryispresentbelowandthefullPDFofthisstoryislinkedhere.\nForquestionslike“Whatisthecentralthemeofthestory?\n”,anupper-levelnodeisretrievedwhichincludesthesentence:“Thisstoryisaboutthepowerofhumanconnection.\n.\n.\ninspiringandupliftingeachotherastheypursuedtheirpassions.\n”Thissummary,notexplicitlypresentintheoriginaltext,almostdirectlyanswersthequestion.\nExcerptfrom”TheEagerWriter”:”Ethan’spassionforwritinghadalwaysbeenapartofhim.\nAsachild,hewouldoftenscribblestoriesandpoemsinhisnotebook,andashegrewolder,hisloveforwritingonlyintensified.\nHiseveningswereoftenspentinthedimlightofhisroom,typingawayathislaptop.\nHehadrecentlytakenajobasacontentwriterforanonlinemarketingfirmtopaythebills,buthisheartstilllongedfortheworldofstorytelling.\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHowever,likemanyaspiringwriters,hestruggledtofindafootholdintheindustry.\nHetookajobasacontentwriterforanonlinemarketingfirm,butitwasgrowingincreasinglyevidenttohimthatthiswasnotthepathhewantedtopursue.\nItwasduringthistimethathestumbleduponthePathwaysapp.\nTheappofferedaplatformforpeopleinsimilarprofessionstoconnectandshareknowledge,andhesawitasanopportunitytofinallyconnectwithotherswhosharedhispassionforwriting.\nEthansawanopportunitytomeetotherswhosharedhispassionandcouldofferguidanceandmentorship.\nHequicklysignedupandwassurprisedbythenumberofwritershefoundontheplatform,fromwellestablishprofessionalstobeginnersjuststartingoutinthebusiness.\n”HNARRATIVEQAEVALUATIONSCRIPTWemadeseveralmodificationstoAllenNLP’sevaluationscript3tobetterfitourevaluationneeds:•AddedSmoothing:SmoothingwasincorporatedtohandlecaseswhereBLEUscoreiszero,duetonon-grammatchesoccurringinthereferencetext.\nABLEUscoreofzeroskewstheresults,leadingtoanoverlyharshevaluationforrareornovelphrases.\nByadding3docs.\nallennlp.\norg/models/main/models/rc/tools/narrativeqa/20PublishedasaconferencepaperatICLR2024asmoothingfunction,wepreventtheBLEUscoresfromdroppingtozero,providingamorefairevaluation.\n•ModifiedBLEU-4Weighting:Theoriginalscriptappliedaweightof1tothehighestordern-gram(4-gram)and0totherestinitsBLEU-4calculation(i.\ne.\n,weights=(0,0,0,1)).\nThisapproachmayoverlyfocuson4-grammatcheswhileneglectinglower-ordermatches.\nToprovideamorebalancedevaluation,weevenlydistributedtheweightacrossalln-gramlevels,changingtheweightsfortheBLEU-4calculationto(0.\n25,0.\n25,0.\n25,0.\n25).\n•TokenizationbeforeMappinginMETEORCalculation:TheoriginalscriptutilizedasimplesplitandmapmethodforMETEORcalculation.\nWefixedthisbyfirsttokenizingthetextandthenmappingthetokens.\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#Findings#raptor_paper#Findings\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.\nThisfindingsubstantiatesourhypothesisthattheclusteringstrategyinRAPTORismoreeffectiveincapturinghomogeneouscontentforsummarization,therebyenhancingtheoverallretrievalperformance.\n16PublishedasaconferencepaperatICLR2024Table9:AblationstudyresultscomparingRAPTORwitharecency-basedtreeapproachConfigurationAccuracyRAPTOR+SBERTembeddings+UnifiedQARecency-basedtree+SBERTembeddings+UnifiedQA56.\n6%55.\n8%CDATASETSTATISTICSANDCOMPRESSIONRATIOSTheaverageratioofthesummarylengthtothesumofchildnodelengthsacrossalldatasetsis0.\n28,indicatinga72%compressionrate.\nOnaverage,thesummarylengthis131tokens,andtheaveragechildnodelengthis86tokens.\nBelowarethedetailedstatisticsforallthreedatasets:Table10:StatisticsofAverageSummaryLengthandChildNodeLengthAcrossDatasetsDatasetAllDatasetsQuALITYNarrativeQAQASPERAvg.\nSummaryLength(tokens)131124.\n4129.\n7145.\n9Avg.\nChildNodeTextLength(tokens)85.\n687.\n985.\n586.\n2Avg.\n#ofChildNodesPerParentAvg.\nCompressionRatio(%)6.\n75.\n76.\n85.\n7.\n28.\n28.\n27.\n35DSUMMARIZATIONPROMPTTable11showsthepromptusedforsummarization.\nTable11:PromptforSummarizationContentRolesystemYouareaSummarizingTextPortaluserWriteasummaryofthefollowing,includingasmanykeydetailsaspossible:{context}:EHALLUCINATIONANALYSISToassessthequalityandaccuracyofthesummarizationswithinourRAPTORmodel,weconductedananalysisfocusingonhallucinationsinthegeneratedsummaries.\nThesummariesweregeneratedbygpt-3.\n5-turboandsubsequentlyannotatedtoquantifytheratesofhallucinations,toexaminewhethersuchinaccuraciespropagatetoparentnodes,andtoevaluatetheirimpactonquestion-answering(QA)tasks.\nE.\n1METHODOLOGYWerandomlysampled150nodesacross40storiesandevaluatedthemforhallucinations.\nThissamplingstrategyprovidesabroadviewofthemodel’sperformanceacrossdifferentcontexts.\nEachnodewasannotatedbyhand,anddeterminedifitcontainedahallucination.\nE.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021)."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023)."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23"
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.\nFinally,intheNarrativeQAdataset,aspresentedinTable6,RAPTORexcelsacrossmultiplemet-rics.\nForROUGE-L,itsurpassesBM25andDPRby7.\n3and2.\n7points,respectively.\nInothermetricslikeBLEU-1,BLEU-4,andMETEOR,RAPTORoutperformsBM25andDPRbymarginsrangingfrom1.\n7to5.\n8and0.\n7to2.\n1points,respectively.\n2FortheDPRexperimentsinTables1and2,weusedthedpr-multiset-basemodelasopposedtodpr-single-nq-basewhichwasusedinrestoftheexperimentsdoneearlier.\nThisdecisionwasbasedontheperformanceobservedinKarpukhinetal.\n(2020),wheredpr-multiset-baseshowedsuperiorresults.\n7PublishedasaconferencepaperatICLR2024Table2:QuALITYandQASPERPerformanceWith+WithoutRAPTOR:Performancecom-parisonacrosstheQuALITYandQASPERdatasetsofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTOR.\nUnifiedQA-3Bisusedasthelanguagemodel.\nRAPTORoutper-formsbaselinesofeachrespectiveretrievalmethodforbothdatasets.\nModelAccuracy(QuALITY)AnswerF1(QASPER)SBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR56.\n6%54.\n9%52.\n1%49.\n9%54.\n7%53.\n1%36.\n70%36.\n23%27.\n00%26.\n47%32.\n23%31.\n70%Table3:ControlledcomparisonofF-1scoresontheQASPERdataset,usingthreedifferentlan-guagemodels(GPT-3,GPT-4,UnifiedQA3B)andvariousretrievalmethods.\nThecolumn”Title+Abstract”reflectsperformancewhenonlythetitleandabstractofthepapersareusedforcontext.\nRAPTORoutperformstheestablishedbaselinesBM25andDPRacrossalltestedlanguagemodels.\nSpecifically,RAPTOR’sF-1scoresareatleast1.\n8%pointshigherthanDPRandatleast5.\n3%pointshigherthanBM25.\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.\nNotably,RAPTOR’scontextoftenencompassestheinformationretrievedbyDPR,eitherdirectlyorwithinhigher-layersummaries.\nourresultsdemonstratethatRAPTOR,whencombinedwithanyretriever,consistentlyoutperformstherespectiveretrieveracrossalldatasets.\n2SinceRAPTORwithSBERThasthebestperformance,weuseitinallsubsequentexperiments.\nWenowcompareRAPTORwithBM25andDPR,usingthreedifferentLLMs:GPT-3,GPT-4,andUnifiedQA.\nAsshowninTable3,RAPTORconsistentlyoutperformsBM25andDPRacrossallthreeLanguageModelsontheQASPERdataset.\nRAPTOR’sF-1Matchscoresare53.\n1%,55.\n7%,and36.\n6%whenusingGPT-3,GPT-4,andUnifiedQA,respectively.\nThesescoressurpassDPRbymarginsof1.\n8,2.\n7,and4.\n5points,andoutdoBM25by6.\n5,5.\n5,and10.\n2pointsacrosstherespectiveLLMs.\nQASPERrequiressynthesizinginformationwithinNLPpapers,soitisunsurprisingthatRAPTOR’shigher-levelsummarynodeswouldallowittooutperformmethodsthatcanonlyextractthetop-kmostsimilarrawchunksoftext,whichmaynotcontainthecorrectresponseinisolation.\nTable1:NarrativeQAPerformanceWith+WithoutRAPTOR:Performancecomparisonofvariousretrievalmethods(SBERT,BM25,DPR)withandwithoutRAPTORontheNarrativeQAdataset,usingUnifiedQA-3Basthelanguagemodel.\nRAPTORoutperformsbaselinesofeachre-spectiveretrievalmethod.\nModelROUGEBLEU-1BLEU-4METEORSBERTwithRAPTORSBERTwithoutRAPTORBM25withRAPTORBM25withoutRAPTORDPRwithRAPTORDPRwithoutRAPTOR30.\n87%23.\n50%6.\n42%29.\n26%5.\n95%22.\n56%27.\n93%21.\n17%5.\n70%23.\n52%4.\n65%17.\n73%30.\n94%23.\n51%6.\n45%6.\n12%22.\n84%29.\n56%19.\n20%18.\n15%17.\n03%13.\n98%19.\n05%18.\n44%Likewise,intheQuALITYdatasetasshowninTable4,RAPTORachievesanaccuracyof62.\n4%,whichisa2%and5.\n1%improvementoverDPRandBM25.\nSimilartrendsareobservedwhenUni-fiedQAisemployed,withRAPTORoutperformingDPRandBM25by2.\n7%and6.\n7%,respectively.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrieverGPT-3F-1MatchGPT-4F-1MatchUnifiedQAF-1MatchTitle+AbstractBM25DPRRAPTOR25.\n246.\n651.\n353.\n122.\n250.\n253.\n055.\n717.\n526.\n432.\n136.\n6ComparisontoState-of-the-artSystemsBuildinguponourcontrolledcomparisons,weexamineRAPTOR’sperformancerelativetootherstate-of-the-artmodels.\nAsshowninTable5,RAPTORwithGPT-4setsanewbenchmarkonQASPER,witha55.\n7%F-1score,surpassingtheCoLT5XL’sscoreof53.\n9%.\nIntheQuALITYdataset,asshowninTable7,RAPTORpairedwithGPT-4setsanewstate-of-the-artwithanaccuracyof82.\n6%,surpass-ingthepreviousbestresultof62.\n3%.\nInpar-ticular,itoutperformsCoLISAby21.\n5%onQuALITY-HARD,whichrepresentsquestionsthathumanstookunusuallylongtocorrectlyanswer,requiringrereadingpartsofthetext,difficultreasoning,orboth.\nTable4:ComparisonofaccuraciesontheQuAL-ITYdevdatasetfortwodifferentlanguagemod-els(GPT-3,UnifiedQA3B)usingvariousretrievalmethods.\nRAPTORoutperformsthebaselinesofBM25andDPRbyatleast2.\n0%inaccuracy.\nModelGPT-3Acc.\nUnifiedQAAcc.\nBM25DPRRAPTOR57.\n360.\n462.\n449.\n953.\n956.\n6Table5:ResultsonF-1MatchscoresofvariousmodelsontheQASPERdataset.\nModelF-1MatchLongT5XL(Guoetal.\n,2022)CoLT5XL(Ainslieetal.\n,2023)RAPTOR+GPT-453.\n153.\n955.\n7FortheNarrativeQAdataset,asrepresentedinTable6,RAPTORpairedwithUnifiedQAsetsanewstate-of-the-artMETEORscore.\nWhencomparedtotherecursivelysummarizingmodelbyWuetal.\n(2021),whichalsoemploysUnifiedQA,RAPTORoutperformsitonallmetrics.\nWhileWuetal.\n(2021)relysolelyonthesummaryinthetoprootnodeofthetreestructure,RAPTORbenefitsfromitsintermediatelayersandclusteringapproaches,whichallowsittocapturearangeofinformation,fromgeneralthemestospecificdetails,contributingtoitsoverallstrongperformance.\n4.\n1CONTRIBUTIONOFTHETREESTRUCTUREWeexaminethecontributionofeachlayerofnodestoRAPTOR’sretrievalcapabilities.\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable12:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nquestionathand.\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFPSEUDOCODEFORRETRIEVALMETHODSAlgorithm1TreeTraversalAlgorithmfunctionTRAVERSETREE(tree,query,k)Scurrent←tree.\nlayer[0]forlayerinrange(tree.\nnumlayers)dotopk←[]fornodeinScurrentdoscore←dotproduct(query,node)topk.\nappend((node,score))endforSlayer←sorted(topk)[:k].\nnodesScurrent←SlayerendforreturnS0∪S1∪S2∪.\n.\n.\n∪SkendfunctionGQUALITATIVEANALYSISToqualitativelyexamineRAPTOR’sretrievalprocess,wetestitonthematic,multi-hopquestionsabouta1500-wordversionofthefairytaleCinderella.\nWecomparethecontextretrievedbyRAP-TORwiththecontextretrievedbyDensePassageRetrieval(DPR).\nFigure4inthemainpaperdetailstheretrievalprocesswithinRAPTOR’streestructurefortwoquestions.\nThenodesthatRAPTORselectsforeachquestionarehighlighted,whiletheleafnodesthatDPRselectsforthesamequestionareindicatedwitharrows.\nThiscomparisonillustratestheadvantageofRAPTOR’streestructure.\nRAPTORselectsnodesfromdifferentlayersdependingonthelevelofgranularityrequiredbythe18PublishedasaconferencepaperatICLR2024Algorithm2CollapsedTreeAlgorithmfunctionCOLLAPSEDTREE(tree,query,k,maxtokens)tree←flatten(tree)topnodes←[]fornodeintreedotopnodes.\nappend((node,dotproduct(query,node))endfortopnodes←sorted(topnodes)result←[]totaltokens←0fornodeintopnodesdoiftotaltokens+node.\ntokensize<maxtokensthenresult.\nappend(node)endiftotaltokens←totaltokens+node.\ntokensizeendforreturnresultendfunction▷Flattentreeinto1DQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFurther,theinformationthatwouldberetrievedbyDPRismoreoftenthannotincludedinthecontextretrievedbyRAPTOR,eitherdirectlyasaleafnodeorindirectlyaspartofasummaryfromahigherlayer.\n”Thefirstquestionweexamineis“HowdoesCinderellafindahappyending?\n”,amulti-hopquestionbestansweredbysynthesizinginformationfromvarioustextsegments.\nTocontrolforthelanguagemodel’spotentialfamiliaritywiththeCinderellastory,weinstructedittorelysolelyontheretrievedinformationforitsanswers.\nTable13showsthetextretrievedbybothRAPTORandDPRforthisquestion.\nRAPTOR’scontextsuccinctlydescribesCinderella’sjourneytohappiness,whileDPR’sleafnodesprimarilyfocusonherinitialtransformation.\nThedifferenceinretrievedinformation19PublishedasaconferencepaperatICLR2024significantlyimpactsdownstreamtasks.\nWhenGPT-4isprovidedwithRAPTOR’scontext,itgen-eratesadetailedanswer:“CinderellafindsahappyendingwhenthePrincesearchesfortheownerofthelostglassslipperanddiscoversitbelongstoCinderella.\nTheyeventuallymarry,transform-ingCinderella’slifeforthebetter.\n”Incontrast,usingDPR’scontext,GPT-4states:“Basedonthegivencontext,itisnotpossibletodeterminehowCinderellafindsahappyending,asthetextlacksinformationaboutthestory’sconclusion.\n”Thesecondquestionweexamineis“Whatisthecentralthemeofthestory?\n”,athematicquestionthatrequiresholisticunderstandingoftheentiretext.\nThetextretrievedbyRAPTORandDPRforthisquestionisshowninTable13.\nThetextretrievedbyRAPTORcontainsshortdescriptionsofallthemajorpartsofthestory,whereasthetextretrievedbyDPRcontainsdetaileddescriptionsofanarrowsubsetofthestory.\nAgain,thedifferenceinretrievalmechanismsaffectstheperformanceofGPT-4whenansweringthequestion.\nGivenDPR’scontext,itoutputs“Thecentralthemeofthestoryistransformationandthepowerofinnerbeauty,asCinderella,akindandhumblegirl,ismagicallytransformedintoabeautifulprincess,capturingtheattentionandadmirationofthePrinceandothersattheball.\n”Thisansweronlytakesintoaccountthefirstportionofthestory,upuntilCinderellafirstmeetstheprince.",
      "#raptor_paper#Findings#raptor_paper#Findings\nThisamendmentimprovestheaccuracyoftheMETEORcalculationbytakingintoaccountthecorrectlinguisticboundariesofwords.\nQuestion:Whatisthecentralthemeofthestory?\nRAPTORFairyGodmotherhelpsCinderellaattendaballbytransformingherrags.\n.\n.\nCinderellaimpressesthePrinceattheball.\n.\n.\nshelosestrackoftimeandhastorunhomealoneinthedarkness.\nThePrinceisunabletofindCinderellaandgoesinsearchofher.\n.\n.\nSheforgavehersisters,andtreatedthemalwaysverykindly,andthePrincehadgreatcausetobegladthathehadfoundtheglassslipper.\nTwomicewereturnedintofootmen;fourgrasshoppersintowhitehorses.\nNext,theFairytouchedCinderella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\n.\nAlas!\ntheclockhadstruckEleven.\nShefoundnocoach,onlyapumpkin,andtheratandmiceranquicklyawaywhentheysawher;whileallherfinedressturnedtorags,andshehadtorunhomealone.\n.\n.\nTheytoldherabeautifulPrincesshadbeenattheball,withwhomthePrincewasdelighted.\nTheydidnotknowitwasCinderella.\nDPRQuestion:HowdoesCinderellafindahappyending?\nRAPTORThestoryofCinderellainvolveshergodmother,afairy,whotransformsapumpkinintoagrandcoachwithherwandandallowsCinderellatoattendtheball.\nHowever,Cinderellamustreturnhomebeforetheclockstrikeselevenorherdresswillturnbackintorags.\n.\n.\nCinderellaimpressesthePrinceattheballbutleavesbeforehecanfindoutwhosheis.\n.\n.\nThePrincesearchedfortheownerofalostglassslipperandfounditbelongedtoCinderella.\nSheforgavehersistersandthePrincewasgladtohavefoundher.\ntheclockhadstruckEleven.\n.\n.\nThePrincewasverymuchsurprisedwhenhemissedCinderellaagain,andleavingtheball,wentinsearchofher.\n.\n.\nFairytouchedCin-derella’srags,andtheybecamerichsatinrobes,trimmedwithpointlace.\n.\n.\nHeroldshoesbecameacharmingpairofglassslippers,whichshonelikediamonds.\n“Nowgototheball,mylove,”shesaid,“andenjoyyourself.\nButremember,youmustleavetheroombeforetheclockstrikeseleven.\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023)."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved.\nThealgorithmstartswithabroadoutlookbyconsideringthetoplayersofthetreeandprogressivelyfocusesonfinerdetailsasitdescendsthroughthelowerlayers.\nThecollapsedtreeapproachoffersasimplerwaytosearchforrelevantinformationbyconsideringallnodesinthetreesimultaneously,asdepictedinFigure2.\nInsteadofgoinglayer-by-layer,thismethodflattensthemulti-layeredtreeintoasinglelayer,essentiallybringingallthenodesontothesamelevelforcomparison.\nThestepsforthismethodareoutlinedbelow:1.\nFirst,collapsetheentireRAPTORtreeintoasinglelayer.\nThisnewsetofnodes,denotedasC,containsnodesfromeverylayeroftheoriginaltree.\n2.\nNext,calculatethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentinthecollapsedsetC.\n3.\nFinally,pickthetop-knodesthathavethehighestcosinesimilarityscoreswiththequery.\nKeepaddingnodestotheresultsetuntilyoureachapredefinedmaximumnumberoftokens,ensuringyoudon’texceedthemodel’sinputlimitations.\nWetestedbothapproacheson20storiesfromtheQASPERdataset.\nFigure3showstheperformanceoftreetraversalwithdifferenttop-sizesandcollapsedtreewithdifferentmaximumtokennumbers.\nThecollapsedtreeapproachconsistentlyperformsbetter.\nWebelievecollapsedtreeretrievalisbetterduetoofferinggreaterflexibilitythantreetraversal;i.\ne.\n,bysearchingthroughallthenodessimultaneously,itretrievesinformationthatisatthecorrectlevelofgranularityforagivenquestion.\nIncomparison,whileusingtreetraversalwiththesamevaluesofdandk,theratioofnodesfromeachlevelofthetreewillbeconstant.\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.",
      "#raptor_paper#Findings#raptor_paper#Findings\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.\nNotably,asignificantpercentageofthenodesretrievedbyRAPTORusingtheDPRretrieverfortheNarrativeQAdatasetcomefromthefirstandsecondlayersofthetree,asopposedtotheleafnodes.\nThispatternisconsistentacrosstheotherdatasetsandretrievers,albeitwithvaryingpercentages.\nTable18:Percentageofnodesfromnon-leafnodesacrossdifferentdatasetsandretrieversDatasetDPRSBERTBM25NarrativeQA57.\n36%36.\n78%34.\n96%32.\n28%24.\n41%32.\n36%Quality22.\n93%18.\n49%22.\n76%Qasper22PublishedasaconferencepaperatICLR2024Table19:PercentageofnodesfromdifferentlayerswithDPRastheretrieverLayerNarrativeQAQualityQasper0123442.\n64%45.\n00%10.\n57%1.\n78%0.\n003%67.\n71%77.\n07%29.\n43%21.\n88%1.\n05%2.\n85%----Table20:PercentageofnodesfromdifferentlayerswithSBERTastheretrieverLayerNarrativeQAQualityQasper012363.\n22%31.\n51%4.\n85%0.\n42%75.\n59%81.\n51%22.\n78%17.\n84%0.\n65%1.\n63%--Table21:PercentageofnodesfromdifferentlayerswithBM25astheretrieverLayerNarrativeQAQualityQasper012365.\n04%28.\n79%5.\n36%0.\n81%67.\n64%77.\n24%28.\n85%21.\n57%1.\n19%3.\n51%--23",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).\nTheNarrativeQA-Storytaskrequiresacomprehensiveunderstandingoftheentirenarrativeinordertoaccuratelyansweritsquestions,thustestingthemodel’sabilitytocomprehendlongertextsintheliterarydomain.\nWemeasureperformanceonthisdatasetusingthestandardBLEU(B-1,B-4),ROUGE(R-L),andMETEOR(M)metrics.\nPleaseseeappendixHformoredetailsontheNarra-tiveQAevaluationscriptusedinourexperiments.\nTheQASPERdatasetincludes5,049questionsacross1,585NLPpapers,witheachquestionprobingforinformationembeddedwithinthefulltext(Dasigietal.\n,2021).\nTheanswertypesinQASPERarecategorizedasAnswerable/Unanswerable,Yes/No,Abstractive,andExtractive.\nAccuracyismeasuredusingstandardF1.\nLastly,theQuALITYdatasetconsistsofmultiple-choicequestions,eachaccompaniedbycontextpassagesaveragingapproximately5,000tokensinlength(Pangetal.\n,2022).\nThisdatasetcallsforreasoningovertheentiredocumentforQAtasks,enablingustomeasuretheperformanceofourre-trievalsystemonmedium-lengthdocuments.\nThedatasetincludesachallengingsubset,QuALITY-HARD,whichcontainsquestionsthatamajorityofhumanannotatorsansweredincorrectlyinaspeed-setting.\nWereportaccuraciesforboththeentiretestsetandtheHARDsubset.\nControlledBaselineComparisonsWefirstpresentcontrolledcomparisonsusingtheUnifiedQA3Basthereader,withSBERT(Reimers&Gurevych,2019),BM25(Robertsonetal.\n,1995;2009),andDPR(Karpukhinetal.\n,2020)astheembeddingmodelswithandwithouttheRAPTORtreestructure,onthreedatasets:QASPER,NarrativeQA,andQuALITY.\nAsshowninTables1and2,6PublishedasaconferencepaperatICLR2024Figure4:QueryingProcess:IllustrationofhowRAPTORretrievesinformationfortwoquestionsabouttheCinderellastory:“Whatisthecentralthemeofthestory?\n”and“HowdidCinderellafindahappyending?\n”.\nHighlightednodesindicateRAPTOR’sselections,whilearrowspointtoDPR’sleafnodes.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.\nJoshuaAinslie,TaoLei,MichieldeJong,SantiagoOnta˜n´on,SiddharthaBrahma,YuryZemlyan-skiy,DavidUthus,MandyGuo,JamesLee-Thorp,YiTay,etal.\nCoLT5:Fasterlong-rangearXivpreprintarXiv:2303.\n09752,2023.\nURLtransformerswithconditionalcomputation.\nhttps://arxiv.\norg/abs/2303.\n09752.\nEkinAkyurek,TolgaBolukbasi,FrederickLiu,BinbinXiong,IanTenney,JacobAndreas,andInKelvinGuu.\nTowardstracingknowledgeinlanguagemodelsbacktothetrainingdata.\nFindingsoftheAssociationforComputationalLinguistics:EMNLP2022,pp.\n2429–2446,AbuDhabi,UnitedArabEmirates,December2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-emnlp.\n180.\nURLhttps://aclanthology.\norg/2022.\nfindings-emnlp.\n180.\nStefanosAngelidisandMirellaLapata.\nSummarizingopinions:Aspectextractionmeetssentimentpredictionandtheyarebothweaklysupervised.\narXivpreprintarXiv:1808.\n08858,2018.\nURLhttps://arxiv.\norg/abs/1808.\n08858.\nManojGhuhanArivazhagan,LanLiu,PengQi,XinchiChen,WilliamYangWang,andZhihengHuang.\nHybridhierarchicalretrievalforopen-domainquestionanswering.\nInAnnaRogers,JordanBoyd-Graber,andNaoakiOkazaki(eds.\n),FindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.",
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSo,theratioofhigher-orderthematicinformationtogranulardetailswillremainthesameregardlessofthequestion.\n5PublishedasaconferencepaperatICLR2024Onedrawback,however,ofthecollapsedtreeapproachisthatitrequirescosinesimilaritysearchtobeperformedonallnodesinthetree.\nHowever,thiscanbemademoreefficientwithfastk-nearestneighborlibrariessuchasFAISS(Johnsonetal.\n,2019).\nOverall,giventhecollapsedtreeapproach’sgreaterflexibilityanditssuperiorperformanceonthesubsetoftheQASPERdataset,thisisthequeryingapproachwithwhichweproceed.\nSpecifically,weusethecollapsedtreewith2000maximumtokens,whichapproximatelyequatestoretrievingthetop-20nodes.\nUsingatoken-basedapproachensuresthecontextdoesnotexceedmodelcontextconstraintsastokencountscanvaryacrossnodes.\nForexperimentswiththeUnifiedQAmodel,weprovide400to-kensofcontext,asUnifiedQAhasamaxcon-textlengthof512tokens.\nWeprovidethesameamountoftokensofcontexttoRAPTORandtothebaselines.\nFigure3:Comparisonofqueryingmethods.\nResultson20storiesfromtheQASPERdatasetusingtreetraversalwithdifferenttop-kvalues,andcollapsedtreewithdifferentcontextlengths.\nCollapsedtreewith2000tokensproducesthebestresults,soweusethisqueryingstrategyforourmainresults.\nQualitativeStudyWeconductaqualitativeanalysistounderstandthebenefitsofRAP-TOR’sretrievalprocesscomparedtoDensePassageRetrieval(DPR)methods.\nOurstudyfocusesonthematic,multi-hopquestionsusinga1500-wordCinderellafairytale.\nAsillustratedinFigure4,RAPTOR’stree-basedretrievalallowsittochoosenodesfromdifferenttreelayers,matchingthequestion’sdetaillevel.\nThisapproachoftenyieldsmorerelevantandcomprehensiveinformationfordownstreamtasksthanDPR.\nForadetaileddiscussionandexamples,includingthetextretrievedbybothRAPTORandDPRforspecificquestions,pleaserefertotheappendixG.\n4EXPERIMENTSDatasetsWemeasureRAPTOR’sperformanceacrossthreequestion-answeringdatasets:Narra-tiveQA,QASPER,andQuALITY.\nNarrativeQAisadatasetthatcomprisesquestion-answerpairsbasedonthefulltextsofbooksandmovietranscripts,totaling1,572documents(Koˇcisk`yetal.\n,2018;Wuetal.\n,2021).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.\nInthecontextofGMM,thenumberofparameterskisafunctionofthedimensionalityoftheinputvectorsandthenumberofclusters.\nWiththeoptimalnumberofclustersdeterminedbyBIC,theExpectation-MaximizationalgorithmisthenusedtoestimatetheGMMparameters,namelythemeans,covariances,andmixtureweights.\nWhiletheGaussianassumptioninGMMsmaynotperfectlyalignwiththenatureoftextdata,whichoftenexhibitsasparseandskeweddistribution,ourempiricalobservationssuggestthatitoffersaneffectivemodelforourpurpose.\nWerunanablationcomparingGMMClusteringwithsummarizingcontiguouschunksandprovidedetailsinAppendixB.\nModel-BasedSummarizationAfterclusteringthenodesusingGaussianMixtureModels,thenodesineachclusteraresenttoalanguagemodelforsummarization.\nThisstepallowsthemodeltotransformlargechunksoftextintoconcise,coherentsummariesoftheselectednodes.\nForourexperiments,weusegpt-3.\n5-turbotogeneratethesummaries.\nThesummarizationstepcon-densesthepotentiallylargevolumeofretrievedinformationintoamanageablesize.\nWeprovidestatisticsonthecompressionduetothesummarizationinAppendixCandthepromptusedforsummarizationinAppendixD.\nWhilethesummarizationmodelgenerallyproducesreliablesummaries,afocusedannotationstudyrevealedthatabout4%ofthesummariescontainedminorhallucinations.\nThesedidnotpropagatetoparentnodesandhadnodiscernibleimpactonquestion-answeringtasks.\nForanin-depthanalysisofhallucinations,refertotheappendixE.\nQueryingInthissection,weelaborateonthetwoqueryingmechanismsemployedbyRAPTOR:treetraversalandcollapsedtree.\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#Findings#raptor_paper#Findings\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.\norg/abs/2209.\n10063.\nShiyueZhang,DavidWan,andMohitBansal.\nExtractiveisnotfaithful:AninvestigationofInAnnaRogers,JordanBoyd-broadunfaithfulnessproblemsinextractivesummarization.\nGraber,andNaoakiOkazaki(eds.\n),Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.\n2153–2174,Toronto,Canada,July2023.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nacl-long.\n120.\nURLhttps://aclanthology.\norg/2023.\nacl-long.\n120.\nASCALABILITYANDCOMPUTATIONALEFFICIENCYOFTHETREE-BUILDINGPROCESSToassessthecomputationalefficiencyandcost-effectivenessofRAPTOR’stree-buildingprocess,weconductedexperimentsonaconsumer-gradelaptop,specificallyanAppleM1Macwith16GBofRAM.\nTheseexperimentsaimedtodemonstratethescalabilityandfeasibilityofRAPTORontypicalhardware.\nWevariedthecontextlengthfrom12,500to78,000tokensandmeasuredboththetokenexpenditureandthetimerequiredtocompletethetree-buildingprocess,frominitialsplittingandembeddingtotheconstructionofthefinalrootnode.\nFigure5:TokencostasafunctionofdocumentlengthforQASPER,NarrativeQA,andQuALITY.\nRAPTORtreeconstructioncostsscalelinearlywithdocumentlengthforeachofthedatasets.\nTokenExpenditureWeempiricallyinvestigatedtherelationshipbetweentheinitialdocumentlengthandthetotalnumberoftokensexpendedduringthetree-buildingprocess,whichincludesboththepromptandcompletiontokens.\nThedocumentlengthsvariedsignificantlyacrossthethree15PublishedasaconferencepaperatICLR2024datasetsexamined:QuALITY,QASPER,andNarrativeQA.\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens."
    ],
    [
      "#raptor_paper#Findings#raptor_paper#Findings\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nfindings-emnlp.\n171.\nURLhttps://aclanthology.\norg/2020.\nfindings-emnlp.\n171.\nOmarKhattabandMateiZaharia.\nColBERT:Efficientandeffectivepassagesearchviacon-InProceedingsofthe43rdInternationalACMSIGIRtextualizedlateinteractionoverbert.\nconferenceonresearchanddevelopmentinInformationRetrieval,pp.\n39–48,2020.\nURLhttps://arxiv.\norg/abs/2004.\n12832.\nTom´aˇsKoˇcisk`y,JonathanSchwarz,PhilBlunsom,ChrisDyer,KarlMoritzHermann,G´aborMelis,andEdwardGrefenstette.\nTheNarrativeQAReadingComprehensionChallenge.\nTransactionsoftheAssociationforComputationalLinguistics,6:317–328,2018.\nURLhttps://arxiv.\norg/abs/1712.\n07040.\n12PublishedasaconferencepaperatICLR2024PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,HeinrichK¨uttler,MikeLewis,Wen-tauYih,TimRockt¨aschel,etal.\nRetrieval-AugmentedGener-ationforKnowledge-IntensiveNLPTasks.\nAdvancesinNeuralInformationProcessingSystems,33:9459–9474,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2005.\n11401.\nJerryLiu.\nLlamaIndex,2022.\nURLhttps://github.\ncom/jerryjliu/llama_index.\nNelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.\nLostinthemiddle:Howlanguagemodelsuselongcontexts.\narXivpreprintarXiv:2307.\n03172,2023.\nURLhttps://arxiv.\norg/abs/2307.\n03172.\nYeLiu,KazumaHashimoto,YingboZhou,SemihYavuz,CaimingXiong,andPhilipYu.\nDensehierarchicalretrievalforopen-domainquestionanswering.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.",
      "#raptor_paper#Findings#raptor_paper#Findings\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n6769–6781,Online,November2020.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n550.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n550.\nDanielKhashabi,SewonMin,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.\nUNIFIEDQA:CrossingformatboundarieswithasingleQAsystem.\nInFindingsoftheAssociationforComputationalLinguistics:EMNLP2020,pp.\n1896–1907,Online,November2020.",
      "#raptor_paper#Findings#raptor_paper#Findings\n),FindingsoftheAssociationforCompu-tationalLinguistics:EMNLP2021,pp.\n188–200,PuntaCana,DominicanRepublic,Novem-ber2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nfindings-emnlp.\n19.\nURLhttps://aclanthology.\norg/2021.\nfindings-emnlp.\n19.\nLelandMcInnes,JohnHealy,andJamesMelville.\nUMAP:UniformManifoldApproximationandProjectionforDimensionReduction,2018.\nURLhttps://arxiv.\norg/abs/1802.\n03426.\narXivpreprintarXiv:1802.\n03426.\nSewonMin,KentonLee,Ming-WeiChang,KristinaToutanova,andHannanehHajishirzi.\nJointpassagerankingfordiversemulti-answerretrieval.\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n6997–7008,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n560.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n560.\nSewonMin,WeijiaShi,MikeLewis,XilunChen,Wen-tauYih,HannanehHajishirzi,andLukeInFindingsoftheAssociationforZettlemoyer.\nNonparametricmaskedlanguagemodeling.\nComputationalLinguistics:ACL2023,pp.\n2097–2118,Toronto,Canada,July2023.\nAssoci-ationforComputationalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n132.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n132.\nEricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelseaFinn.\nInInternationalConferenceonMachineLearning,Memory-basedmodeleditingatscale.\npp.\n15817–15831.\nPMLR,2022.\nURLhttps://proceedings.\nmlr.\npress/v162/mitchell22a/mitchell22a.\npdf.\nXiangyangMou,MoYu,BingshengYao,ChenghaoYang,XiaoxiaoGuo,SaloniPotdar,andHuiSu.\nFrustratinglyhardevidenceretrievalforQAoverbooks.\nInProceedingsoftheFirstJointWorkshoponNarrativeUnderstanding,Storylines,andEvents,pp.\n108–113,Online,July2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.",
      "#raptor_paper#Findings#raptor_paper#Findings\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nacl-long.\n58.\nURLhttps://aclanthology.\norg/2022.\nacl-long.\n58.\nDanqiChen,AdamFisch,JasonWeston,andAntoineBordes.\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nnuse-1.\n13.\nURLhttps://aclanthology.\norg/2020.\nnuse-1.\n13.\nInderjeetNair,AparnaGarimella,BalajiVasanSrinivasan,NatwarModani,NiyatiChhaya,Srikr-ishnaKaranam,andSumitShekhar.\nAneuralCRF-basedhierarchicalapproachforlin-InFindingsoftheAssociationforComputationalLinguistics:EACLeartextsegmentation.\n2023,pp.\n883–893,Dubrovnik,Croatia,May2023.\nAssociationforComputationalLinguis-tics.\ndoi:10.\n18653/v1/2023.\nfindings-eacl.\n65.\nURLhttps://aclanthology.\norg/2023.\nfindings-eacl.\n65.\nBenjaminNewman,LucaSoldaini,RaymondFok,ArmanCohan,andKyleLo.\nAcontrollableqa-basedframeworkfordecontextualization.\narXivpreprintarXiv:2305.\n14772,2023.\nURLhttps://arxiv.\norg/pdf/2305.\n14772.\npdf.\nOpenAI.\nGPT-4TechnicalReport.\nArXiv,abs/2303.\n08774,2023.\nURLhttps://arxiv.\norg/abs/2303.\n08774.\nRichardYuanzhePang,AliciaParrish,NitishJoshi,NikitaNangia,JasonPhang,AngelicaChen,VishakhPadmakumar,JohnnyMa,JanaThompson,HeHe,andSamuelBowman.\nQuALITY:13PublishedasaconferencepaperatICLR2024InProceedingsofthe2022ConferenceofQuestionAnsweringwithLongInputTexts,Yes!\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pp.\n5336–5358,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\nURLhttps://aclanthology.\norg/2022.\nnaacl-main.\n391.\nFabioPetroni,TimRockt¨aschel,PatrickLewis,AntonBakhtin,YuxiangWu,AlexanderHMiller,andSebastianRiedel.\nLanguagemodelsasknowledgebases?\narXivpreprintarXiv:1909.\n01066,2019.\nURLhttps://arxiv.\norg/abs/1909.\n01066.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.\nScalinglanguagemodels:Methods,Analysis&InsightsfromTrainingGopher.\narXivpreprintarXiv:2112.\n11446,2021.\nURLhttps://arxiv.\norg/abs/2112.\n11446.\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nReadingWikipediatoAnswerInProceedingsofthe55thAnnualMeetingoftheAssociationforOpen-DomainQuestions.\nComputationalLinguistics(Volume1:LongPapers),pp.\n1870–1879,Vancouver,Canada,July2017.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P17-1171.\nURLhttps://aclanthology.\norg/P17-1171.\nAakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal.\nPaLM:ScalingLanguageModelingwithPathways.\narXivpreprintarXiv:2204.\n02311,2022.\nURLhttps://arxiv.\norg/abs/2204.\n02311.\nArmanCohanandNazliGoharian.\nContextualizingcitationsforscientificsummarizationusingwordembeddingsanddomainknowledge.\nInProceedingsofthe40thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,pp.\n1133–1136,2017.\nURLhttps://dl.\nacm.\norg/doi/abs/10.\n1145/3077136.\n3080740.\nZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocLe,andRuslanSalakhutdinov.\nTransformer-XL:Attentivelanguagemodelsbeyondafixed-lengthcontext.\nInProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pp.\n2978–2988,Florence,Italy,July2019.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/P19-1285.\nURLhttps://aclanthology.\norg/P19-1285.\nTriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.\nFlashAttention:Fastandmemory-efficientexactattentionwithIO-Awareness.\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFindingsoftheAssociationforComputationalLinguistics:ACL2023,pp.\n10680–10689,Toronto,Canada,July2023.\nAssociationforComputa-tionalLinguistics.\ndoi:10.\n18653/v1/2023.\nfindings-acl.\n679.\nURLhttps://aclanthology.\norg/2023.\nfindings-acl.\n679.\nIzBeltagy,MatthewE.\nPeters,andArmanCohan.\nLongformer:TheLong-documentTransformer,2020.\nURLhttps://arxiv.\norg/abs/2004.\n05150.\narXivpreprintarXiv:2004.\n05150.\nSebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMilli-can,GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal.\nImprovinglanguagemodelsbyretrievingfromtrillionsoftokens.\nInInternationalconferenceonmachinelearning,pp.\n2206–2240.\nPMLR,2022.\nURLhttps://arxiv.\norg/abs/2112.\n04426.\nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-wal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRad-ford,IlyaSutskever,andDarioAmodei.\nInH.\nLarochelle,M.\nRanzato,R.\nHadsell,M.\nF.\nBalcan,andH.\nLin(eds.\n),AdvancesinNeu-ralInformationProcessingSystems,volume33,pp.\n1877–1901.\nCurranAssociates,Inc.\n,LanguageModelsareFew-ShotLearners.\n10PublishedasaconferencepaperatICLR20242020.\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nURLhttps://proceedings.\nneurips.\ncc/paper_files/paper/2020/S´ebastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,EceKa-mar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal.\nSparksofArtificialGeneralIntelligence:EarlyExperimentswithGPT-4.\narXivpreprintarXiv:2303.\n12712,2023.\nURLhttps://arxiv.\norg/abs/2303.\n12712.\nShuyangCaoandLuWang.\nHIBRIDS:Attentionwithhierarchicalbiasesforstructure-awarelongInProceedingsofthe60thAnnualMeetingoftheAssociationfordocumentsummarization.\nComputationalLinguistics(Volume1:LongPapers),pp.\n786–807,Dublin,Ireland,May2022.\nAssociationforComputationalLinguistics.\ndoi:10.",
      "#raptor_paper#Findings#raptor_paper#Findings\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526.\nSimengSun,KalpeshKrishna,AndrewMattarella-Micke,andMohitIyyer.\nDolong-rangelanguagemodelsactuallyuselong-rangecontext?\nInMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,andScottWen-tauYih(eds.\n),Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.\n807–822,OnlineandPuntaCana,DominicanRepublic,November2021.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nemnlp-main.\n62.\nURLhttps://aclanthology.\norg/2021.\nemnlp-main.\n62.\nZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.\nRecitation-augmentedlanguagemodels.\narXivpreprintarXiv:2210.\n01296,2022.\nURLhttps://arxiv.\norg/abs/2210.\n01296.\n14PublishedasaconferencepaperatICLR2024AlonTalmor,YanaiElazar,YoavGoldberg,andJonathanBerant.\noLMpics–onwhatlanguagemodelpre-trainingcaptures.\nTransactionsoftheAssociationforComputationalLinguistics,8:743–758,2020.\nURLhttps://arxiv.\norg/abs/1912.\n13283.\nBoxinWang,WeiPing,PengXu,LawrenceMcAfee,ZihanLiu,MohammadShoeybi,YiDong,OleksiiKuchaiev,BoLi,ChaoweiXiao,etal.\nShallwepretrainautoregressivelanguagemodelswithretrieval?\nacomprehensivestudy.\narXivpreprintarXiv:2304.\n06762,2023.\nURLhttps://arxiv.\norg/abs/2304.\n06762.\nJeffWu,LongOuyang,DanielM.\nZiegler,NisanStiennon,RyanLowe,JanLeike,andPaulChristiano.\nRecursivelySummarizingBookswithHumanFeedback,2021.\nURLhttps://arxiv.\norg/abs/2109.\n10862.\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,RuiZhao,KaiChen,MohammadNorouzi,andQuocV.\nLe.\nQANet:CombiningLocalConvolutionwithGlobalSelf-AttentionforRead-ingComprehension,2018.\nURLhttps://arxiv.\norg/abs/1804.\n09541.\narXivpreprintarXiv:1804.\n09541.\nWenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,andMengJiang.\nGenerateratherthanretrieve:LargeLanguageModelsarestrongcontextgenerators,2022.\nURLhttps://arxiv.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#Findings#raptor_paper#Findings\nAdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.\nURLhttps://arxiv.\norg/abs/2205.\n14135.\nPradeepDasigi,KyleLo,IzBeltagy,ArmanCohan,NoahA.\nSmith,andMattGardner.\nADatasetInProceed-ofInformation-SeekingQuestionsandAnswersAnchoredinResearchPapers.\ningsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics:HumanLanguageTechnologies,pp.\n4599–4610,Online,June2021.\nAsso-ciationforComputationalLinguistics.\ndoi:10.\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.",
      "#raptor_paper#Findings#raptor_paper#Findings\nOriRam,YoavLevine,Brown,andYoavShoham.\narXiv:2302.\n00083,2023.\nURLhttps://arxiv.\norg/abs/2302.\n00083.\nItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-In-contextretrieval-augmentedlanguagemodels.\narXivpreprintNilsReimersandIrynaGurevych.\nSentence-BERT:SentenceembeddingsusingSiameseBERT-InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLan-networks.\nguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pp.\n3982–3992,HongKong,China,November2019.\nAssociationforCom-putationalLinguistics.\ndoi:10.\n18653/v1/D19-1410.\nURLhttps://aclanthology.\norg/D19-1410.\nAdamRoberts,ColinRaffel,andNoamShazeer.\nHowMuchKnowledgeCanYouPackIntoInProceedingsofthe2020ConferenceonEmpir-theParametersofaLanguageModel?\nicalMethodsinNaturalLanguageProcessing(EMNLP),pp.\n5418–5426,Online,November2020.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2020.\nemnlp-main.\n437.\nURLhttps://aclanthology.\norg/2020.\nemnlp-main.\n437.\nStephenRobertson,HugoZaragoza,etal.\nTheProbabilisticRelevanceFramework:BM25andBeyond.\nFoundationsandTrendsinInformationRetrieval,3(4):333–389,2009.\nURLhttps://doi.\norg/10.\n1561/1500000019.\nStephenERobertson,SteveWalker,SusanJones,MichelineMHancock-Beaulieu,MikeGatford,etal.\nOkapiatTREC-3.\nNistSpecialPublicationSp,109:109,1995.\nURLhttps://www.\nmicrosoft.\ncom/en-us/research/publication/okapi-at-trec-3/.\nDevendraSinghSachan,MikeLewis,DaniYogatama,LukeZettlemoyer,JoellePineau,andManzilZaheer.\nQuestionsareallyouneedtotrainadensepassageretriever.\nTransactionsoftheAs-sociationforComputationalLinguistics,11:600–616,2023.\ndoi:10.\n1162/tacla00564.\nURLhttps://aclanthology.\norg/2023.\ntacl-1.\n35.\nGideonSchwarz.\nEstimatingtheDimensionofaModel.\nTheannalsofstatistics,pp.\n461–464,1978.\nURLhttps://projecteuclid.\norg/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.\n1214/aos/1176344136.\nfull.\nKarenSp¨arckJones.\nAStatisticalInterpretationofTermSpecificityanditsApplicationinRe-trieval.\nJournalofdocumentation,28(1):11–21,1972.\nURLhttps://doi.\norg/10.\n1108/eb026526."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n18653/v1/2021.\nnaacl-main.\n365.\nURLhttps://aclanthology.\norg/2021.\nnaacl-main.\n365.\nMengxingDong,BoweiZou,YanlingLi,andYuHong.\nCoLISA:InnerInteractionviaContrastiveLearningforMulti-choiceReadingComprehension.\nInAdvancesinInformationRetrieval:45thEuropeanConferenceonInformationRetrieval,ECIR2023,Dublin,Ireland,April2–6,2023,Proceedings,PartI,pp.\n264–278.\nSpringer,2023a.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/978-3-031-28244-7_17.\nZicanDong,TianyiTang,LunyiLi,andWayneXinZhao.\nAsurveyonlongtextmodelingwithtransformers.\narXivpreprintarXiv:2302.\n14502,2023b.\nURLhttps://arxiv.\norg/abs/2302.\n14502.\nTianyuGao,HowardYen,JiatongYu,andDanqiChen.\nEnablinglargelanguagemodelstogeneratetextwithcitations.\narXivpreprintarXiv:2305.\n14627,2023.\nURLhttps://arxiv.\norg/abs/2305.\n14627.\n11PublishedasaconferencepaperatICLR2024MandyGuo,JoshuaAinslie,DavidUthus,SantiagoOntanon,JianmoNi,Yun-HsuanSung,andYinfeiYang.\nLongT5:Efficienttext-to-texttransformerforlongsequences.\nInFindingsoftheAssociationforComputationalLinguistics:NAACL2022,pp.\n724–736,Seattle,UnitedStates,July2022.\nAssociationforComputationalLinguistics.\ndoi:10.\n18653/v1/2022.\nfindings-naacl.\n55.\nURLhttps://aclanthology.\norg/2022.\nfindings-naacl.\n55.\nKelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMingweiChang.\nRetrievalAugmentedLanguageModelPre-Training.\nInInternationalconferenceonmachinelearning,pp.\n3929–3938.\nPMLR,2020.\nURLhttps://doi.\norg/10.\n48550/arXiv.\n2002.\n08909.\nJordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.\nTrainingcompute-optimallargelanguagemodels.\narXivpreprintarXiv:2203.\n15556,2022.\nURLhttps://arxiv.\norg/abs/2203.\n15556.\nGautierIzacardandEdouardGrave.\nDistillingKnowledgefromReadertoRetrieverforQues-tionAnswering,2022.\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.\nReadingextractedsnippetsfromtechnicalorscientificdocumentsmaylackimportantcontextmakingthemdifficulttoreadorevenmisleading.\n(Cohan&Goharian,2017;Newmanetal.\n,2023;Zhangetal.\n,2023).\nRecursivesummarizationasContextSummarizationtechniquesprovideacondensedviewofdocuments,enablingmorefocusedengagementwiththecontent(Angelidis&Lapata,2018).\nThesummarization/snippetmodelbyGaoetal.\n(2023)usessummarizationsandsnippetsofpassages,whichimprovescorrectnessonmostdatasetsbutcansometimesbealossymeansofcompression.\nTherecursive-abstractivesummarizationmodelbyWuetal.\n(2021)employstaskdecompositiontosummarizesmallertextchunks,whicharelaterintegratedtoformsummariesoflargersections.\nWhilethismethodiseffectiveforcapturingbroaderthemes,itcanmissgranulardetails.\nLlamaIndex(Liu,2022)mitigatesthisissuebysimilarlysummarizingadjacenttextchunksbutalsoretainingintermediatenodesthusstoringvaryinglevelsofdetail,keepinggranulardetails.\nHowever,bothmethods,duetotheirrelianceonadjacencyforgroupingorsummarizingadjacentnodes,maystilloverlookdistantinterdependencieswithinthetext,whichwecanfindandgroupwithRAPTOR.\n3METHODSOverviewofRAPTORBuildingontheideathatlongtextsoftenpresentsubtopicsandhierarchi-calstructures(Cao&Wang,2022;Dongetal.\n,2023b),RAPTORaddressestheissueofsemanticdepthandconnectioninreadingbybuildingarecursivetreestructurethatbalancesbroaderthematiccomprehensionwithgranulardetailsandwhichallowsnodestobegroupedbasedonsemanticsim-ilaritynotjustorderinthetext.\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nURLhttps://arxiv.\norg/abs/2012.\n04584.\narXivpreprintarXiv:2012.\n04584.\nGautierIzacard,PatrickLewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,ArmandJoulin,SebastianRiedel,andEdouardGrave.\nFew-shotlearningwithre-trievalaugmentedlanguagemodels.\narXivpreprintarXiv:2208.\n03299,2022.\nURLhttps://arxiv.\norg/abs/2208.\n03299.\nZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig.\nHowcanweknowwhatlanguagemodelsknow?\nTransactionsoftheAssociationforComputationalLinguistics,8:423–438,2020.\nURLhttps://arxiv.\norg/abs/1911.\n12543.\nJeffJohnson,MatthijsDouze,andHerv´eJ´egou.\nBillion-ScaleSimilaritySearchwithGPUs.\nIEEETransactionsonBigData,7(3):535–547,2019.\nURLhttps://arxiv.\norg/abs/1702.\n08734.\nNikhilKandpal,HaikangDeng,AdamRoberts,EricWallace,andColinRaffel.\nLargeLanguageModelsstruggletolearnLong-TailKnowledge.\nInInternationalConferenceonMachineLearn-ing,pp.\n15696–15707.\nPMLR,2023.\nURLhttps://proceedings.\nmlr.\npress/v202/kandpal23a/kandpal23a.\npdf.\nVladimirKarpukhin,BarlasOguz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,DanqiChen,andWen-tauYih.\nDensePassageRetrievalforOpen-DomainQuestionAnswering.\nInProceedingsofthe2020ConferenceonEmpirical"
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.\nGMMsassumethatdatapointsaregeneratedfromamixtureofseveralGaussiandistributions.\n3PublishedasaconferencepaperatICLR2024GivenasetofNtextsegments,eachrepresentedasad-dimensionaldensevectorembedding,thelikelihoodofatextvector,x,givenitsmembershipinthekthGaussiandistribution,isdenotedbyP(x|k)=N(x;µk,Σk).\nTheoverallprobabilitydistributionisaweightedcombinationP(x)=(cid:80)Kk=1πkN(x;µk,Σk),whereπksignifiesthemixtureweightforthekthGaussiandistribution.\nThehighdimensionalityofvectorembeddingspresentsachallengefortraditionalGMMs,asdis-tancemetricsmaybehavepoorlywhenusedtomeasuresimilarityinhigh-dimensionalspaces(Ag-garwaletal.\n,2001).\nTomitigatethis,weemployUniformManifoldApproximationandProjection(UMAP),amanifoldlearningtechniquefordimensionalityreduction(McInnesetal.\n,2018).\nThenumberofnearestneighborsparameter,nneighbors,inUMAPdeterminesthebalancebetweenthepreservationoflocalandglobalstructures.\nOuralgorithmvariesnneighborstocreateahierar-chicalclusteringstructure:itfirstidentifiesglobalclustersandthenperformslocalclusteringwithintheseglobalclusters.\nThistwo-stepclusteringprocesscapturesabroadspectrumofrelationshipsamongthetextdata,frombroadthemestospecificdetails.\nShouldalocalcluster’scombinedcontexteverexceedthesummarizationmodel’stokenthreshold,ouralgorithmrecursivelyappliesclusteringwithinthecluster,ensuringthatthecontextremainswithinthetokenthreshold.\nTodeterminetheoptimalnumberofclusters,weemploytheBayesianInformationCriterion(BIC)formodelselection.\nBICnotonlypenalizesmodelcomplexitybutalsorewardsgoodnessoffit(Schwarz,1978).\nTheBICforagivenGMMisBIC=ln(N)k−2ln(ˆL),whereNisthenumberoftextsegments(ordatapoints),kisthenumberofmodelparameters,andˆListhemaximizedvalueofthelikelihoodfunctionofthemodel.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext.",
      "#raptor_paper#Findings#raptor_paper#Findings\nFigure5illustratesaclearlinearcorre-lationbetweentheinitialdocumentlengthandthetotaltokenexpenditure,emphasizingthatRAP-TORmaintainsalineartokenscalingregardlessofdocumentcomplexityorlength.\nFigure6:Buildtimeasafunctionofdocumentlengthfordocumentsofupto80,000tokens.\nRAP-TORtreeconstructiontimescaleslinearlywithdocumentlengthforeachofthedatasets.\nBuildTimeWealsoempiricallyobservedaconsistentlineartrendbetweenthedocumentlengthandthebuildtime,asshowninFigure6.\nThissuggeststhatRAPTORscaleslinearlyintermsoftime,makingitaviablesolutionforefficientlyprocessinglargecorporaofvaryinglengths.\nConclusionOverall,ourempiricalresultsindicatethatRAPTORscalesbothintermsoftokensexpendedandbuildtime.\nEvenasthecomplexityandvolumeoftheinputtextgrow,thecostofconstructingthetreescalespredictablyandlinearly.\nThisdemonstratesthatRAPTORiscomputa-tionallyefficientandwell-suitedforprocessinglargeanddiversecorpora.\nBABLATIONSTUDYONCLUSTERINGMECHANISMINRAPTORToassesstheeffectivenessoftheclusteringmechanisminourRAPTORapproach,weconductedanablationstudyontheQuALITYdataset.\nThisstudycomparesRAPTOR’sperformancewithabalancedtree-styleencodingandsummarizationofcontiguouschunks,incontrasttoourstandardclusteringmethod.\nB.\n1METHODOLOGYBothconfigurationsinthisablationstudyutilizedSBERTembeddingsandUnifiedQAtomaintainconsistencyinretrieval.\nForRAPTOR,weemployedourtypicalclusteringandsummarizationprocess.\nIncontrast,thealternativesetupinvolvedcreatingabalancedtreebyrecursivelyencodingandsummarizingcontiguoustextchunks.\nWedeterminedthewindowsizeforthissetupbasedontheaverageclustersizeobservedinRAPTOR,whichisapproximately6.\n7nodes.\nHence,wechoseawindowsizeof7nodes.\nThecollapsedtreeapproachwasappliedforretrievalinbothmodels.\nB.\n2RESULTS&DISCUSSIONTheresultsoftheablationstudyarepresentedintable9.\nTheresultsfromthisablationstudyclearlyindicateanimprovementinaccuracywhenemployingRAPTOR’sclusteringmechanismovertherecency-basedtreeapproach."
    ],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n4202naJ13]LC.\nsc[1v95081.\n1042:viXraPublishedasaconferencepaperatICLR2024RAPTOR:RECURSIVEABSTRACTIVEPROCESSINGFORTREE-ORGANIZEDRETRIEVALParthSarthi,SalmanAbdullah,AditiTuli,ShubhKhanna,AnnaGoldie,ChristopherD.\nManningStanfordUniversitypsarthi@cs.\nstanford.\neduABSTRACTRetrieval-augmentedlanguagemodelscanbetteradapttochangesinworldstateandincorporatelong-tailknowledge.\nHowever,mostexistingmethodsretrieveonlyshortcontiguouschunksfromaretrievalcorpus,limitingholisticunder-standingoftheoveralldocumentcontext.\nWeintroducethenovelapproachofrecursivelyembedding,clustering,andsummarizingchunksoftext,constructingatreewithdifferinglevelsofsummarizationfromthebottomup.\nAtinferencetime,ourRAPTORmodelretrievesfromthistree,integratinginformationacrosslengthydocumentsatdifferentlevelsofabstraction.\nControlledexperimentsshowthatretrievalwithrecursivesummariesofferssignificantimprovementsovertra-ditionalretrieval-augmentedLMsonseveraltasks.\nOnquestion-answeringtasksthatinvolvecomplex,multi-stepreasoning,weshowstate-of-the-artresults;forexample,bycouplingRAPTORretrievalwiththeuseofGPT-4,wecanimprovethebestperformanceontheQuALITYbenchmarkby20%inabsoluteaccuracy.\n1INTRODUCTIONLargeLanguageModels(LLMs)haveemergedastransformativetoolsshowingimpressiveperfor-manceonmanytasks.\nWiththegrowingsizeofLLMs,theycanservestandaloneasveryeffectiveknowledgestores,withfactsencodedwithintheirparameters(Petronietal.\n,2019;Jiangetal.\n,2020;Talmoretal.\n,2020;Raeetal.\n,2021;Hoffmannetal.\n,2022;Chowdheryetal.\n,2022;Bubecketal.\n,2023;Kandpaletal.\n,2023)andmodelscanbefurtherimprovedwithfine-tuningondownstreamtasks(Robertsetal.\n,2020).\nNevertheless,evenalargemodeldoesnotcontainsufficientdomain-specificknowledgeforparticulartasksandtheworldcontinuestochange,invalidatingfactsintheLLM.\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nUpdatingtheknowledgeofthesemodelsthroughadditionalfine-tuningoreditingisdifficult,particularlywhendealingwithvasttextcorpora(Lewisetal.\n,2020;Mitchelletal.\n,2022).\nAnalter-nativeapproach,pioneeredinopendomainquestionansweringsystems(Chenetal.\n,2017;Yuetal.\n,2018),istoindexlargequantitiesoftext,aftersplittingitintochunks(paragraphs),inaseparateinformationretrievalsystem.\nRetrievedinformationisthenpresentedtotheLLMalongwiththequestionascontext(“retrievalaugmentation”,Lewisetal.\n,2020;Izacardetal.\n,2022;Minetal.\n,2023;Rametal.\n,2023),makingiteasytoprovideasystemwithcurrentknowledgeparticulartosomedomainandenablingeasyinterpretabilityandprovenancetracking,whereastheparametricknowledgeofLLMsisopaqueanddifficulttotracebacktoitssource(Akyureketal.\n,2022).\nNevertheless,existingretrieval-augmentedapproachesalsohaveflaws.\nTheonewetackleisthatmostexistingmethodsretrieveonlyafewshort,contiguoustextchunks,whichlimitstheirabilitytorepresentandleveragelarge-scalediscoursestructure.\nThisisparticularlyrelevantforthematicquestionsthatrequireintegratingknowledgefrommultiplepartsofatext,suchasunderstandinganentirebook,asintheNarrativeQAdataset(Koˇcisk`yetal.\n,2018).\nConsiderthefairytaleofCinderella,andthequestion“HowdidCinderellareachherhappyending?\n”.\nThetop-kretrievedshortcontiguoustextswillnotcontainenoughcontexttoanswerthequestion.\nToaddressthis,wedesignanindexingandretrievalsystemthatusesatreestructuretocapturebothhigh-levelandlow-leveldetailsaboutatext.\nAsshowninFigure1,oursystem,RAPTOR,clusterschunksoftext,generatestextsummariesofthoseclusters,andthenrepeats,generatingatreefromthebottomup.\nThisstructureenablesRAPTORtoloadintoanLLM’scontextchunksrepresentingthetextatdifferentlevelssothatitcaneffectivelyandefficientlyanswerquestionsatdifferentlevels.\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\n1PublishedasaconferencepaperatICLR2024Figure1:Treeconstructionprocess:RAPTORrecursivelyclusterschunksoftextbasedontheirvectorembeddingsandgeneratestextsummariesofthoseclusters,constructingatreefromthebottomup.\nNodesclusteredtogetheraresiblings;aparentnodecontainsthetextsummaryofthatcluster.\nOurmaincontributionistheideaofusingtextsummarizationtoallowretrievalaugmentationofcontextatdifferentscales,andtoshowitseffectivenessinexperimentsoncollectionsoflongdoc-uments.\nControlledexperimentswiththreelanguagemodels(UnifiedQA(Khashabietal.\n,2020),GPT-3(Brownetal.\n,2020)andGPT-4(OpenAI,2023))showthatRAPTORoutperformscurrentretrievalaugmentation.\nMoreover,RAPTORcoupledwithGPT-4,andsometimesevenwithUni-fiedQA,givesnewstate-of-the-artresultsonthreeQAtasks:freetextresponsequestionsonbooksandmovies(NarrativeQA,Koˇcisk`yetal.\n2018),full-textNLPpapers(QASPER,Dasigietal.\n2021),andmultiple-choicequestionsbasedonmedium-lengthpassages(QuALITY,Pangetal.\n2022).\n12RELATEDWORKWhyRetrieval?\nRecentadvancesinhardwareandalgorithmshaveindeedexpandedthecon-textlengthsthatmodelscanhandle,leadingtoquestionsabouttheneedforretrievalsystems(Daietal.\n,2019;Daoetal.\n,2022;Liuetal.\n,2023).\nHowever,asLiuetal.\n(2023)andSunetal.\n(2021)havenoted,modelstendtounderutilizelong-rangecontextandseediminishingperformanceascon-textlengthincreases,especiallywhenpertinentinformationisembeddedwithinalengthycontext.\nMoreover,practically,useoflongcontextsisexpensiveandslow.\nThissuggeststhatselectingthemostrelevantinformationforknowledge-intensivetasksisstillcrucial.\nRetrievalMethodsRetrieval-augmentedlanguagemodels(RALMs)haveseenimprovementsinvariouscomponents:theretriever,thereader,andend-to-endsystemtraining.\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nRetrievalmethodshavetransitionedfromtraditionalterm-basedtechniqueslikeTF-IDF(Sp¨arckJones,1972)andBM25(Robertsonetal.\n,1995;Robertsetal.\n,2020)todeeplearning–basedstrategies(Karpukhinetal.\n,2020;Khattab&Zaharia,2020;Sachanetal.\n,2023).\nSomerecentworkproposesusinglargelanguagemodelsasretrieversduetotheirabilitytomemorizeextensiveknowledge(Yuetal.\n,2022;Sunetal.\n,2022).\nResearchonthereadercomponentincludesFusion-in-Decoder(FiD)(Izacard&Grave,2022),whichemploysbothDPRandBM25forretrievalandprocessespassagesindependentlyintheencoderandRETRO(Borgeaudetal.\n,2022;Wangetal.\n,2023),whichutilizescross-chunkedattentionandchunkwiseretrievaltogeneratetextgroundedonretrievedcontext.\nEnd-to-endsystemtrainingworkincludesAtlas(Izacardetal.\n,2022),whichfine-tunesanencoder-decodermodelinconjunctionwiththeretriever;REALM(Guuetal.\n,2020),abidirectional,maskedLMfine-tunedforopen-domainquestionanswering;andRAG(Retrieval-AugmentedGenera-tion)(Lewisetal.\n,2020),whichintegratespre-trainedsequence-to-sequencemodelswithaneuralretriever.\nMinetal.\n(2021)introducedJointPassageRetrieval(JPR)modelwhichusesatree-decodingalgorithmtohandlepassagediversityandrelevanceinmulti-answerretrieval.\nDenseHi-erarchicalRetrieval(DHR)andHybridHierarchicalRetrieval(HHR)representadvancementsinretrievalaccuracybycombiningdocumentandpassagelevelretrievalsandintegratingsparseanddenseretrievalmethods,respectively(Liuetal.\n,2021;Arivazhaganetal.\n,2023).\n1WewillreleasethecodeofRAPTORpubliclyhere.\n2234511233455687Index#8Text:summaryofnodes2and3ChildNodes:2,3TextEmbeddingTextchunks3.\n14.\n152.\nSummarizationbyLLM1.\nClustering10712843569FormationofonetreelayerRootlayerLeaflayerContentsofanodeRAPTORTreePublishedasaconferencepaperatICLR2024Despiteadiversityinmethods,theretrievingcomponentsofmodelspredominantlyrelyonstan-dardapproaches,i.\ne.\n,chunkingcorporaandencodingwithBERT-basedretrievers.\nAlthoughthisapproachiswidelyadopted,Nairetal.\n(2023)highlightsapotentialshortcoming:contiguousseg-mentationmightnotcapturethecompletesemanticdepthofthetext."
    ],
    [],
    [
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.\nByprovidingboththeoriginaltextandhigher-levelsummariesforretrieval,RAPTORcaneffectivelyhandleawiderrangeofquestions,fromhigher-orderthematicqueriestodetail-orientedquestions.\nDetailedresultsforadditionalstoriesandanablationstudyonlayercontributionscanbefoundinAppendixI.\n5CONCLUSIONInthispaper,wehavepresentedRAPTOR,anoveltree-basedretrievalsystemthataugmentstheparametricknowledgeoflargelanguagemodelswithcontextualinformationatvariouslevelsofabstraction.\nByemployingrecursiveclusteringandsummarizationtechniques,RAPTORcreatesahierarchicaltreestructurethatiscapableofsynthesizinginformationacrossvarioussectionsoftheretrievalcorpora.\nDuringthequeryphase,RAPTORleveragesthistreestructureformoreeffectiveretrieval.\nOurcontrolledexperimentsdemonstratedthatRAPTORnotonlyoutperformstraditionalretrievalmethodsbutalsosetsnewperformancebenchmarksonseveralquestion-answeringtasks.\n9PublishedasaconferencepaperatICLR20246REPRODUCIBILITYSTATEMENTLanguageModelsforQAandSummarizationFourlanguagemodelsareusedinourRAPTORexperiments:GPT-3andGPT-4forQAtasks,andGPT-3.\n5-turboforsummarization.\nThegpt-3,gpt-4,andgpt-3.\n5-turbomodelscanbeaccessedviaAPIcalls(OpenAIAPI).\nUnifiedQA,whichisusedforQAtasks,ispubliclyavailableatHuggingFace.\nEvaluationDatasetsThethreeevaluationdatasetsusedinourexperiments—QuALITY,QASPER,andNarrativeQA—areallpubliclyaccessible.\nThesedatasetsensurethattheretrievalandQAtestsconductedinthisstudycanbereplicated.\nSourceCodeThesourcecodeforRAPTORwillbepubliclyavailablehere.\nREFERENCESCharuCAggarwal,AlexanderHinneburg,andDanielAKeim.\nOntheSurprisingBehaviorofDis-tanceMetricsinHighDimensionalSpace.\nInDatabaseTheory—ICDT2001:8thInternationalConferenceLondon,UK,January4–6,2001Proceedings8,pp.\n420–434.\nSpringer,2001.\nURLhttps://link.\nspringer.\ncom/chapter/10.\n1007/3-540-44503-x_27.",
      "#raptor_paper#Findings#raptor_paper#Findings\nIfyoudonotyourdresswillreturntoitsoriginalrags.\n”DPRTable13:RelevantexcerptsfromtextretrievedbyRAPTORandDPRforthequestionsonthefairytaleCinderella.\nIANALYSISOFDIFFERENTLAYERSONRAPTOR’SPERFORMANCEI.\n1HOWDODIFFERENTLAYERSIMPACTPERFORMANCE?\nInthissection,wepresentadetailedbreakdownofRAPTOR’sretrievalperformancewhenqueryingdifferentlayersofthehierarchicaltreestructureforvariousstories.\nThesetablesvalidatetheutilityofRAPTOR’smulti-layeredstructurefordiversequeryrequirements.\nTable14:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory2.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers58.\n8--47.\n164.\n7-41.\n152.\n947.\n121PublishedasaconferencepaperatICLR2024Figure7:HistogramshowingthepercentageofnodesretrievedfromdifferentlayersoftheRAPTORtreeacrossthreedatasets(NarrativeQA,Quality,andQasper)usingthreeretrievers(SBERT,BM25,andDPR).\nThedataindicatethatasubstantialportionofthenodescontributingtothefinalretrievalcomesfromnon-leaflayers,withanotablepercentagefromthefirstandsecondlayers,highlightingtheimportanceofRAPTOR’shierarchicalsummarizationintheretrievalprocess.\nTable15:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory3.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers66.\n6--61.\n166.\n6-61.\n166.\n683.\n3Table16:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory4.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers94.\n7-84.\n289.\n4Table17:PerformanceofRAPTORwhenqueryingdifferentlayersofthetreeforStory5.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer11layer2layers57.\n9-47.\n368.\n4I.\n2WHICHLAYERSDORETRIEVEDNODESCOMEFROM?\nWefurtherconductanablationstudyacrossallthreedatasetsandacrossthreedifferentretrieverswithRAPTORwiththecollapsedtreeretrievaltoexaminethelayersfromwhichtheretrievednodesoriginate.\nWeobservethatbetween18.\n5%to57%oftheretrievednodescomefromnon-leafnodes.\nAsillustratedinFigure7,theretrievalpatternacrosslayersrevealstheimportanceofRAPTOR’smulti-layeredtreestructure.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nWehy-pothesizedthatuppernodesplayacrucialroleinhandlingthematicormulti-hopqueriesrequiringabroaderunderstandingofthetext.\n8PublishedasaconferencepaperatICLR2024Table6:PerformancecomparisonontheNarrativeQAdatasetacrossmultiplemodels,focusingonfourmetrics:ROUGE-L,BLEU-1,BLEU-4,andMETEOR.\nRAPTOR,whenpairedwithUni-fiedQA3B,notonlysurpassesretrievalmethodslikeBM25andDPRbutalsosetsanewstate-of-the-artintheMETEORmetric.\nModelROUGE-LBLEU-1BLEU-4METEORBiDAF(Koˇcisk`yetal.\n,2018)BM25+BERT(Mouetal.\n,2020)RecursivelySummarizingBooks(Wuetal.\n,2021)Retriever+Reader(Izacard&Grave,2022)RAPTOR+UnifiedQA6.\n215.\n521.\n632.\n030.\n85.\n714.\n522.\n335.\n323.\n50.\n31.\n44.\n27.\n56.\n43.\n75.\n010.\n611.\n119.\n1Table7:AccuraciesoftheQuALITYdatasetonboththeoveralltestsetandthemorechallenginghardsubset.\nGPT-4withRAPTORsetsanewstate-of-the-art.\nModelAccuracyTestSetHardSubsetLongformer-base(Beltagyetal.\n,2020)DPRandDeBERTaV3-large(Pangetal.\n,2022)CoLISA(DeBERTaV3-large)(Dongetal.\n,2023a)RAPTOR+GPT-439.\n555.\n462.\n382.\n635.\n346.\n154.\n776.\n2Table8:PerformanceofRAPTORwhenqueryingdifferenttreelayersforStory1fromtheQuAL-ITYdataset.\nColumnsrepresentdifferentstartingpoints(highestlayer)androwsrepresentdifferentnumbersoflayersqueried.\nLayersQueried/StartLayerLayer0(LeafNodes)Layer1Layer21layer2layers3layers57.\n9--57.\n852.\n6-57.\n963.\n1573.\n68Wevalidatedthishypothesisbothquantitativelyandqualitatively.\nWepresentqualitativeanalysisinappendixG.\nToquantitativelyunderstandthecontributionoftheupper-levelnodes,weusedstoriesfromtheQuALITYdataset.\nTheRAPTORtreeisbuiltforeachofthesestories,asdescribedinSection3.\nHowever,duringretrieval,welimitthesearchtodifferentsubsetsoflayers.\nForexample,weexclusivelyretrievefromtheleafnodesandeachupperlayer,aswellasfromdifferentcontiguoussubsetsofthelayers.\nWeshowfindingsspecifictoonestoryinTable8,revealingthatafull-treesearch,utilizingalllayers,outperformedretrievalstrategiesthatfocusedonlyonspecificlayers.\nThesefindingshighlighttheimportanceofthefulltreestructureinRAPTOR.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nConstructionoftheRAPTORtreebeginswithsegmentingtheretrievalcorpusintoshort,contiguoustextsoflength100,similartotraditionalretrievalaugmentationtechniques.\nIfasentenceexceedsthe100-tokenlimit,wemovetheentiresentencetothenextchunk,ratherthancuttingitmid-sentence.\nThispreservesthecontextualandsemanticcoherenceofthetextwithineachchunk.\nThesetextsarethenembeddedusingSBERT,aBERT-basedencoder(multi-qa-mpnet-base-cos-v1)(Reimers&Gurevych,2019).\nThechunksandtheircorrespondingSBERTembeddingsformtheleafnodesofourtreestructure.\nTogroupsimilartextchunks,weemployaclusteringalgorithm.\nOnceclustered,aLanguageModelisusedtosummarizethegroupedtexts.\nThesesummarizedtextsarethenre-embedded,andthecycleofembedding,clustering,andsummarizationcontinuesuntilfurtherclusteringbecomesinfeasible,resultinginastructured,multi-layeredtreerepresentationoftheoriginaldocuments.\nAnimportantaspectofRAPTORisitscomputationalefficiency.\nThesystemscaleslinearlyintermsofbothbuildtimeandtokenexpenditure,makingitsuitableforprocessinglargeandcomplexcorpora.\nForacomprehensivediscussiononRAPTOR’sscalability,pleaserefertotheAppendixA.\nForqueryingwithinthistree,weintroducetwodistinctstrategies:treetraversalandcollapsedtree.\nThetreetraversalmethodtraversesthetreelayer-by-layer,pruningandselectingthemostrelevantnodesateachlevel.\nThecollapsedtreemethodevaluatesnodescollectivelyacrossalllayerstofindthemostrelevantones.\nClusteringAlgorithmClusteringplaysakeyroleinbuildingtheRAPTORtree,organizingtextsegmentsintocohesivegroups.\nThisstepgroupsrelatedcontenttogether,whichhelpsthesubse-quentretrievalprocess.\nOneoftheuniqueaspectsofourclusteringapproachistheuseofsoftclustering,wherenodescanbelongtomultipleclusterswithoutrequiringafixednumberofclusters.\nThisflexibilityisessen-tialbecauseindividualtextsegmentsoftencontaininformationrelevanttovarioustopics,therebywarrantingtheirinclusioninmultiplesummaries.\nOurclusteringalgorithmisbasedonGaussianMixtureModels(GMMs),anapproachthatoffersbothflexibilityandaprobabilisticframework.",
      "#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf#raptor_paper#D:\\projects\\KAG\\kag\\examples\\raptor_paper_test\\builder\\./data/raptor_paper.pdf\nThesemethodsofferuniquewaysoftraversingthemulti-layeredRAPTORtreetoretrieverelevantinformation,eachwithitsownadvantagesandtrade-offs.\nWeprovidethepseudocodeofbothmethodsinAppendixF.\nNotethatweembedallnodesusingSBERT.\nThetreetraversalmethodfirstselectsthetop-kmostrelevantrootnodesbasedontheircosinesimilaritytothequeryembedding.\nThechildrenoftheseselectednodesareconsideredatthenextlayerandthetop-knodesareselectedfromthispoolagainbasedontheircosinesimilaritytothequeryvector.\nThisprocessisrepeateduntilwereachtheleafnodes.\nFinally,thetextfromallselectednodesisconcatenatedtoformtheretrievedcontext.\nThealgorithm’sstepsareoutlinedbelow:1.\nStartattherootlayeroftheRAPTORtree.\nComputethecosinesimilaritybetweenthequeryembeddingandtheembeddingsofallnodespresentatthisinitiallayer.\n2.\nChoosethetop-knodesbasedonthehighestcosinesimilarityscores,formingthesetS1.\n4PublishedasaconferencepaperatICLR2024Figure2:Illustrationofthetreetraversalandcollapsedtreeretrievalmechanisms.\nTreetraver-salstartsattherootlevelofthetreeandretrievesthetop-k(here,top-1)node(s)basedoncosinesimilaritytothequeryvector.\nAteachlevel,itretrievesthetop-knode(s)fromthechildnodesofthepreviouslayer’stop-k.\nCollapsedtreecollapsesthetreeintoasinglelayerandretrievesnodesuntilathresholdnumberoftokensisreached,basedoncosinesimilaritytothequeryvector.\nThenodesonwhichcosinesimilaritysearchisperformedarehighlightedinbothillustrations.\n3.\nProceedtothechildnodesoftheelementsinsetS1.\nComputethecosinesimilaritybetweenthequeryvectorandthevectorembeddingsofthesechildnodes.\n4.\nSelectthetopkchildnodeswiththehighestcosinesimilarityscorestothequery,formingthesetS2.\n5.\nContinuethisprocessrecursivelyfordlayers,producingsetsS1,S2,.\n.\n.\n,Sd.\n6.\nConcatenatesetsS1throughSdtoassembletherelevantcontexttothequery.\nByadjustingthedepthdandthenumberofnodeskselectedateachlayer,thetreetraversalmethodofferscontroloverthespecificityandbreadthoftheinformationretrieved."
    ]
  ],
  "ground_truth": [
    "Dense Hierarchical Retrieval (DHR) represents advancements in retrieval accuracy by combining document and passage level retrievals and integrating sparse and dense retrieval methods.",
    "The purpose of fine-tuning the REALM model is for open-domain question answering.",
    "The use of multiple clusters benefits the clustering approach in text segmentation by allowing nodes to belong to multiple clusters, which is essential because individual text segments often contain information relevant to various topics. This flexibility warrants their inclusion in multiple summaries.",
    "Aspiring writers often struggle to find a foothold in the industry, as exemplified by the individual in the context who had to take a job as a content writer for an online marketing firm despite it not being the path he wanted to pursue.",
    "Leland McInnes is one of the authors of the paper titled \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\".",
    "RAPTOR enhances reading comprehension by building a recursive tree structure that balances broader thematic comprehension with granular details, allowing nodes to be grouped based on semantic similarity rather than just order in the text.",
    "The purpose of conducting an ablation study on the RAPTOR approach was to assess the effectiveness of the clustering mechanism.",
    "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.",
    "A limitation of existing retrieval-augmented approaches in handling thematic questions is that they retrieve only a few short, contiguous text chunks, which limits their ability to represent and leverage large-scale discourse structure.",
    "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.",
    "Selecting the most relevant information is important for knowledge-intensive tasks because it helps manage text length, making it less expensive and faster to process, especially when pertinent information is embedded within a lengthy context.",
    "The empirical observations suggest that the Gaussian assumption in GMMs offers an effective model for the purpose, despite not perfectly aligning with the nature of text data.",
    "gpt-3.5-turbo is used to generate summaries in the model-based summarization process.",
    "Cosine similarity is used in tree traversal to retrieve the top-k nodes at each level based on their similarity to the query vector. In collapsed tree retrieval, nodes are retrieved until a threshold number of tokens is reached, also based on cosine similarity to the query vector.",
    "Hallucinations had no discernible impact on the performance of QA tasks, suggesting that hallucination is not a major concern for the summarization component in the RAPTOR architecture.",
    "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer helped publish research on long-range context in language models at EMNLP 2021.",
    "RAPTOR uses iterative text grouping by employing a clustering algorithm to group similar text chunks. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and summarization continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents.",
    "The Fairy Godmother helps Cinderella attend a ball by transforming her rags, which aids her journey from hardship to fulfillment. This transformation allows Cinderella to impress the Prince at the ball, leading to her eventual happiness and love with him. Additionally, Cinderella forgives her sisters and treats them kindly, which is part of her journey towards fulfillment.",
    "Retrieval advances and passage processing boost RALM reader function by employing methods like Fusion-in-Decoder (FiD), which uses both DPR and BM25 for retrieval and processes passages independently in the encoder.",
    "RAPTOR creates a hierarchical tree structure by employing recursive clustering and summarization techniques, which synthesizes information across various sections of the retrieval corpora. This tree structure is leveraged during the query phase for more effective retrieval.",
    "RAPTOR outperforms baselines of each respective retrieval method for both QuALITY and QASPER datasets, and similarly outperforms baselines on the NarrativeQA dataset.",
    "RAPTOR outperforms baselines of each respective retrieval method for both datasets, including DPR, when used with UnifiedQA-3B.",
    "The dimensionality reduction method, specifically UMAP, overcomes traditional GMM limits in high-dimensional spaces by varying the number of nearest neighbors parameter, n neighbors, to balance the preservation of local and global structures. This approach allows for a hierarchical clustering structure that first identifies global clusters and then performs local clustering within these global clusters, capturing a broad spectrum of relationships among the text data.",
    "Text grouping in RAPTOR trees boosts efficiency by organizing text segments into cohesive groups, which aids the subsequent retrieval process. This structured, multi-layered tree representation allows for efficient summarization and re-embedding, enhancing computational efficiency.",
    "RAPTOR's QA is assessed using three datasets: NarrativeQA, QASPER, and QuALITY.",
    "RAPTOR paired with GPT-4 sets a new state-of-the-art on the QuALITY dataset with an accuracy of 82.6%, surpassing the previous best result of 62.3%. It outperforms CoLISA by 21.5% on QuALITY-HARD. On the QASPER benchmark, RAPTOR with GPT-4 sets a new benchmark with a 55.7% F-1 score, surpassing the CoLT5 XL's score of 53.9%.",
    "The context indicates that RAPTOR scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. This linear scaling ensures that even as the complexity and volume of the input text grow, the cost of constructing the tree scales predictably and linearly, demonstrating computational efficiency.",
    "The token threshold affects node selection in the Collapsed Tree Algorithm by limiting the total number of tokens that can be included in the result. Nodes are added to the result until the total token size reaches the maximum token threshold.",
    "GPT-3.5-turbo aids in clustering and summarization by generating summaries of the nodes in each cluster after they have been grouped using Gaussian Mixture Models. This helps transform large chunks of text into concise, coherent summaries, which are then re-embedded and further processed until a structured, multi-layered tree representation of the original documents is achieved.",
    "Most existing methods retrieve only a few short, contiguous text chunks, which limits their ability to represent and leverage large-scale discourse structure.",
    "The Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, but empirical observations suggest that it offers an effective model for the purpose of clustering.",
    "RAPTOR boosts query efficiency by leveraging a hierarchical tree structure created through recursive clustering and summarization techniques. This structure synthesizes information across various sections of the retrieval corpora, allowing for more effective retrieval during the query phase.",
    "The challenges in tracing LLMs' knowledge origins include the opacity and difficulty in tracing back to its source, whereas retrieval systems provide easy interpretability and provenance tracking.",
    "The 2020 EMNLP paper by Colin Raffel is titled 'How Much Knowledge Can You Pack Into the Parameters of a Language Model?' and his later work, 'Large Language Models struggle to learn Long-Tail Knowledge,' addresses issues related to long-tail knowledge.",
    "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand. The magic ends when the clock strikes eleven, as Cinderella must return home before then or her dress will turn back into rags.",
    "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach. She must return home before the clock strikes eleven, or her dress will turn back into rags.",
    "RAPTOR builds a recursive tree structure that balances broader thematic comprehension with granular details, allowing nodes to be grouped based on semantic similarity, not just order in the text.",
    "Wu et al.'s 2021 model uses task decomposition to summarize smaller text chunks, which are later integrated to form summaries of larger sections. While this method is effective for capturing broader themes, it can miss granular details.",
    "The scalar product (dot product) is used to determine the similarity between the query and each node in the tree. Nodes are then sorted based on this similarity, and the top nodes are selected based on their scalar product values.",
    "Cinderella gets to the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand. The time limit is that Cinderella must return home before the clock strikes eleven, or her dress will turn back into rags.",
    "BIC penalizes model complexity while rewarding goodness of fit, helping to determine the optimal number of clusters in GMM.",
    "Models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context. Recursive summarization can help by embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up.",
    "Networking platforms like the Pathways app help new writers find connections and mentors by offering a platform for people in similar professions to connect and share knowledge. This allows aspiring writers to meet others who share their passion and can offer guidance and mentorship.",
    "In the context of GMM, the number of parameters k is a function of the dimensionality of the input vectors and the number of clusters. The Bayesian Information Criterion (BIC) is used for model selection, which penalizes model complexity and rewards goodness of fit.",
    "Text segmentation improves METEOR's accuracy by tokenizing the text before mapping the tokens, which takes into account the correct linguistic boundaries of words, unlike a simple split method.",
    "RAPTOR outperforms baselines of each respective retrieval method for both QuALITY and QASPER datasets, as well as the NarrativeQA dataset, when using UnifiedQA-3B as the language model.",
    "The ACL's 57th meeting with Transformer-XL in July 2019 was held in Florence, Italy.",
    "The GPT-3, GPT-4, and GPT-3.5-turbo models can be accessed via API calls (OpenAI API).",
    "The metrics that evaluate literary dataset performance are BLEU (B-1, B-4), ROUGE (R-L), and METEOR (M).",
    "Recursive summarization boosts context in retrieval models by embedding, clustering, and summarizing chunks of text to construct a tree with differing levels of summarization from the bottom up.",
    "Collective node evaluation, as used in the collapsed tree method, differs from layer-by-layer evaluation in that it evaluates nodes collectively across all layers to find the most relevant ones, rather than traversing the tree layer-by-layer and selecting nodes at each level.",
    "With the growing size of LLMs, they can serve standalone as very effective knowledge stores, with facts encoded within their parameters.",
    "Text summarization improves retrieval for UnifiedQA, GPT-3, and GPT-4 by allowing retrieval augmentation of context at different scales, which has been shown to be effective in experiments on collections of long documents.",
    "RAPTOR's linear scaling boosts efficiency with large corpora by ensuring that both build time and token expenditure increase linearly, making it suitable for processing large and complex corpora.",
    "To keep a cluster's context within token limits, the algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold.",
    "Focusing solely on 4-gram matches in BLEU-4 calculation by applying a weight of 1 to the highest order n-gram and 0 to the rest may overly emphasize 4-gram matches while neglecting lower-order matches, leading to an imbalanced evaluation.",
    "The chunking method for LLM context in QA involves splitting large quantities of text into chunks (paragraphs) and indexing them in a separate information retrieval system.",
    "RAPTOR's scaling boosts large corpus processing by being computationally efficient, as it scales linearly in terms of both build time and token expenditure.",
    "The Prince's search is linked to finding the glass slipper because he uses it to identify and find Cinderella, who impressed him at the ball but had to leave in a hurry, leaving the slipper behind.",
    "Top-k node retrieval in trees is linked to cosine similarity by retrieving the top-k node(s) based on their cosine similarity to the query vector at each level of the tree.",
    "The NarrativeQA-Story task tests a model's understanding of full narratives.",
    "A Statistical Interpretation of Term Specificity and its Application in Retrieval.",
    "Retrieval augmentation involves indexing large quantities of text by splitting it into chunks (paragraphs) in a separate information retrieval system. Retrieved information is then presented to the LLM along with the question as context.",
    "The novel approach of recursively embedding, clustering, and summarizing chunks of text helps language models adapt by using hierarchical text summaries.",
    "RAPTOR's tree structure helps with text efficiency by capturing both high-level and low-level details about a text, enabling it to load into an LLM's context chunks representing the text at different levels. This allows RAPTOR to effectively and efficiently answer questions at different levels.",
    "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi helped publish UNIFIEDQA at EMNLP 2020.",
    "Atlas fine-tunes an encoder-decoder model in conjunction with the retriever.",
    "Soft clustering's flexibility matters for text relevance because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.",
    "Retrieval systems clarify and trace sources by providing a system with current knowledge particular to some domain, enabling easy interpretability and provenance tracking, whereas the parametric knowledge of LLMs is opaque and difficult to trace back to its source.",
    "Cinderella magically attends the ball with the help of her godmother, a fairy, who transforms a pumpkin into a grand coach with her wand, allowing Cinderella to attend the ball. She impresses the Prince at the ball.",
    "RAPTOR leverages its tree structure during the query phase for more effective retrieval, which contributes to its improved performance."
  ],
  "evolution_type": [
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "simple",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "multi_context",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning",
    "reasoning"
  ],
  "metadata": [
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      },
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ],
    [
      {
        "source": "raptor_paper.pdf"
      }
    ]
  ],
  "episode_done": [
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true,
    true
  ],
  "answer": [
    "Dense Hierarchical Retrieval (DHR) achieves advancements in retrieval accuracy by employing a layered structure to handle vast amounts of data efficiently, which allows for more precise and relevant search results. It utilizes dense representation of data to ensure each query is matched against a comprehensive and nuanced database. The hierarchical aspect of DHR further refines the process by organizing information in a way that mirrors human cognitive patterns. This innovative approach significantly improves precision and recall metrics, leading to better performance in retrieving accurate information compared to traditional retrieval methods.",
    "The purpose of fine-tuning the REALM model for open-domain question answering is to enhance the model's ability to retrieve and utilize relevant information effectively when generating responses. Fine-tuning allows REALM, which stands for Retrieval-Augmented Language Model, to better integrate external information into its reasoning process, thereby improving its performance on tasks that require understanding and synthesizing information from multiple parts of a text. This process also addresses the challenge of keeping up with an ever-changing world and domain-specific knowledge, making the system more interpretable and allowing for provenance tracking of the information used in generating answers.",
    "The use of multiple clusters in text segmentation benefits the clustering approach by providing flexibility and the ability to capture nuanced relationships within text data. This approach allows individual text segments to be included in multiple summaries, which is crucial when these segments contain information relevant to various topics. The clustering algorithm, based on Gaussian Mixture Models (GMMs), supports soft clustering, meaning nodes can belong to multiple clusters without the constraint of a fixed number of clusters. This flexibility ensures that the clustering approach can accommodate the diverse and overlapping nature of information in text segments, enabling more comprehensive and coherent summaries. By allowing for multiple clusters, the approach adeptly manages the complexity inherent in text data, preserving both global and local structures through hierarchical clustering. This method effectively organizes text segments into cohesive groups, facilitating a more efficient retrieval and summarization process that can scale with the complexity and volume of the input text.",
    "Aspiring writers face several challenges in finding a foothold in the industry. One of the primary difficulties is breaking into a competitive market that is filled with numerous talented individuals competing for limited opportunities. This intense competition can lead to frustration and disillusionment. Additionally, many writers struggle with balancing their financial needs with their creative pursuits, often taking jobs that are related to writing but do not align with their true aspirations, such as content writing for marketing firms. Furthermore, the lack of connections and guidance can make the journey even more challenging. Tools like the Pathways app can help by providing opportunities for mentorship and connection with others who share similar passions, which can be crucial for navigating the complex landscape of the writing industry.",
    "One of the authors of the paper titled \"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\" is Leland McInnes. This information is found in the provided reference, which lists Leland McInnes, John Healy, and James Melville as the authors of the paper.",
    "RAPTOR enhances reading comprehension by building a recursive tree structure that captures the subtopics and hierarchical structures often present in long texts. This approach allows RAPTOR to address the challenge of semantic depth and connection in reading. The process involves segmenting the retrieval corpus into short, contiguous text chunks to preserve contextual and semantic coherence. These chunks are embedded using a BERT-based encoder and grouped using a clustering algorithm based on semantic similarity. RAPTOR organizes these segments into cohesive groups, allowing for a structured, multi-layered tree representation of the original documents. This tree structure provides varying levels of detail, balancing broader thematic comprehension with granular details, and allows nodes to be grouped based on semantic similarity rather than just text order. This organization and summarization process enables effective navigation and understanding of complex texts.",
    "The purpose of conducting an ablation study on the RAPTOR approach was to evaluate the effectiveness of its clustering mechanism. The study compared RAPTOR’s performance using its standard clustering mechanism with an alternative method to determine the contribution of the clustering mechanism to the overall performance of RAPTOR. The results indicated that RAPTOR's clustering mechanism improved accuracy compared to the alternative method.",
    "The impact of hallucinations on the summarization component in the RAPTOR architecture is minor. About 4% of the summaries generated by the model contain minor hallucinations, which typically involve the model adding small amounts of information not present in the original text or incorrectly extrapolating details. However, these hallucinations do not propagate to parent nodes, meaning they do not affect the integrity of higher-level summaries. Importantly, hallucinations have no discernible impact on the performance of question-answering tasks. Therefore, while hallucinations do occur, they are not a major concern for the efficacy of the summarization component within the RAPTOR architecture.",
    "The limitation of existing retrieval-augmented approaches in handling thematic questions is their reliance on retrieving only a few short, contiguous text chunks. This approach hampers their ability to represent and leverage large-scale discourse structures necessary for comprehensive understanding. Thematic questions often require integrating knowledge from multiple parts of a text, which these approaches struggle with.",
    "The impact of hallucinations on the summarization component in the RAPTOR architecture is minimal. While hallucinations occurred in approximately 4% of the sampled nodes, they were typically due to minor additions or incorrect extrapolations and did not propagate to higher layers of the hierarchical structure. Importantly, these inaccuracies did not have a discernible impact on the performance of question-answering tasks. Therefore, the presence of hallucinations did not significantly alter the thematic interpretation of the text or compromise the effectiveness of the summarization component within the RAPTOR architecture.",
    "Selecting the most relevant information is important for knowledge-intensive tasks because language models often underutilize long-range context, which can lead to diminished performance when pertinent information is embedded within lengthy contexts. Efficiently identifying and retrieving the most relevant information is crucial because using long contexts is both expensive and slow. Improvements in retrieval-augmented language models (RALMs) focus on selecting relevant information to enhance the model's ability to perform complex reasoning and answer questions effectively, as demonstrated by models like RAPTOR. These models enable efficient retrieval and integration of information at different levels of granularity, leading to better performance in knowledge-intensive tasks.",
    "The empirical observations suggest that while the Gaussian assumption in Gaussian Mixture Models (GMMs) may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, GMMs still offer an effective model for the purpose at hand. Despite the discrepancies between the Gaussian assumptions and the characteristics of text data, GMMs have demonstrated effectiveness in clustering for subsequent summarization tasks. This effectiveness is empirically validated, indicating that GMMs can be a suitable choice for handling text data in specific applications, even when the data's distribution does not strictly adhere to Gaussian assumptions.",
    "GPT-3.5-turbo plays a crucial role in model-based summarization by generating concise and coherent summaries from large chunks of text. In the RAPTOR system, it is used after text data is clustered using Gaussian Mixture Models (GMM), and the nodes within each cluster are sent to GPT-3.5-turbo for summarization. This process helps transform large volumes of information into more manageable summaries, which are used to enhance contextual information at different abstraction levels within RAPTOR's hierarchical tree structure.",
    "Cosine similarity is used in both tree traversal and collapsed tree retrieval mechanisms as a measure to evaluate the similarity between a query embedding and the node embeddings within a RAPTOR tree. \n\nIn the tree traversal method, the process starts at the root layer of the tree. Cosine similarity is calculated between the query embedding and the embeddings of all nodes at this initial layer. The top-k nodes with the highest cosine similarity scores are selected to form a set, S1. This process is repeated recursively for subsequent layers, where the children of the selected nodes from the previous layer are considered, and the top-k nodes based on cosine similarity are chosen at each layer. This recursive selection continues until the leaf nodes are reached, aiming to assemble relevant context for the query by concatenating the sets formed at each layer.\n\nIn the collapsed tree retrieval method, the entire RAPTOR tree is collapsed into a single layer, bringing all nodes to the same level for comparison. Cosine similarity is again used to evaluate the similarity between the query embedding and the embeddings of all nodes in this collapsed set. Nodes with the highest cosine similarity scores are selected, and this selection continues until a predefined maximum number of tokens is reached, ensuring that the model’s input limitations are not exceeded. This method allows for more flexibility by searching through all nodes simultaneously, retrieving information at the correct level of granularity for a given question. However, it is computationally intensive as it requires performing cosine similarity searches on all nodes in the tree.",
    "The impact of hallucinations on the summarization component in the RAPTOR architecture is minimal. According to the evidence provided, hallucinations were found in only 4% of the nodes analyzed, and these were minor, mostly involving the addition of minor information not present in the original text or incorrect extrapolations. Importantly, these hallucinations did not propagate to higher layers and did not alter the thematic interpretation of the text. Furthermore, they had no discernible effect on the performance of question-answering tasks, suggesting that hallucinations are not a major concern for the summarization component in the RAPTOR architecture.",
    "Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer helped publish research on long-range context in language models at EMNLP 2021. The evidence states that these individuals contributed to the study featured in the proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
    "RAPTOR uses iterative text grouping for efficient text retrieval by organizing and summarizing content in a recursive tree structure. The process starts with segmenting the retrieval corpus into short, contiguous text chunks of 100 tokens. These chunks are then embedded using SBERT, forming the leaf nodes of a tree. A clustering algorithm is applied to group similar text chunks into cohesive clusters using soft clustering, which allows nodes to belong to multiple clusters. This is important for capturing the diverse topics that may exist within a single text segment.\n\nOnce the text chunks are clustered, a language model summarizes the grouped texts, and these summaries are re-embedded. This cycle of embedding, clustering, and summarization continues until further clustering becomes infeasible. The resulting multi-layered tree structure represents both high-level themes and granular details efficiently.\n\nBy utilizing this iterative process, RAPTOR preserves the semantic depth of the original text while ensuring computational efficiency, allowing it to scale linearly with the size of the corpus. Furthermore, by clustering text based on semantic similarity rather than their order in the text, RAPTOR overcomes the limitations of relying solely on adjacency for grouping, thereby improving the accuracy and relevance of text retrieval.",
    "In the story of Cinderella, a magical figure known as the Fairy Godmother plays a crucial role in aiding the protagonist's journey from hardship to fulfillment. The Fairy Godmother intervenes in Cinderella's life by transforming a pumpkin into a grand coach and Cinderella's rags into a stunning gown, complete with glass slippers. This transformation enables Cinderella to attend the royal ball, where she captures the heart of the Prince. Despite losing one of her glass slippers in her haste to leave the ball before midnight, the Prince uses the slipper to find Cinderella. Her identity is revealed when the slipper fits her perfectly. The story concludes with Cinderella forgiving her step-sisters and treating them with kindness, thus completing her journey from adversity to happiness with the Prince, all aided by the magical intervention of her Fairy Godmother.",
    "Retrieval advances and passage processing boost the RALM reader function by improving the accuracy and efficiency with which relevant information is selected and integrated for knowledge-intensive tasks. Advances in retrieval methods, such as moving from traditional methods like TF-IDF and BM25 to deep learning-based strategies, enhance retrieval accuracy by leveraging large language models as retrievers. Techniques like Fusion-in-Decoder (FiD) and RETRO further optimize the reading process by independently processing passages and employing advanced retrieval strategies like cross-chunked attention and chunkwise retrieval. These innovations allow RALMs to handle long contexts more effectively, ensuring that the most relevant information is used, and enabling better integration and synthesis of information, which results in improved performance of the reader function in RALMs.",
    "A hierarchical tree boosts LLMs' knowledge synthesis and retrieval by organizing information in a structured manner that captures both high-level and low-level details of the text. It enables retrieval of information at different levels of abstraction, allowing LLMs to efficiently integrate and synthesize information across extensive documents. This structure provides both original text and higher-level summaries, facilitating effective handling of a wide range of questions, from thematic queries to detail-oriented inquiries. Additionally, hierarchical trees improve the performance of retrieval-augmented language models by overcoming the limitations of retrieving only short, contiguous text chunks, which often restricts the ability to represent and leverage large-scale discourse structures. This enhances the adaptability of LLMs to changes in world state and long-tail knowledge integration, leading to improved results in tasks involving complex, multi-step reasoning.",
    "Yes, RAPTOR boosts retrieval on the QuALITY dataset and works similarly on datasets like NarrativeQA. According to the evidence provided, RAPTOR enhances retrieval performance on the QuALITY dataset by achieving an accuracy of 62.4%, which is a 2% and 5.1% improvement over traditional retrieval methods such as DPR and BM25, respectively. Additionally, RAPTOR shows comparable performance on the NarrativeQA dataset, surpassing other retrieval methods like BM25 and DPR in multiple metrics, including BLEU and ROUGE-L scores. This demonstrates RAPTOR's effectiveness in boosting retrieval performance across different datasets by utilizing its recursive abstractive processing and tree-organized retrieval approach.",
    "In the QuALITY dataset, RAPTOR, when combined with UnifiedQA-3B, improves the accuracy by 2% over DPR, achieving an accuracy of 62.4%. Similarly, in the QASPER dataset, RAPTOR's F-1 Match score with UnifiedQA-3B surpasses DPR by 4.5 points, achieving a score of 36.6%. This indicates that RAPTOR enhances the retrieval performance of UnifiedQA-3B more effectively than DPR in both datasets. RAPTOR's tree-based retrieval approach provides more comprehensive context, which contributes to its superior performance in retrieving relevant information for question-answering tasks.",
    "The dimensionality reduction method, specifically Uniform Manifold Approximation and Projection (UMAP), overcomes the limitations of traditional Gaussian Mixture Models (GMMs) in high-dimensional spaces by addressing the challenges associated with distance metrics in such spaces. High-dimensional spaces often lead to poor behavior of distance metrics, making it difficult for GMMs to effectively measure similarity and cluster data.\n\nUMAP mitigates this issue by reducing the dimensionality of the data while preserving important structures. It includes a parameter, n_neighbors, which allows it to balance the preservation of local and global structures within the data. By adjusting n_neighbors, UMAP can create a hierarchical clustering structure that first identifies global clusters and then performs more detailed local clustering within these global clusters. This two-step process captures a wide range of relationships among the data, from broad themes to specific details, thus effectively addressing the limitations that traditional GMMs face in high-dimensional spaces.",
    "Text grouping in RAPTOR trees boosts efficiency through several key mechanisms:\n\n1. **Semantic Coherence and Contextual Integrity**: The initial segmentation of the retrieval corpus into short text chunks of approximately 100 tokens, while ensuring sentences that exceed this limit are moved entirely to the next chunk, preserves the contextual integrity of the text. This ensures that the grouped text remains semantically coherent.\n\n2. **Clustering with Gaussian Mixture Models (GMMs)**: The use of a clustering algorithm, specifically GMMs, allows for the grouping of similar text chunks. This soft clustering approach enables nodes to belong to multiple clusters, accommodating information relevant to various topics and maintaining semantic relevance.\n\n3. **Recursive Summarization**: The process of summarization condenses large volumes of text into concise summaries. These summaries are then re-embedded and further clustered, creating a structured, multi-layered tree representation. This recursive summarization ensures that relevant information is accessible and manageable.\n\n4. **Linear Scalability**: RAPTOR scales linearly in terms of build time and token expenditure. As the input text grows, the computational cost of constructing the tree remains predictable, making it suitable for processing large and complex corpora.\n\n5. **Efficient Information Retrieval**: The structured approach to grouping text segments supports effective querying within the multi-layered tree structure, facilitating efficient information retrieval.\n\nOverall, these aspects of text grouping and summarization in RAPTOR trees enhance computational efficiency, especially when dealing with large and complex datasets.",
    "The datasets that assess RAPTOR's QA capabilities are NarrativeQA, Quality, and Qasper. These datasets provide a comprehensive framework for evaluating RAPTOR's question-answering systems across different domains and types of content. NarrativeQA focuses on narrative-driven question-answering, Quality assesses the relevance and accuracy of generated answers, and Qasper evaluates proficiency in handling scientific literature.",
    "RAPTOR, when paired with GPT-4, significantly improves performance on both the QuALITY and QASPER benchmarks. For the QuALITY dataset, RAPTOR combined with GPT-4 achieves a new state-of-the-art accuracy of 82.6%, surpassing the previous best result of 62.3%. It also notably outperforms CoLISA by 21.5% on the challenging QuALITY-HARD subset, which consists of questions requiring more complex reasoning. On the QASPER dataset, RAPTOR with GPT-4 achieves a new benchmark F-1 score of 55.7%, surpassing the score of 53.9% set by CoLT5XL. These improvements highlight RAPTOR's ability to outperform existing models by using a comprehensive retrieval process that captures a wide range of thematic information essential for accurately answering complex questions.",
    "Linear token scaling improves RAPTOR's handling of complex data by ensuring computational efficiency and scalability. This is achieved through maintaining a linear relationship between the initial document length and the total token expenditure, regardless of the document's complexity or length. As a result, the costs associated with constructing the tree scale predictably and linearly, enabling RAPTOR to process large and diverse corpora without a significant increase in computational resources. This predictability and efficiency allow RAPTOR to manage extensive datasets effectively, maintaining performance while minimizing resource expenditure.",
    "The token threshold affects node selection in the Collapsed Tree Algorithm by determining how many nodes can be included in the result set. It serves as a limit to ensure that the selection process does not exceed the model’s input limitations. When the algorithm collapses the RAPTOR tree into a single layer and calculates the cosine similarity between the query embedding and the embeddings of all nodes, it selects the top-k nodes with the highest similarity scores. Nodes are then added to the result set until the total number of tokens reaches the predefined token threshold. This process ensures that the retrieval of relevant information is balanced with the need to maintain the input size within acceptable limits.",
    "In the document structuring process, gpt-3.5-turbo aids primarily in the summarization phase, following the clustering of text chunks. Initially, the retrieval corpus is divided into short, contiguous texts, which are then embedded using a BERT-based encoder like SBERT. These text chunks serve as the leaf nodes of a tree structure, and similar chunks are grouped together using Gaussian Mixture Models (GMMs) for clustering. \n\nOnce the clustering is done, gpt-3.5-turbo is employed to generate summaries for these grouped texts. This step is crucial because it transforms potentially large volumes of text into concise and coherent summaries, making the data more manageable and easier to retrieve for further queries. The summarization by gpt-3.5-turbo condenses the information while preserving its contextual integrity, which helps in creating a structured, multi-layered tree that facilitates efficient information retrieval.\n\nOverall, gpt-3.5-turbo plays a significant role in condensing information and maintaining context, which improves the manageability and retrievability of the document structure, even though some minor hallucinations were observed in a small percentage of summaries.",
    "Retrieval methods such as Dense Passage Retrieval (DPR), SBERT, and BM25 face limitations in fully grasping themes in datasets like NarrativeQA primarily because they tend to retrieve only short, contiguous chunks of text from a corpus. This approach restricts the ability to capture the complete semantic depth and overarching themes of a document, as contiguous segmentation may not fully encapsulate the broader thematic elements. This issue is particularly significant when dealing with complex narratives or stories, where understanding the entire narrative requires integrating knowledge from multiple sections of a text. Additionally, these methods can struggle to provide sufficient context, making the extracted snippets challenging to interpret and potentially misleading. Recursive summarization techniques, while helpful in some ways, often miss granular details and may overlook distant interdependencies, leading to a loss of important thematic connections that are not immediately adjacent in the text.",
    "Gaussian assumptions aid Gaussian Mixture Models (GMMs) in skewed text clustering by providing a flexible and probabilistic framework. Although text data often has a sparse and skewed distribution that may not perfectly match Gaussian assumptions, GMMs assume data points are generated from a mixture of several Gaussian distributions. This allows GMMs to capture a broad spectrum of relationships among text segments. The probabilistic nature of GMMs is crucial for handling the complexity and variability of text data, making them robust in clustering even when the data does not perfectly adhere to Gaussian assumptions.",
    "RAPTOR's synthesis boosts query efficiency by organizing information into a hierarchical tree structure that integrates both the original text and higher-level summaries. This structure is created using recursive clustering and summarization techniques, which allows RAPTOR to handle a wide range of questions efficiently. By employing a full-tree search strategy, RAPTOR utilizes all layers of the tree, capturing both general themes and specific details. This comprehensive approach enhances its ability to retrieve relevant information more effectively compared to strategies that focus only on specific layers.",
    "The main challenge in tracing the origins of knowledge in Large Language Models (LLMs) is due to their parametric nature, which makes it difficult to determine where specific pieces of information are sourced from. This results in opaque knowledge provenance, meaning that it is not clear or transparent where the information comes from. In contrast, retrieval systems index large quantities of text and allow for specific chunks of information to be retrieved and presented alongside questions. This makes it easier to track and verify the source of knowledge, ensuring clear and traceable provenance. Therefore, the challenge with LLMs is their lack of transparency in knowledge sourcing compared to retrieval systems, which provide more traceable information origins.",
    "The 2020 EMNLP paper by Colin Raffel that examines language model capacity is titled \"How Much Knowledge Can You Pack Into the Parameters of a Language Model?\" This paper investigates the ability of language models to store knowledge within their parameters. In his later work, Raffel addresses the challenges associated with long-tail knowledge issues in the paper \"Large Language Models Struggle to Learn Long-Tail Knowledge,\" highlighting the difficulties that large language models face in learning and representing less frequent or niche information.",
    "Cinderella gets to the ball with the help of her fairy godmother, who uses magic to transform her rags into a beautiful dress and her old shoes into glass slippers. The fairy godmother also magically transforms a pumpkin into a grand coach, two mice into footmen, and four grasshoppers into white horses, providing Cinderella with a splendid means of transportation to the ball. The magic is temporary and ends when the clock strikes eleven, at which point Cinderella's dress returns to rags, and the magical transformations, including the coach, end.",
    "Cinderella gets to the ball with the magical assistance of her Fairy Godmother. Her godmother uses magic to transform a pumpkin into a grand coach, mice into footmen, grasshoppers into white horses, and Cinderella's ragged clothes into rich satin robes with glass slippers. The time limit given to Cinderella is that she must leave the ball and return home before the clock strikes eleven, or else the magical transformations will revert to their original forms.",
    "RAPTOR's recursive tree aids in finding distant semantic links in long texts by organizing the content into a hierarchical structure that captures semantic depth and connections. The process begins by segmenting the text into short, contiguous chunks, which are then embedded and form the leaf nodes of the tree. A clustering algorithm groups these text chunks based on semantic similarity, allowing related content to be grouped together regardless of their original order. This method captures distant interdependencies within the text. The recursive summarization and re-embedding of these clusters refine the tree, maintaining semantic links and varying levels of detail. This multi-layered representation enables efficient retrieval of relevant information, even from complex and lengthy documents, by loading context chunks into an LLM's context at different levels.",
    "Wu et al.'s 2021 model uses task decomposition for summarization by breaking down larger documents into smaller, more manageable chunks. This recursive-abstract summarization approach enables the model to create summaries of these smaller sections and then integrate them into comprehensive summaries of larger sections. This method effectively captures broader themes and provides an overview of the main ideas in a document. However, the focus on broader themes can lead to missing finer, granular details, which may be critical in certain contexts. This trade-off between capturing the overall meaning and retaining detailed information is a characteristic of their summarization method.",
    "The scalar product, specifically the cosine similarity, affects node choice in a single-layer tree by determining which nodes are most relevant to a given query. In the collapsed tree approach within the RAPTOR framework, each node from every layer of the original tree is considered simultaneously. The algorithm calculates the cosine similarity between the query embedding and the embeddings of all nodes in this collapsed set. Nodes are selected based on their cosine similarity scores, with the nodes having the highest scores being chosen. Therefore, the scalar product directly influences node choice by identifying the top-k nodes with the highest cosine similarity scores, ensuring that the most relevant nodes are included in the result set.",
    "Cinderella gets to the ball with the help of her Fairy Godmother, who uses magic to transform everyday objects into extraordinary ones. The Fairy Godmother turns a pumpkin into a grand coach, Cinderella's rags into rich satin robes, and her old shoes into glass slippers. She also transforms two mice into footmen and four grasshoppers into white horses. However, there is a strict time limit on this magic: Cinderella must leave the ball and return home before the clock strikes eleven, or else all the magical transformations will be undone.",
    "The Bayesian Information Criterion (BIC) weighs complexity versus fit in Gaussian Mixture Models (GMM) by balancing the goodness of fit of the model against its complexity. The BIC formula is given as BIC = ln(N)k - 2ln(ˆL), where:\n\n- N is the number of data points.\n- k is the number of model parameters, which increases with the number of clusters and the dimensionality of the data.\n- ˆL is the maximized likelihood of the model.\n\nThe BIC penalizes the model complexity by including the term ln(N)k, which increases with the number of parameters. This discourages overly complex models with too many clusters. Conversely, it rewards models with a better fit through the term -2ln(ˆL), which decreases with higher likelihood values, indicating a better fit of the model to the data.\n\nBy minimizing the BIC, one can achieve a balance, selecting a model that fits the data well while avoiding unnecessary complexity, thus helping to determine the optimal number of clusters.",
    "Extended contexts challenge models by leading to the underutilization of relevant information and a decrease in performance when dealing with long-range dependencies. Models often struggle with maintaining efficiency and accuracy as the context length increases, which is due to pertinent information being embedded within lengthy sections of text, making it difficult for the models to extract and utilize effectively. Additionally, processing extended contexts can be computationally expensive and slow, limiting practical applications.\n\nRecursive summarization can help by providing a condensed view of documents, enabling more focused engagement with the content. Techniques like those used in the RAPTOR model process large documents hierarchically, embedding, clustering, and summarizing chunks of text to construct a tree structure with various levels of summarization. This hierarchical approach allows for the integration of information across lengthy documents at different levels of abstraction, significantly enhancing the model's understanding and performance in tasks involving complex, multi-step reasoning. As a result, recursive summarization helps models leverage the most relevant information without being overwhelmed by the vastness of extended contexts.",
    "Networking platforms help new writers find connections and mentors by providing a space where individuals in similar professions can connect and share knowledge. For example, the Pathways app allows aspiring writers to meet a diverse range of individuals, from well-established professionals to beginners. These platforms offer opportunities to engage with fellow writers and receive guidance and mentorship, which are invaluable resources for growth and success in the writing field. By facilitating such connections, networking platforms enable new writers to build relationships that can inspire and support them in their professional journeys.",
    "In Gaussian Mixture Models (GMM), the number of parameters is influenced by two main factors: the dimensionality of the input vectors and the number of clusters. The dimensionality of the input vectors affects the complexity of the model because each additional dimension requires more parameters to model the data adequately. Similarly, adding more clusters increases the number of parameters, as each cluster has its own set of parameters (such as means, covariances, and mixing coefficients).\n\nThe Bayesian Information Criterion (BIC) is used in model selection to find the optimal number of clusters. BIC helps balance model complexity and goodness of fit by penalizing the number of parameters, thus discouraging overfitting. It rewards models that fit the data well but with fewer parameters, which is particularly important in high-dimensional spaces where the behavior of distance metrics can be unpredictable. By applying BIC, the model adjusts the number of clusters dynamically, ensuring that the GMM accurately captures the underlying data structure without unnecessary complexity.",
    "Text segmentation improves METEOR's accuracy over basic splits in literary reviews by maintaining the contextual and semantic coherence of the text. Traditional methods, which often involve basic splits or chunking, can lead to the loss of critical semantic depth and context. Advanced segmentation techniques, like those used in models such as RAPTOR, improve upon this by segmenting the retrieval corpus into short, contiguous texts of manageable lengths (e.g., 100 tokens) without abruptly cutting off sentences. This approach preserves the integrity of the text's meaning and ensures that the flow and nuances of language, which carry important interpretative weight in literary reviews, are maintained. Additionally, clustering algorithms group similar text chunks, allowing related content to be processed together. This structured approach enhances the retrieval process's effectiveness and leads to improved performance metrics like METEOR, which evaluates text summarization and translation quality based on precision, recall, and alignment to human judgment. By preserving and understanding the deeper connections within the text, these sophisticated segmentation processes result in more accurate literary analysis.",
    "RAPTOR significantly enhances retrieval performance when combined with UnifiedQA-3B. In the QuALITY dataset, RAPTOR improves accuracy by 2.7% over DPR and 6.7% over BM25. Additionally, in the NarrativeQA dataset, it sets a new state-of-the-art METEOR score and performs well across multiple metrics like ROUGE-L, BLEU-1, and BLEU-4. This improvement demonstrates RAPTOR's effectiveness in enhancing the retrieval process by capturing a wide range of information from general themes to specific details, outperforming other retrieval methods.",
    "The ACL's 57th meeting, where the Transformer-XL model was featured, took place in Florence, Italy, in July 2019. This is based on the evidence provided.",
    "The API that accesses GPT for QA and summarization is the OpenAI API. This API provides access to models like GPT-3, GPT-4, and GPT-3.5-turbo, which are utilized for various question-answering tasks and summarization processes.",
    "The metrics that evaluate literary dataset performance, like B-1 and B-4, are BLEU-1 (B-1), BLEU-4 (B-4), ROUGE-L, and METEOR. These metrics are used to assess the quality of text generated by models in comparison to reference texts, focusing on different aspects of textual similarity and quality.",
    "Recursive summarization boosts context in retrieval models by constructing a hierarchical tree structure that captures different levels of text abstraction. This approach involves recursively embedding, clustering, and summarizing chunks of text, which allows the model to integrate information across lengthy documents effectively. By creating summaries at various levels, recursive summarization enables a more comprehensive understanding of the text, balancing thematic comprehension with granular details. This hierarchical approach is beneficial in retrieval tasks because it allows the model to access both the original text and higher-level summaries, facilitating the handling of a wide range of queries, from thematic to detail-oriented questions. Additionally, by organizing text based on semantic similarity rather than mere text order, recursive summarization can reveal and utilize distant interdependencies within the text, enhancing the model's ability to provide relevant and contextually rich information for knowledge-intensive tasks.",
    "The collective node evaluation approach differs from the layer-by-layer traversal method in tree queries in several key ways:\n\n1. **Structure and Process**:\n   - **Collective Node Evaluation**: This approach collapses the entire tree into a single layer, bringing all nodes onto the same level. It allows for simultaneous comparison of all nodes in the tree by calculating the cosine similarity between the query embedding and the embeddings of all nodes at once.\n   - **Layer-by-Layer Traversal**: This method processes nodes hierarchically, starting at the root level and selecting the top-k nodes based on their cosine similarity to the query vector. It then moves to the child nodes of the selected nodes, continuing this process recursively until reaching the leaf nodes.\n\n2. **Flexibility and Retrieval**:\n   - **Collective Node Evaluation**: Offers greater flexibility and efficiency by considering all nodes simultaneously. It adapts to the granularity required by the query and selects nodes with the highest similarity scores until a predefined token limit is reached.\n   - **Layer-by-Layer Traversal**: Maintains a consistent ratio of higher-order thematic information to granular details due to its structured hierarchical approach. It ensures the context relevant to the query is formed by concatenating selected nodes from each layer.\n\n3. **Computational Intensity**:\n   - **Collective Node Evaluation**: Can be computationally intensive because it performs cosine similarity searches on all nodes in the tree at once.\n   - **Layer-by-Layer Traversal**: May be less computationally intensive as it progresses through the tree in a structured manner, focusing only on relevant nodes at each layer.\n\nOverall, the collective node evaluation offers greater flexibility and can be more efficient for certain queries, but it comes at the cost of higher computational demands compared to the more structured layer-by-layer traversal method.",
    "Large Language Models (LLMs) boost performance due to their impressive size and the vast number of parameters they incorporate. The large size and high parameter count enable these models to store a wealth of facts directly within their parameters, which enhances their ability to perform well across a wide array of tasks. This intrinsic knowledge allows LLMs to generalize effectively across various contexts and tasks, making them powerful tools in natural language processing. The parameters also capture domain-specific knowledge to some extent, contributing to their robust performance.",
    "Text summarization improves retrieval for UnifiedQA, GPT-3, and GPT-4 by enabling the construction of a hierarchical tree structure through a method called RAPTOR. This method clusters chunks of text based on their vector embeddings and generates text summaries of these clusters. By doing so, it allows for the augmentation of context retrieval at different scales, which is particularly useful for handling thematic or multi-hop queries that require a broader understanding of the text, as well as detail-oriented questions. Through recursive clustering and summarization techniques, RAPTOR synthesizes information across various sections of the retrieval corpora, providing both the original text and higher-level summaries for retrieval. This approach enhances the retrieval capabilities of these language models, as shown in controlled experiments where RAPTOR outperformed traditional retrieval methods and set new performance benchmarks on several question-answering tasks.",
    "RAPTOR's linear scaling boosts efficiency with large corpora by ensuring that both the token expenditure and build time increase predictably with the length of the document, regardless of its complexity. This is achieved through a linear correlation between the initial document length and the total token expenditure, as well as the time required for tree construction. RAPTOR segments the retrieval corpus into smaller, manageable chunks and uses a clustering algorithm to group similar text segments. This approach allows for a structured and coherent representation of large documents while maintaining computational efficiency. As a result, the cost of constructing the tree remains manageable and predictable as document length increases, making RAPTOR a viable solution for processing extensive and diverse corpora.",
    "To keep a cluster's context within token limits, the algorithm uses a recursive clustering approach. Initially, it groups text segments into clusters using Gaussian Mixture Models (GMMs), which are probabilistic and flexible, allowing text segments to be included in multiple summaries if they are relevant to various topics. If a cluster's combined context exceeds the summarization model's token threshold, the algorithm applies clustering recursively within that cluster. This recursive process ensures that the context does not exceed the token limit while maintaining the summarized information's integrity and coherence.",
    "The 4-gram focus impacts BLEU-4 balance by enhancing the precision of four-word sequences in generated text, leading to improved BLEU-4 scores. As seen in the evidence, the inclusion of RAPTOR with different retrieval methods consistently results in higher BLEU-4 scores compared to when RAPTOR is not used. This indicates that the 4-gram focus helps capture longer sequences of words more accurately, thus contributing to a more coherent and contextually accurate text generation. Therefore, a better 4-gram focus enhances the balance in BLEU-4 by improving the precision and coherence of the generated sequences.",
    "The chunking method for LLM context in question answering (QA) involves splitting a large corpus of text into smaller, manageable pieces known as \"chunks.\" These chunks are then indexed within a separate information retrieval system. This method is essential for retrieval-augmented approaches, where the retrieval system provides relevant chunks as context to the large language model (LLM) alongside a given question. This allows the LLM to use these chunks to generate more accurate and contextually relevant answers.",
    "RAPTOR's scaling boosts large corpus processing by maintaining linear scaling capabilities in terms of both build time and token expenditure, regardless of the document's complexity or length. This means that as the size of the corpus increases, the time and resources needed to process it increase linearly rather than exponentially. This linear scaling is achieved through the construction of a RAPTOR tree, which involves segmenting the retrieval corpus into short, contiguous text chunks and embedding them using a BERT-based encoder. The chunks are then grouped using a clustering algorithm, forming a structure that allows for efficient processing. This efficiency is demonstrated empirically, with experiments showing a consistent linear trend between document length and build time, even on consumer-grade hardware. Thus, RAPTOR's scalability and computational efficiency make it suitable for handling large and complex corpora effectively.",
    "The Prince's search is intricately linked to the discovery of the glass slipper because it serves as the unique identifier in his quest to find the enchanting woman he met at the grand ball. After Cinderella captivates the Prince with her grace and charm, she must leave hastily before the clock strikes eleven, leaving behind the glass slipper. This slipper becomes the key to the Prince's quest, as he uses it to identify and locate Cinderella by trying it on every maiden in the kingdom. The pivotal moment occurs when the slipper fits Cinderella perfectly, revealing her true identity and leading to their happy ending. Thus, the glass slipper is crucial in propelling the Prince's search and ultimately uniting him with Cinderella.",
    "Top-k node retrieval in trees is linked to cosine similarity through a structured process that involves navigating a multi-layered tree structure. The process begins at the root layer of the tree, where the cosine similarity between a query embedding and the embeddings of all nodes at this layer is computed. Based on these similarity scores, the top-k nodes with the highest scores are selected to form a set. This selection process is recursively applied to subsequent layers, where the children of the previously selected top-k nodes are evaluated against the query vector using cosine similarity. A new top-k selection is made at each layer, continuing the process until the traversal reaches the leaf nodes. This method allows for controlling the specificity and breadth of the retrieved information by adjusting the depth of traversal and the number of nodes selected at each layer. Thus, cosine similarity plays an integral role in determining which nodes are selected at each step of the retrieval process.",
    "The task that tests a model's understanding of full narratives is the \"NarrativeQA-Story task.\" This task is part of the NarrativeQA dataset and requires a deep understanding of entire narratives to accurately answer questions derived from full texts of books and movie scripts.",
    "The 1972 work by K. Spärck Jones that affects term specificity in retrieval is titled \"A Statistical Interpretation of Term Specificity and its Application in Retrieval.\" This seminal paper provides a statistical framework for understanding term specificity, which is crucial for the effectiveness of information retrieval systems. It laid the foundation for advancements in retrieval methods, influencing the development of techniques such as TF-IDF (Term Frequency-Inverse Document Frequency).",
    "Retrieval augmentation in question answering (QA) systems uses text chunks by indexing large quantities of text, which are split into smaller chunks, typically paragraphs. These chunks are stored in a separate information retrieval system. When a question is presented to the system, it retrieves the relevant text chunks based on the question and provides them as context to a language model. This approach enhances the system's ability to provide accurate and contextually relevant answers by using the indexed chunks as sources of current knowledge. It allows the system to track the provenance of the information used to answer questions and addresses the limitations of language models in handling large-scale discourse structures or integrating knowledge from multiple text parts.",
    "The method that helps language models adapt by using hierarchical text summaries is RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval). This method constructs a hierarchical tree structure by recursively embedding, clustering, and summarizing chunks of text, which aids in integrating information from different parts of a document and enhances language model performance, especially in complex reasoning tasks.",
    "RAPTOR's tree structure enhances text efficiency by organizing and segmenting the retrieval corpus into short, contiguous chunks of 100 tokens. This method preserves the contextual and semantic coherence of the text within each chunk by ensuring that sentences are not cut mid-way and are instead moved to the next chunk. The structured, multi-layered tree representation allows for efficient querying through tree traversal and collapsed tree methods, which select the most relevant nodes for retrieval. Additionally, clustering algorithms, specifically Gaussian Mixture Models, are used to organize text segments into cohesive groups, facilitating more efficient retrieval by grouping related content. RAPTOR scales linearly in terms of build time and token expenditure, making it suitable for large and complex corpora. The recursive summarization and embedding process further contribute to efficiency by capturing both granular details and broader themes, organizing nodes based on semantic similarity. This approach supports efficient processing and retrieval of large volumes of text while maintaining important contextual information.",
    "The individuals who helped publish UNIFIEDQA at EMNLP 2020 are Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. This information is supported by the reference provided.",
    "Atlas refines an encoder-decoder model with a retriever through end-to-end training by fine-tuning the encoder-decoder model together with the retrieval component. This integration allows the system to optimize both retrieval and generation processes simultaneously, resulting in enhanced performance for tasks that require retrieving relevant information from large corpora before generating responses or answers. By refining the model in this manner, Atlas combines retrieval with language modeling to improve the accuracy and relevance of its outputs in open-domain question answering and related tasks.",
    "Soft clustering's flexibility matters for text relevance because it allows individual text segments to belong to multiple clusters, reflecting their relevance to various topics. This adaptability ensures that text segments with information pertinent to different subjects are included in multiple summaries, enhancing the comprehensiveness and relevance of the retrieved information. By accommodating the multifaceted nature of text data, where a single segment might provide valuable insights into multiple thematic areas, soft clustering ensures that all relevant information is captured and utilized effectively, particularly in contexts where diverse and intersecting topics are prevalent.",
    "Retrieval systems clarify and trace sources by indexing large quantities of text, splitting them into chunks, and using a separate information retrieval system to retrieve relevant information. This retrieved information is then presented to the language model along with the question as context, a process known as \"retrieval augmentation.\" This approach allows the system to provide current and domain-specific knowledge, making it easier to interpret and track the provenance of the information.\n\nIn contrast, the parametric knowledge of large language models (LLMs) is opaque and difficult to trace back to its source. The facts are encoded within the model's parameters, making it challenging to update or edit the knowledge in response to changes in the world or domain-specific requirements. Therefore, retrieval systems offer a clear advantage in terms of interpretability and traceability over LLMs.",
    "Cinderella magically attends the ball with the help of her fairy godmother, who uses her magical powers to transform Cinderella. The fairy godmother provides Cinderella with a stunning gown and a beautiful carriage, enabling her to attend the ball despite her initial circumstances. At the ball, Cinderella impresses the Prince with her enchanting beauty, grace, and charm, leaving a lasting impression on him and making her the center of attention and admiration.",
    "RAPTOR's tree structure improves retrieval by organizing information in a hierarchical manner, which allows it to handle a wide range of queries more effectively. This is achieved through the construction of a tree by recursively clustering and summarizing sections of the retrieval corpus, creating a layered structure that synthesizes information at various levels of abstraction. This organization enables RAPTOR to support both high-level thematic queries and detail-oriented questions, leveraging both the original text and higher-level summaries during retrieval.\n\nThe multi-layered design of the tree is crucial because it allows RAPTOR to perform a full-tree search, utilizing all layers. This comprehensive approach has been shown to outperform retrieval strategies that focus only on specific layers. A significant portion of the nodes contributing to the final retrieval come from non-leaf layers, particularly the first and second layers, emphasizing the importance of hierarchical summarization.\n\nAdditionally, RAPTOR employs a clustering algorithm, specifically Gaussian Mixture Models, for soft clustering, where nodes can belong to multiple clusters. This ensures that text segments relevant to various topics are included in multiple summaries, enhancing the retrieval process by providing a more nuanced and comprehensive view of the information. Overall, RAPTOR's tree structure improves retrieval performance by efficiently organizing and summarizing information, allowing for more effective query handling."
  ]
}